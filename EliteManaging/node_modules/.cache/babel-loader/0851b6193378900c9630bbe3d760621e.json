{"ast":null,"code":"import { __assign } from \"tslib\";\nimport { SENSITIVE_STRING } from \"@aws-sdk/smithy-client\";\nexport var BufferingHints;\n\n(function (BufferingHints) {\n  BufferingHints.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(BufferingHints || (BufferingHints = {}));\n\nexport var CloudWatchLoggingOptions;\n\n(function (CloudWatchLoggingOptions) {\n  CloudWatchLoggingOptions.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CloudWatchLoggingOptions || (CloudWatchLoggingOptions = {}));\n\nexport var CompressionFormat;\n\n(function (CompressionFormat) {\n  CompressionFormat[\"GZIP\"] = \"GZIP\";\n  CompressionFormat[\"HADOOP_SNAPPY\"] = \"HADOOP_SNAPPY\";\n  CompressionFormat[\"SNAPPY\"] = \"Snappy\";\n  CompressionFormat[\"UNCOMPRESSED\"] = \"UNCOMPRESSED\";\n  CompressionFormat[\"ZIP\"] = \"ZIP\";\n})(CompressionFormat || (CompressionFormat = {}));\n\nexport var ConcurrentModificationException;\n\n(function (ConcurrentModificationException) {\n  ConcurrentModificationException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ConcurrentModificationException || (ConcurrentModificationException = {}));\n\nexport var ContentEncoding;\n\n(function (ContentEncoding) {\n  ContentEncoding[\"GZIP\"] = \"GZIP\";\n  ContentEncoding[\"NONE\"] = \"NONE\";\n})(ContentEncoding || (ContentEncoding = {}));\n\nexport var CopyCommand;\n\n(function (CopyCommand) {\n  CopyCommand.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CopyCommand || (CopyCommand = {}));\n\nexport var KeyType;\n\n(function (KeyType) {\n  KeyType[\"AWS_OWNED_CMK\"] = \"AWS_OWNED_CMK\";\n  KeyType[\"CUSTOMER_MANAGED_CMK\"] = \"CUSTOMER_MANAGED_CMK\";\n})(KeyType || (KeyType = {}));\n\nexport var DeliveryStreamEncryptionConfigurationInput;\n\n(function (DeliveryStreamEncryptionConfigurationInput) {\n  DeliveryStreamEncryptionConfigurationInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeliveryStreamEncryptionConfigurationInput || (DeliveryStreamEncryptionConfigurationInput = {}));\n\nexport var ElasticsearchBufferingHints;\n\n(function (ElasticsearchBufferingHints) {\n  ElasticsearchBufferingHints.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ElasticsearchBufferingHints || (ElasticsearchBufferingHints = {}));\n\nexport var ProcessorParameterName;\n\n(function (ProcessorParameterName) {\n  ProcessorParameterName[\"BUFFER_INTERVAL_IN_SECONDS\"] = \"BufferIntervalInSeconds\";\n  ProcessorParameterName[\"BUFFER_SIZE_IN_MB\"] = \"BufferSizeInMBs\";\n  ProcessorParameterName[\"LAMBDA_ARN\"] = \"LambdaArn\";\n  ProcessorParameterName[\"LAMBDA_NUMBER_OF_RETRIES\"] = \"NumberOfRetries\";\n  ProcessorParameterName[\"ROLE_ARN\"] = \"RoleArn\";\n})(ProcessorParameterName || (ProcessorParameterName = {}));\n\nexport var ProcessorParameter;\n\n(function (ProcessorParameter) {\n  ProcessorParameter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProcessorParameter || (ProcessorParameter = {}));\n\nexport var Processor;\n\n(function (Processor) {\n  Processor.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Processor || (Processor = {}));\n\nexport var ProcessingConfiguration;\n\n(function (ProcessingConfiguration) {\n  ProcessingConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProcessingConfiguration || (ProcessingConfiguration = {}));\n\nexport var ElasticsearchRetryOptions;\n\n(function (ElasticsearchRetryOptions) {\n  ElasticsearchRetryOptions.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ElasticsearchRetryOptions || (ElasticsearchRetryOptions = {}));\n\nexport var KMSEncryptionConfig;\n\n(function (KMSEncryptionConfig) {\n  KMSEncryptionConfig.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KMSEncryptionConfig || (KMSEncryptionConfig = {}));\n\nexport var EncryptionConfiguration;\n\n(function (EncryptionConfiguration) {\n  EncryptionConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EncryptionConfiguration || (EncryptionConfiguration = {}));\n\nexport var S3DestinationConfiguration;\n\n(function (S3DestinationConfiguration) {\n  S3DestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(S3DestinationConfiguration || (S3DestinationConfiguration = {}));\n\nexport var VpcConfiguration;\n\n(function (VpcConfiguration) {\n  VpcConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VpcConfiguration || (VpcConfiguration = {}));\n\nexport var ElasticsearchDestinationConfiguration;\n\n(function (ElasticsearchDestinationConfiguration) {\n  ElasticsearchDestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ElasticsearchDestinationConfiguration || (ElasticsearchDestinationConfiguration = {}));\n\nexport var HiveJsonSerDe;\n\n(function (HiveJsonSerDe) {\n  HiveJsonSerDe.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HiveJsonSerDe || (HiveJsonSerDe = {}));\n\nexport var OpenXJsonSerDe;\n\n(function (OpenXJsonSerDe) {\n  OpenXJsonSerDe.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(OpenXJsonSerDe || (OpenXJsonSerDe = {}));\n\nexport var Deserializer;\n\n(function (Deserializer) {\n  Deserializer.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Deserializer || (Deserializer = {}));\n\nexport var InputFormatConfiguration;\n\n(function (InputFormatConfiguration) {\n  InputFormatConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InputFormatConfiguration || (InputFormatConfiguration = {}));\n\nexport var OrcCompression;\n\n(function (OrcCompression) {\n  OrcCompression[\"NONE\"] = \"NONE\";\n  OrcCompression[\"SNAPPY\"] = \"SNAPPY\";\n  OrcCompression[\"ZLIB\"] = \"ZLIB\";\n})(OrcCompression || (OrcCompression = {}));\n\nexport var OrcFormatVersion;\n\n(function (OrcFormatVersion) {\n  OrcFormatVersion[\"V0_11\"] = \"V0_11\";\n  OrcFormatVersion[\"V0_12\"] = \"V0_12\";\n})(OrcFormatVersion || (OrcFormatVersion = {}));\n\nexport var OrcSerDe;\n\n(function (OrcSerDe) {\n  OrcSerDe.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(OrcSerDe || (OrcSerDe = {}));\n\nexport var ParquetCompression;\n\n(function (ParquetCompression) {\n  ParquetCompression[\"GZIP\"] = \"GZIP\";\n  ParquetCompression[\"SNAPPY\"] = \"SNAPPY\";\n  ParquetCompression[\"UNCOMPRESSED\"] = \"UNCOMPRESSED\";\n})(ParquetCompression || (ParquetCompression = {}));\n\nexport var ParquetWriterVersion;\n\n(function (ParquetWriterVersion) {\n  ParquetWriterVersion[\"V1\"] = \"V1\";\n  ParquetWriterVersion[\"V2\"] = \"V2\";\n})(ParquetWriterVersion || (ParquetWriterVersion = {}));\n\nexport var ParquetSerDe;\n\n(function (ParquetSerDe) {\n  ParquetSerDe.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ParquetSerDe || (ParquetSerDe = {}));\n\nexport var Serializer;\n\n(function (Serializer) {\n  Serializer.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Serializer || (Serializer = {}));\n\nexport var OutputFormatConfiguration;\n\n(function (OutputFormatConfiguration) {\n  OutputFormatConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(OutputFormatConfiguration || (OutputFormatConfiguration = {}));\n\nexport var SchemaConfiguration;\n\n(function (SchemaConfiguration) {\n  SchemaConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SchemaConfiguration || (SchemaConfiguration = {}));\n\nexport var DataFormatConversionConfiguration;\n\n(function (DataFormatConversionConfiguration) {\n  DataFormatConversionConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DataFormatConversionConfiguration || (DataFormatConversionConfiguration = {}));\n\nexport var ExtendedS3DestinationConfiguration;\n\n(function (ExtendedS3DestinationConfiguration) {\n  ExtendedS3DestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ExtendedS3DestinationConfiguration || (ExtendedS3DestinationConfiguration = {}));\n\nexport var HttpEndpointBufferingHints;\n\n(function (HttpEndpointBufferingHints) {\n  HttpEndpointBufferingHints.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HttpEndpointBufferingHints || (HttpEndpointBufferingHints = {}));\n\nexport var HttpEndpointConfiguration;\n\n(function (HttpEndpointConfiguration) {\n  HttpEndpointConfiguration.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.Url && {\n      Url: SENSITIVE_STRING\n    }), obj.AccessKey && {\n      AccessKey: SENSITIVE_STRING\n    });\n  };\n})(HttpEndpointConfiguration || (HttpEndpointConfiguration = {}));\n\nexport var HttpEndpointCommonAttribute;\n\n(function (HttpEndpointCommonAttribute) {\n  HttpEndpointCommonAttribute.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.AttributeName && {\n      AttributeName: SENSITIVE_STRING\n    }), obj.AttributeValue && {\n      AttributeValue: SENSITIVE_STRING\n    });\n  };\n})(HttpEndpointCommonAttribute || (HttpEndpointCommonAttribute = {}));\n\nexport var HttpEndpointRequestConfiguration;\n\n(function (HttpEndpointRequestConfiguration) {\n  HttpEndpointRequestConfiguration.filterSensitiveLog = function (obj) {\n    return __assign(__assign({}, obj), obj.CommonAttributes && {\n      CommonAttributes: obj.CommonAttributes.map(function (item) {\n        return HttpEndpointCommonAttribute.filterSensitiveLog(item);\n      })\n    });\n  };\n})(HttpEndpointRequestConfiguration || (HttpEndpointRequestConfiguration = {}));\n\nexport var HttpEndpointRetryOptions;\n\n(function (HttpEndpointRetryOptions) {\n  HttpEndpointRetryOptions.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HttpEndpointRetryOptions || (HttpEndpointRetryOptions = {}));\n\nexport var HttpEndpointDestinationConfiguration;\n\n(function (HttpEndpointDestinationConfiguration) {\n  HttpEndpointDestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointConfiguration.filterSensitiveLog(obj.EndpointConfiguration)\n    }), obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration)\n    });\n  };\n})(HttpEndpointDestinationConfiguration || (HttpEndpointDestinationConfiguration = {}));\n\nexport var KinesisStreamSourceConfiguration;\n\n(function (KinesisStreamSourceConfiguration) {\n  KinesisStreamSourceConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisStreamSourceConfiguration || (KinesisStreamSourceConfiguration = {}));\n\nexport var RedshiftRetryOptions;\n\n(function (RedshiftRetryOptions) {\n  RedshiftRetryOptions.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RedshiftRetryOptions || (RedshiftRetryOptions = {}));\n\nexport var RedshiftDestinationConfiguration;\n\n(function (RedshiftDestinationConfiguration) {\n  RedshiftDestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.Username && {\n      Username: SENSITIVE_STRING\n    }), obj.Password && {\n      Password: SENSITIVE_STRING\n    });\n  };\n})(RedshiftDestinationConfiguration || (RedshiftDestinationConfiguration = {}));\n\nexport var SplunkRetryOptions;\n\n(function (SplunkRetryOptions) {\n  SplunkRetryOptions.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SplunkRetryOptions || (SplunkRetryOptions = {}));\n\nexport var SplunkDestinationConfiguration;\n\n(function (SplunkDestinationConfiguration) {\n  SplunkDestinationConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SplunkDestinationConfiguration || (SplunkDestinationConfiguration = {}));\n\nexport var Tag;\n\n(function (Tag) {\n  Tag.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Tag || (Tag = {}));\n\nexport var CreateDeliveryStreamInput;\n\n(function (CreateDeliveryStreamInput) {\n  CreateDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.RedshiftDestinationConfiguration && {\n      RedshiftDestinationConfiguration: RedshiftDestinationConfiguration.filterSensitiveLog(obj.RedshiftDestinationConfiguration)\n    }), obj.HttpEndpointDestinationConfiguration && {\n      HttpEndpointDestinationConfiguration: HttpEndpointDestinationConfiguration.filterSensitiveLog(obj.HttpEndpointDestinationConfiguration)\n    });\n  };\n})(CreateDeliveryStreamInput || (CreateDeliveryStreamInput = {}));\n\nexport var CreateDeliveryStreamOutput;\n\n(function (CreateDeliveryStreamOutput) {\n  CreateDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateDeliveryStreamOutput || (CreateDeliveryStreamOutput = {}));\n\nexport var InvalidArgumentException;\n\n(function (InvalidArgumentException) {\n  InvalidArgumentException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidArgumentException || (InvalidArgumentException = {}));\n\nexport var InvalidKMSResourceException;\n\n(function (InvalidKMSResourceException) {\n  InvalidKMSResourceException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidKMSResourceException || (InvalidKMSResourceException = {}));\n\nexport var LimitExceededException;\n\n(function (LimitExceededException) {\n  LimitExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(LimitExceededException || (LimitExceededException = {}));\n\nexport var ResourceInUseException;\n\n(function (ResourceInUseException) {\n  ResourceInUseException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceInUseException || (ResourceInUseException = {}));\n\nexport var DeleteDeliveryStreamInput;\n\n(function (DeleteDeliveryStreamInput) {\n  DeleteDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteDeliveryStreamInput || (DeleteDeliveryStreamInput = {}));\n\nexport var DeleteDeliveryStreamOutput;\n\n(function (DeleteDeliveryStreamOutput) {\n  DeleteDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteDeliveryStreamOutput || (DeleteDeliveryStreamOutput = {}));\n\nexport var ResourceNotFoundException;\n\n(function (ResourceNotFoundException) {\n  ResourceNotFoundException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceNotFoundException || (ResourceNotFoundException = {}));\n\nexport var DeliveryStreamFailureType;\n\n(function (DeliveryStreamFailureType) {\n  DeliveryStreamFailureType[\"CREATE_ENI_FAILED\"] = \"CREATE_ENI_FAILED\";\n  DeliveryStreamFailureType[\"CREATE_KMS_GRANT_FAILED\"] = \"CREATE_KMS_GRANT_FAILED\";\n  DeliveryStreamFailureType[\"DELETE_ENI_FAILED\"] = \"DELETE_ENI_FAILED\";\n  DeliveryStreamFailureType[\"DISABLED_KMS_KEY\"] = \"DISABLED_KMS_KEY\";\n  DeliveryStreamFailureType[\"ENI_ACCESS_DENIED\"] = \"ENI_ACCESS_DENIED\";\n  DeliveryStreamFailureType[\"INVALID_KMS_KEY\"] = \"INVALID_KMS_KEY\";\n  DeliveryStreamFailureType[\"KMS_ACCESS_DENIED\"] = \"KMS_ACCESS_DENIED\";\n  DeliveryStreamFailureType[\"KMS_KEY_NOT_FOUND\"] = \"KMS_KEY_NOT_FOUND\";\n  DeliveryStreamFailureType[\"KMS_OPT_IN_REQUIRED\"] = \"KMS_OPT_IN_REQUIRED\";\n  DeliveryStreamFailureType[\"RETIRE_KMS_GRANT_FAILED\"] = \"RETIRE_KMS_GRANT_FAILED\";\n  DeliveryStreamFailureType[\"SECURITY_GROUP_ACCESS_DENIED\"] = \"SECURITY_GROUP_ACCESS_DENIED\";\n  DeliveryStreamFailureType[\"SECURITY_GROUP_NOT_FOUND\"] = \"SECURITY_GROUP_NOT_FOUND\";\n  DeliveryStreamFailureType[\"SUBNET_ACCESS_DENIED\"] = \"SUBNET_ACCESS_DENIED\";\n  DeliveryStreamFailureType[\"SUBNET_NOT_FOUND\"] = \"SUBNET_NOT_FOUND\";\n  DeliveryStreamFailureType[\"UNKNOWN_ERROR\"] = \"UNKNOWN_ERROR\";\n})(DeliveryStreamFailureType || (DeliveryStreamFailureType = {}));\n\nexport var FailureDescription;\n\n(function (FailureDescription) {\n  FailureDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FailureDescription || (FailureDescription = {}));\n\nexport var DeliveryStreamEncryptionStatus;\n\n(function (DeliveryStreamEncryptionStatus) {\n  DeliveryStreamEncryptionStatus[\"DISABLED\"] = \"DISABLED\";\n  DeliveryStreamEncryptionStatus[\"DISABLING\"] = \"DISABLING\";\n  DeliveryStreamEncryptionStatus[\"DISABLING_FAILED\"] = \"DISABLING_FAILED\";\n  DeliveryStreamEncryptionStatus[\"ENABLED\"] = \"ENABLED\";\n  DeliveryStreamEncryptionStatus[\"ENABLING\"] = \"ENABLING\";\n  DeliveryStreamEncryptionStatus[\"ENABLING_FAILED\"] = \"ENABLING_FAILED\";\n})(DeliveryStreamEncryptionStatus || (DeliveryStreamEncryptionStatus = {}));\n\nexport var DeliveryStreamEncryptionConfiguration;\n\n(function (DeliveryStreamEncryptionConfiguration) {\n  DeliveryStreamEncryptionConfiguration.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeliveryStreamEncryptionConfiguration || (DeliveryStreamEncryptionConfiguration = {}));\n\nexport var DeliveryStreamStatus;\n\n(function (DeliveryStreamStatus) {\n  DeliveryStreamStatus[\"ACTIVE\"] = \"ACTIVE\";\n  DeliveryStreamStatus[\"CREATING\"] = \"CREATING\";\n  DeliveryStreamStatus[\"CREATING_FAILED\"] = \"CREATING_FAILED\";\n  DeliveryStreamStatus[\"DELETING\"] = \"DELETING\";\n  DeliveryStreamStatus[\"DELETING_FAILED\"] = \"DELETING_FAILED\";\n})(DeliveryStreamStatus || (DeliveryStreamStatus = {}));\n\nexport var S3DestinationDescription;\n\n(function (S3DestinationDescription) {\n  S3DestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(S3DestinationDescription || (S3DestinationDescription = {}));\n\nexport var VpcConfigurationDescription;\n\n(function (VpcConfigurationDescription) {\n  VpcConfigurationDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VpcConfigurationDescription || (VpcConfigurationDescription = {}));\n\nexport var ElasticsearchDestinationDescription;\n\n(function (ElasticsearchDestinationDescription) {\n  ElasticsearchDestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ElasticsearchDestinationDescription || (ElasticsearchDestinationDescription = {}));\n\nexport var ExtendedS3DestinationDescription;\n\n(function (ExtendedS3DestinationDescription) {\n  ExtendedS3DestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ExtendedS3DestinationDescription || (ExtendedS3DestinationDescription = {}));\n\nexport var HttpEndpointDescription;\n\n(function (HttpEndpointDescription) {\n  HttpEndpointDescription.filterSensitiveLog = function (obj) {\n    return __assign(__assign({}, obj), obj.Url && {\n      Url: SENSITIVE_STRING\n    });\n  };\n})(HttpEndpointDescription || (HttpEndpointDescription = {}));\n\nexport var HttpEndpointDestinationDescription;\n\n(function (HttpEndpointDestinationDescription) {\n  HttpEndpointDestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointDescription.filterSensitiveLog(obj.EndpointConfiguration)\n    }), obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration)\n    });\n  };\n})(HttpEndpointDestinationDescription || (HttpEndpointDestinationDescription = {}));\n\nexport var RedshiftDestinationDescription;\n\n(function (RedshiftDestinationDescription) {\n  RedshiftDestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign(__assign({}, obj), obj.Username && {\n      Username: SENSITIVE_STRING\n    });\n  };\n})(RedshiftDestinationDescription || (RedshiftDestinationDescription = {}));\n\nexport var SplunkDestinationDescription;\n\n(function (SplunkDestinationDescription) {\n  SplunkDestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SplunkDestinationDescription || (SplunkDestinationDescription = {}));\n\nexport var DestinationDescription;\n\n(function (DestinationDescription) {\n  DestinationDescription.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.RedshiftDestinationDescription && {\n      RedshiftDestinationDescription: RedshiftDestinationDescription.filterSensitiveLog(obj.RedshiftDestinationDescription)\n    }), obj.HttpEndpointDestinationDescription && {\n      HttpEndpointDestinationDescription: HttpEndpointDestinationDescription.filterSensitiveLog(obj.HttpEndpointDestinationDescription)\n    });\n  };\n})(DestinationDescription || (DestinationDescription = {}));\n\nexport var KinesisStreamSourceDescription;\n\n(function (KinesisStreamSourceDescription) {\n  KinesisStreamSourceDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisStreamSourceDescription || (KinesisStreamSourceDescription = {}));\n\nexport var SourceDescription;\n\n(function (SourceDescription) {\n  SourceDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SourceDescription || (SourceDescription = {}));\n\nexport var DeliveryStreamDescription;\n\n(function (DeliveryStreamDescription) {\n  DeliveryStreamDescription.filterSensitiveLog = function (obj) {\n    return __assign(__assign({}, obj), obj.Destinations && {\n      Destinations: obj.Destinations.map(function (item) {\n        return DestinationDescription.filterSensitiveLog(item);\n      })\n    });\n  };\n})(DeliveryStreamDescription || (DeliveryStreamDescription = {}));\n\nexport var DescribeDeliveryStreamInput;\n\n(function (DescribeDeliveryStreamInput) {\n  DescribeDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeDeliveryStreamInput || (DescribeDeliveryStreamInput = {}));\n\nexport var DescribeDeliveryStreamOutput;\n\n(function (DescribeDeliveryStreamOutput) {\n  DescribeDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign(__assign({}, obj), obj.DeliveryStreamDescription && {\n      DeliveryStreamDescription: DeliveryStreamDescription.filterSensitiveLog(obj.DeliveryStreamDescription)\n    });\n  };\n})(DescribeDeliveryStreamOutput || (DescribeDeliveryStreamOutput = {}));\n\nexport var S3DestinationUpdate;\n\n(function (S3DestinationUpdate) {\n  S3DestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(S3DestinationUpdate || (S3DestinationUpdate = {}));\n\nexport var ElasticsearchDestinationUpdate;\n\n(function (ElasticsearchDestinationUpdate) {\n  ElasticsearchDestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ElasticsearchDestinationUpdate || (ElasticsearchDestinationUpdate = {}));\n\nexport var ExtendedS3DestinationUpdate;\n\n(function (ExtendedS3DestinationUpdate) {\n  ExtendedS3DestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ExtendedS3DestinationUpdate || (ExtendedS3DestinationUpdate = {}));\n\nexport var ListDeliveryStreamsInput;\n\n(function (ListDeliveryStreamsInput) {\n  ListDeliveryStreamsInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListDeliveryStreamsInput || (ListDeliveryStreamsInput = {}));\n\nexport var ListDeliveryStreamsOutput;\n\n(function (ListDeliveryStreamsOutput) {\n  ListDeliveryStreamsOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListDeliveryStreamsOutput || (ListDeliveryStreamsOutput = {}));\n\nexport var ListTagsForDeliveryStreamInput;\n\n(function (ListTagsForDeliveryStreamInput) {\n  ListTagsForDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListTagsForDeliveryStreamInput || (ListTagsForDeliveryStreamInput = {}));\n\nexport var ListTagsForDeliveryStreamOutput;\n\n(function (ListTagsForDeliveryStreamOutput) {\n  ListTagsForDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListTagsForDeliveryStreamOutput || (ListTagsForDeliveryStreamOutput = {}));\n\nexport var _Record;\n\n(function (_Record) {\n  _Record.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(_Record || (_Record = {}));\n\nexport var PutRecordInput;\n\n(function (PutRecordInput) {\n  PutRecordInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PutRecordInput || (PutRecordInput = {}));\n\nexport var PutRecordOutput;\n\n(function (PutRecordOutput) {\n  PutRecordOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PutRecordOutput || (PutRecordOutput = {}));\n\nexport var ServiceUnavailableException;\n\n(function (ServiceUnavailableException) {\n  ServiceUnavailableException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ServiceUnavailableException || (ServiceUnavailableException = {}));\n\nexport var PutRecordBatchInput;\n\n(function (PutRecordBatchInput) {\n  PutRecordBatchInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PutRecordBatchInput || (PutRecordBatchInput = {}));\n\nexport var PutRecordBatchResponseEntry;\n\n(function (PutRecordBatchResponseEntry) {\n  PutRecordBatchResponseEntry.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PutRecordBatchResponseEntry || (PutRecordBatchResponseEntry = {}));\n\nexport var PutRecordBatchOutput;\n\n(function (PutRecordBatchOutput) {\n  PutRecordBatchOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PutRecordBatchOutput || (PutRecordBatchOutput = {}));\n\nexport var StartDeliveryStreamEncryptionInput;\n\n(function (StartDeliveryStreamEncryptionInput) {\n  StartDeliveryStreamEncryptionInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartDeliveryStreamEncryptionInput || (StartDeliveryStreamEncryptionInput = {}));\n\nexport var StartDeliveryStreamEncryptionOutput;\n\n(function (StartDeliveryStreamEncryptionOutput) {\n  StartDeliveryStreamEncryptionOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartDeliveryStreamEncryptionOutput || (StartDeliveryStreamEncryptionOutput = {}));\n\nexport var StopDeliveryStreamEncryptionInput;\n\n(function (StopDeliveryStreamEncryptionInput) {\n  StopDeliveryStreamEncryptionInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopDeliveryStreamEncryptionInput || (StopDeliveryStreamEncryptionInput = {}));\n\nexport var StopDeliveryStreamEncryptionOutput;\n\n(function (StopDeliveryStreamEncryptionOutput) {\n  StopDeliveryStreamEncryptionOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopDeliveryStreamEncryptionOutput || (StopDeliveryStreamEncryptionOutput = {}));\n\nexport var TagDeliveryStreamInput;\n\n(function (TagDeliveryStreamInput) {\n  TagDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TagDeliveryStreamInput || (TagDeliveryStreamInput = {}));\n\nexport var TagDeliveryStreamOutput;\n\n(function (TagDeliveryStreamOutput) {\n  TagDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TagDeliveryStreamOutput || (TagDeliveryStreamOutput = {}));\n\nexport var UntagDeliveryStreamInput;\n\n(function (UntagDeliveryStreamInput) {\n  UntagDeliveryStreamInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(UntagDeliveryStreamInput || (UntagDeliveryStreamInput = {}));\n\nexport var UntagDeliveryStreamOutput;\n\n(function (UntagDeliveryStreamOutput) {\n  UntagDeliveryStreamOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(UntagDeliveryStreamOutput || (UntagDeliveryStreamOutput = {}));\n\nexport var HttpEndpointDestinationUpdate;\n\n(function (HttpEndpointDestinationUpdate) {\n  HttpEndpointDestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointConfiguration.filterSensitiveLog(obj.EndpointConfiguration)\n    }), obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration)\n    });\n  };\n})(HttpEndpointDestinationUpdate || (HttpEndpointDestinationUpdate = {}));\n\nexport var RedshiftDestinationUpdate;\n\n(function (RedshiftDestinationUpdate) {\n  RedshiftDestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.Username && {\n      Username: SENSITIVE_STRING\n    }), obj.Password && {\n      Password: SENSITIVE_STRING\n    });\n  };\n})(RedshiftDestinationUpdate || (RedshiftDestinationUpdate = {}));\n\nexport var SplunkDestinationUpdate;\n\n(function (SplunkDestinationUpdate) {\n  SplunkDestinationUpdate.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SplunkDestinationUpdate || (SplunkDestinationUpdate = {}));\n\nexport var UpdateDestinationInput;\n\n(function (UpdateDestinationInput) {\n  UpdateDestinationInput.filterSensitiveLog = function (obj) {\n    return __assign(__assign(__assign({}, obj), obj.RedshiftDestinationUpdate && {\n      RedshiftDestinationUpdate: RedshiftDestinationUpdate.filterSensitiveLog(obj.RedshiftDestinationUpdate)\n    }), obj.HttpEndpointDestinationUpdate && {\n      HttpEndpointDestinationUpdate: HttpEndpointDestinationUpdate.filterSensitiveLog(obj.HttpEndpointDestinationUpdate)\n    });\n  };\n})(UpdateDestinationInput || (UpdateDestinationInput = {}));\n\nexport var UpdateDestinationOutput;\n\n(function (UpdateDestinationOutput) {\n  UpdateDestinationOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(UpdateDestinationOutput || (UpdateDestinationOutput = {}));","map":{"version":3,"mappings":";AAAA,SAASA,gBAAT,QAAuE,wBAAvE;AA+BA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAACC,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiBD,cAAc,KAAdA,cAAc,MAA/B;;AA4BA,OAAM,IAAWE,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACD,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiBC,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAMA,WAAYC,iBAAZ;;AAAA,WAAYA,iBAAZ,EAA6B;EAC3BA;EACAA;EACAA;EACAA;EACAA;AACD,CAND,EAAYA,iBAAiB,KAAjBA,iBAAiB,MAA7B;;AAqBA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAACH,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiBG,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAMA,WAAYC,eAAZ;;AAAA,WAAYA,eAAZ,EAA2B;EACzBA;EACAA;AACD,CAHD,EAAYA,eAAe,KAAfA,eAAe,MAA3B;;AA4CA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAACL,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiBK,WAAW,KAAXA,WAAW,MAA5B;;AAMA,WAAYC,OAAZ;;AAAA,WAAYA,OAAZ,EAAmB;EACjBA;EACAA;AACD,CAHD,EAAYA,OAAO,KAAPA,OAAO,MAAnB;;AAwCA,OAAM,IAAWC,0CAAX;;AAAN,WAAiBA,0CAAjB,EAA2D;EAC5CA,gEAAqB,UAACP,GAAD,EAAgD;IAAU,oBACvFA,GADuF;EAE1F,CAFW;AAGd,CAJD,EAAiBO,0CAA0C,KAA1CA,0CAA0C,MAA3D;;AA6BA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACR,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBQ,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAQA,WAAYC,sBAAZ;;AAAA,WAAYA,sBAAZ,EAAkC;EAChCA;EACAA;EACAA;EACAA;EACAA;AACD,CAND,EAAYA,sBAAsB,KAAtBA,sBAAsB,MAAlC;;AAuBA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACV,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBU,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAuBA,OAAM,IAAWC,SAAX;;AAAN,WAAiBA,SAAjB,EAA0B;EACXA,+BAAqB,UAACX,GAAD,EAAe;IAAU,oBACtDA,GADsD;EAEzD,CAFW;AAGd,CAJD,EAAiBW,SAAS,KAATA,SAAS,MAA1B;;AAqBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACZ,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBY,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAoBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACb,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBa,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAoBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACd,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBc,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAwBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACf,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBe,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAgEA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAChB,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBgB,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AA4FA,OAAM,IAAWC,gBAAX;;AAAN,WAAiBA,gBAAjB,EAAiC;EAClBA,sCAAqB,UAACjB,GAAD,EAAsB;IAAU,oBAC7DA,GAD6D;EAEhE,CAFW;AAGd,CAJD,EAAiBiB,gBAAgB,KAAhBA,gBAAgB,MAAjC;;AAyGA,OAAM,IAAWC,qCAAX;;AAAN,WAAiBA,qCAAjB,EAAsD;EACvCA,2DAAqB,UAAClB,GAAD,EAA2C;IAAU,oBAClFA,GADkF;EAErF,CAFW;AAGd,CAJD,EAAiBkB,qCAAqC,KAArCA,qCAAqC,MAAtD;;AAuBA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAACnB,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiBmB,aAAa,KAAbA,aAAa,MAA9B;;AAuCA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAACpB,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiBoB,cAAc,KAAdA,cAAc,MAA/B;;AA8BA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAACrB,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiBqB,YAAY,KAAZA,YAAY,MAA7B;;AAkBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACtB,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiBsB,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAMA,WAAYC,cAAZ;;AAAA,WAAYA,cAAZ,EAA0B;EACxBA;EACAA;EACAA;AACD,CAJD,EAAYA,cAAc,KAAdA,cAAc,MAA1B;;AAMA,WAAYC,gBAAZ;;AAAA,WAAYA,gBAAZ,EAA4B;EAC1BA;EACAA;AACD,CAHD,EAAYA,gBAAgB,KAAhBA,gBAAgB,MAA5B;;AAiFA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAACzB,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiByB,QAAQ,KAARA,QAAQ,MAAzB;;AAMA,WAAYC,kBAAZ;;AAAA,WAAYA,kBAAZ,EAA8B;EAC5BA;EACAA;EACAA;AACD,CAJD,EAAYA,kBAAkB,KAAlBA,kBAAkB,MAA9B;;AAMA,WAAYC,oBAAZ;;AAAA,WAAYA,oBAAZ,EAAgC;EAC9BA;EACAA;AACD,CAHD,EAAYA,oBAAoB,KAApBA,oBAAoB,MAAhC;;AAkDA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAAC5B,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiB4B,YAAY,KAAZA,YAAY,MAA7B;;AA0BA,OAAM,IAAWC,UAAX;;AAAN,WAAiBA,UAAjB,EAA2B;EACZA,gCAAqB,UAAC7B,GAAD,EAAgB;IAAU,oBACvDA,GADuD;EAE1D,CAFW;AAGd,CAJD,EAAiB6B,UAAU,KAAVA,UAAU,MAA3B;;AAmBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC9B,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB8B,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAmDA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC/B,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB+B,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAyCA,OAAM,IAAWC,iCAAX;;AAAN,WAAiBA,iCAAjB,EAAkD;EACnCA,uDAAqB,UAAChC,GAAD,EAAuC;IAAU,oBAC9EA,GAD8E;EAEjF,CAFW;AAGd,CAJD,EAAiBgC,iCAAiC,KAAjCA,iCAAiC,MAAlD;;AAqFA,OAAM,IAAWC,kCAAX;;AAAN,WAAiBA,kCAAjB,EAAmD;EACpCA,wDAAqB,UAACjC,GAAD,EAAwC;IAAU,oBAC/EA,GAD+E;EAElF,CAFW;AAGd,CAJD,EAAiBiC,kCAAkC,KAAlCA,kCAAkC,MAAnD;;AA8BA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAClC,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBkC,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AA4BA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACnC,GAAD,EAA+B;IAAU,sCACtEA,GADsE,GAErEA,GAAG,CAACoC,GAAJ,IAAW;MAAEA,GAAG,EAAEtC;IAAP,CAF0D,GAGrEE,GAAG,CAACqC,SAAJ,IAAiB;MAAEA,SAAS,EAAEvC;IAAb,CAHoD;EAIzE,CAJW;AAKd,CAND,EAAiBqC,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAwBA,OAAM,IAAWG,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACtC,GAAD,EAAiC;IAAU,sCACxEA,GADwE,GAEvEA,GAAG,CAACuC,aAAJ,IAAqB;MAAEA,aAAa,EAAEzC;IAAjB,CAFkD,GAGvEE,GAAG,CAACwC,cAAJ,IAAsB;MAAEA,cAAc,EAAE1C;IAAlB,CAHiD;EAI3E,CAJW;AAKd,CAND,EAAiBwC,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAwBA,OAAM,IAAWG,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAACzC,GAAD,EAAsC;IAAU,6BAC7EA,GAD6E,GAE5EA,GAAG,CAAC0C,gBAAJ,IAAwB;MAC1BA,gBAAgB,EAAE1C,GAAG,CAAC0C,gBAAJ,CAAqBC,GAArB,CAAyB,UAACC,IAAD,EAAK;QAAK,kCAA2B,CAACC,kBAA5B,CAA+CD,IAA/C;MAAoD,CAAvF;IADQ,CAFoD;EAKhF,CALW;AAMd,CAPD,EAAiBH,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAwBA,OAAM,IAAWK,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC9C,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB8C,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAqEA,OAAM,IAAWC,oCAAX;;AAAN,WAAiBA,oCAAjB,EAAqD;EACtCA,0DAAqB,UAAC/C,GAAD,EAA0C;IAAU,sCACjFA,GADiF,GAEhFA,GAAG,CAACgD,qBAAJ,IAA6B;MAC/BA,qBAAqB,EAAEb,yBAAyB,CAACU,kBAA1B,CAA6C7C,GAAG,CAACgD,qBAAjD;IADQ,CAFmD,GAKhFhD,GAAG,CAACiD,oBAAJ,IAA4B;MAC9BA,oBAAoB,EAAER,gCAAgC,CAACI,kBAAjC,CAAoD7C,GAAG,CAACiD,oBAAxD;IADQ,CALoD;EAQpF,CARW;AASd,CAVD,EAAiBF,oCAAoC,KAApCA,oCAAoC,MAArD;;AA6BA,OAAM,IAAWG,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAAClD,GAAD,EAAsC;IAAU,oBAC7EA,GAD6E;EAEhF,CAFW;AAGd,CAJD,EAAiBkD,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAqBA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACnD,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBmD,oBAAoB,KAApBA,oBAAoB,MAArC;;AA6EA,OAAM,IAAWC,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAACpD,GAAD,EAAsC;IAAU,sCAC7EA,GAD6E,GAE5EA,GAAG,CAACqD,QAAJ,IAAgB;MAAEA,QAAQ,EAAEvD;IAAZ,CAF4D,GAG5EE,GAAG,CAACsD,QAAJ,IAAgB;MAAEA,QAAQ,EAAExD;IAAZ,CAH4D;EAIhF,CAJW;AAKd,CAND,EAAiBsD,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAwBA,OAAM,IAAWG,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACvD,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBuD,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAuEA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACxD,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBwD,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAyBA,OAAM,IAAWC,GAAX;;AAAN,WAAiBA,GAAjB,EAAoB;EACLA,yBAAqB,UAACzD,GAAD,EAAS;IAAU,oBAChDA,GADgD;EAEnD,CAFW;AAGd,CAJD,EAAiByD,GAAG,KAAHA,GAAG,MAApB;;AA0FA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC1D,GAAD,EAA+B;IAAU,sCACtEA,GADsE,GAErEA,GAAG,CAACoD,gCAAJ,IAAwC;MAC1CA,gCAAgC,EAAEA,gCAAgC,CAACP,kBAAjC,CAChC7C,GAAG,CAACoD,gCAD4B;IADQ,CAF6B,GAOrEpD,GAAG,CAAC+C,oCAAJ,IAA4C;MAC9CA,oCAAoC,EAAEA,oCAAoC,CAACF,kBAArC,CACpC7C,GAAG,CAAC+C,oCADgC;IADQ,CAPyB;EAYzE,CAZW;AAad,CAdD,EAAiBW,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAuBA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC3D,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB2D,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAkBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC5D,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB4D,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAoBA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC7D,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB6D,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAkBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAAC9D,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiB8D,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAkBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAAC/D,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiB+D,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAwBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAChE,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBgE,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAQA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACjE,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBiE,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAkBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAClE,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBkE,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAMA,WAAYC,yBAAZ;;AAAA,WAAYA,yBAAZ,EAAqC;EACnCA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;AACD,CAhBD,EAAYA,yBAAyB,KAAzBA,yBAAyB,MAArC;;AAmCA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACpE,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBoE,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAMA,WAAYC,8BAAZ;;AAAA,WAAYA,8BAAZ,EAA0C;EACxCA;EACAA;EACAA;EACAA;EACAA;EACAA;AACD,CAPD,EAAYA,8BAA8B,KAA9BA,8BAA8B,MAA1C;;AA6CA,OAAM,IAAWC,qCAAX;;AAAN,WAAiBA,qCAAjB,EAAsD;EACvCA,2DAAqB,UAACtE,GAAD,EAA2C;IAAU,oBAClFA,GADkF;EAErF,CAFW;AAGd,CAJD,EAAiBsE,qCAAqC,KAArCA,qCAAqC,MAAtD;;AAMA,WAAYC,oBAAZ;;AAAA,WAAYA,oBAAZ,EAAgC;EAC9BA;EACAA;EACAA;EACAA;EACAA;AACD,CAND,EAAYA,oBAAoB,KAApBA,oBAAoB,MAAhC;;AA+DA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACxE,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiBwE,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAgGA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACzE,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiByE,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAoFA,OAAM,IAAWC,mCAAX;;AAAN,WAAiBA,mCAAjB,EAAoD;EACrCA,yDAAqB,UAAC1E,GAAD,EAAyC;IAAU,oBAChFA,GADgF;EAEnF,CAFW;AAGd,CAJD,EAAiB0E,mCAAmC,KAAnCA,mCAAmC,MAApD;;AAiFA,OAAM,IAAWC,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAAC3E,GAAD,EAAsC;IAAU,oBAC7EA,GAD6E;EAEhF,CAFW;AAGd,CAJD,EAAiB2E,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAqBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAC5E,GAAD,EAA6B;IAAU,6BACpEA,GADoE,GAEnEA,GAAG,CAACoC,GAAJ,IAAW;MAAEA,GAAG,EAAEtC;IAAP,CAFwD;EAGvE,CAHW;AAId,CALD,EAAiB8E,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAoEA,OAAM,IAAWC,kCAAX;;AAAN,WAAiBA,kCAAjB,EAAmD;EACpCA,wDAAqB,UAAC7E,GAAD,EAAwC;IAAU,sCAC/EA,GAD+E,GAE9EA,GAAG,CAACgD,qBAAJ,IAA6B;MAC/BA,qBAAqB,EAAE4B,uBAAuB,CAAC/B,kBAAxB,CAA2C7C,GAAG,CAACgD,qBAA/C;IADQ,CAFiD,GAK9EhD,GAAG,CAACiD,oBAAJ,IAA4B;MAC9BA,oBAAoB,EAAER,gCAAgC,CAACI,kBAAjC,CAAoD7C,GAAG,CAACiD,oBAAxD;IADQ,CALkD;EAQlF,CARW;AASd,CAVD,EAAiB4B,kCAAkC,KAAlCA,kCAAkC,MAAnD;;AAqEA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAAC9E,GAAD,EAAoC;IAAU,6BAC3EA,GAD2E,GAE1EA,GAAG,CAACqD,QAAJ,IAAgB;MAAEA,QAAQ,EAAEvD;IAAZ,CAF0D;EAG9E,CAHW;AAId,CALD,EAAiBgF,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAmEA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAAC/E,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiB+E,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AA8CA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAAChF,GAAD,EAA4B;IAAU,sCACnEA,GADmE,GAElEA,GAAG,CAAC8E,8BAAJ,IAAsC;MACxCA,8BAA8B,EAAEA,8BAA8B,CAACjC,kBAA/B,CAC9B7C,GAAG,CAAC8E,8BAD0B;IADQ,CAF4B,GAOlE9E,GAAG,CAAC6E,kCAAJ,IAA0C;MAC5CA,kCAAkC,EAAEA,kCAAkC,CAAChC,kBAAnC,CAClC7C,GAAG,CAAC6E,kCAD8B;IADQ,CAPwB;EAYtE,CAZW;AAad,CAdD,EAAiBG,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAwCA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACjF,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBiF,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAkBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAAClF,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBkF,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AA6FA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACnF,GAAD,EAA+B;IAAU,6BACtEA,GADsE,GAErEA,GAAG,CAACoF,YAAJ,IAAoB;MACtBA,YAAY,EAAEpF,GAAG,CAACoF,YAAJ,CAAiBzC,GAAjB,CAAqB,UAACC,IAAD,EAAK;QAAK,6BAAsB,CAACC,kBAAvB,CAA0CD,IAA1C;MAA+C,CAA9E;IADQ,CAFiD;EAKzE,CALW;AAMd,CAPD,EAAiBuC,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AA4BA,OAAM,IAAWE,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACrF,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBqF,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAaA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACtF,GAAD,EAAkC;IAAU,6BACzEA,GADyE,GAExEA,GAAG,CAACmF,yBAAJ,IAAiC;MACnCA,yBAAyB,EAAEA,yBAAyB,CAACtC,kBAA1B,CAA6C7C,GAAG,CAACmF,yBAAjD;IADQ,CAFuC;EAK5E,CALW;AAMd,CAPD,EAAiBG,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAmEA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACvF,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBuF,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAyFA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACxF,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBwF,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAkFA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACzF,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiByF,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAwCA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC1F,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB0F,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAkBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC3F,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB2F,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AA4BA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAAC5F,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiB4F,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAsBA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAAC7F,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiB6F,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAiBA,OAAM,IAAWC,OAAX;;AAAN,WAAiBA,OAAjB,EAAwB;EACTA,6BAAqB,UAAC9F,GAAD,EAAa;IAAU,oBACpDA,GADoD;EAEvD,CAFW;AAGd,CAJD,EAAiB8F,OAAO,KAAPA,OAAO,MAAxB;;AAkBA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAAC/F,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiB+F,cAAc,KAAdA,cAAc,MAA/B;;AAkBA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAAChG,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiBgG,eAAe,KAAfA,eAAe,MAAhC;;AAqBA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACjG,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBiG,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAkBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAClG,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBkG,mBAAmB,KAAnBA,mBAAmB,MAApC;;AA6BA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACnG,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBmG,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AA2BA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACpG,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBoG,oBAAoB,KAApBA,oBAAoB,MAArC;;AAoBA,OAAM,IAAWC,kCAAX;;AAAN,WAAiBA,kCAAjB,EAAmD;EACpCA,wDAAqB,UAACrG,GAAD,EAAwC;IAAU,oBAC/EA,GAD+E;EAElF,CAFW;AAGd,CAJD,EAAiBqG,kCAAkC,KAAlCA,kCAAkC,MAAnD;;AAQA,OAAM,IAAWC,mCAAX;;AAAN,WAAiBA,mCAAjB,EAAoD;EACrCA,yDAAqB,UAACtG,GAAD,EAAyC;IAAU,oBAChFA,GADgF;EAEnF,CAFW;AAGd,CAJD,EAAiBsG,mCAAmC,KAAnCA,mCAAmC,MAApD;;AAcA,OAAM,IAAWC,iCAAX;;AAAN,WAAiBA,iCAAjB,EAAkD;EACnCA,uDAAqB,UAACvG,GAAD,EAAuC;IAAU,oBAC9EA,GAD8E;EAEjF,CAFW;AAGd,CAJD,EAAiBuG,iCAAiC,KAAjCA,iCAAiC,MAAlD;;AAQA,OAAM,IAAWC,kCAAX;;AAAN,WAAiBA,kCAAjB,EAAmD;EACpCA,wDAAqB,UAACxG,GAAD,EAAwC;IAAU,oBAC/EA,GAD+E;EAElF,CAFW;AAGd,CAJD,EAAiBwG,kCAAkC,KAAlCA,kCAAkC,MAAnD;;AAkBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACzG,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiByG,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAQA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAC1G,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiB0G,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAmBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC3G,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB2G,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAQA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC5G,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB4G,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAmEA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAAC7G,GAAD,EAAmC;IAAU,sCAC1EA,GAD0E,GAEzEA,GAAG,CAACgD,qBAAJ,IAA6B;MAC/BA,qBAAqB,EAAEb,yBAAyB,CAACU,kBAA1B,CAA6C7C,GAAG,CAACgD,qBAAjD;IADQ,CAF4C,GAKzEhD,GAAG,CAACiD,oBAAJ,IAA4B;MAC9BA,oBAAoB,EAAER,gCAAgC,CAACI,kBAAjC,CAAoD7C,GAAG,CAACiD,oBAAxD;IADQ,CAL6C;EAQ7E,CARW;AASd,CAVD,EAAiB4D,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AA+EA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC9G,GAAD,EAA+B;IAAU,sCACtEA,GADsE,GAErEA,GAAG,CAACqD,QAAJ,IAAgB;MAAEA,QAAQ,EAAEvD;IAAZ,CAFqD,GAGrEE,GAAG,CAACsD,QAAJ,IAAgB;MAAEA,QAAQ,EAAExD;IAAZ,CAHqD;EAIzE,CAJW;AAKd,CAND,EAAiBgH,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAuEA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAC/G,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiB+G,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AA2DA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAAChH,GAAD,EAA4B;IAAU,sCACnEA,GADmE,GAElEA,GAAG,CAAC8G,yBAAJ,IAAiC;MACnCA,yBAAyB,EAAEA,yBAAyB,CAACjE,kBAA1B,CAA6C7C,GAAG,CAAC8G,yBAAjD;IADQ,CAFiC,GAKlE9G,GAAG,CAAC6G,6BAAJ,IAAqC;MACvCA,6BAA6B,EAAEA,6BAA6B,CAAChE,kBAA9B,CAC7B7C,GAAG,CAAC6G,6BADyB;IADQ,CAL6B;EAUtE,CAVW;AAWd,CAZD,EAAiBG,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAgBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACjH,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBiH,uBAAuB,KAAvBA,uBAAuB,MAAxC","names":["SENSITIVE_STRING","BufferingHints","obj","CloudWatchLoggingOptions","CompressionFormat","ConcurrentModificationException","ContentEncoding","CopyCommand","KeyType","DeliveryStreamEncryptionConfigurationInput","ElasticsearchBufferingHints","ProcessorParameterName","ProcessorParameter","Processor","ProcessingConfiguration","ElasticsearchRetryOptions","KMSEncryptionConfig","EncryptionConfiguration","S3DestinationConfiguration","VpcConfiguration","ElasticsearchDestinationConfiguration","HiveJsonSerDe","OpenXJsonSerDe","Deserializer","InputFormatConfiguration","OrcCompression","OrcFormatVersion","OrcSerDe","ParquetCompression","ParquetWriterVersion","ParquetSerDe","Serializer","OutputFormatConfiguration","SchemaConfiguration","DataFormatConversionConfiguration","ExtendedS3DestinationConfiguration","HttpEndpointBufferingHints","HttpEndpointConfiguration","Url","AccessKey","HttpEndpointCommonAttribute","AttributeName","AttributeValue","HttpEndpointRequestConfiguration","CommonAttributes","map","item","filterSensitiveLog","HttpEndpointRetryOptions","HttpEndpointDestinationConfiguration","EndpointConfiguration","RequestConfiguration","KinesisStreamSourceConfiguration","RedshiftRetryOptions","RedshiftDestinationConfiguration","Username","Password","SplunkRetryOptions","SplunkDestinationConfiguration","Tag","CreateDeliveryStreamInput","CreateDeliveryStreamOutput","InvalidArgumentException","InvalidKMSResourceException","LimitExceededException","ResourceInUseException","DeleteDeliveryStreamInput","DeleteDeliveryStreamOutput","ResourceNotFoundException","DeliveryStreamFailureType","FailureDescription","DeliveryStreamEncryptionStatus","DeliveryStreamEncryptionConfiguration","DeliveryStreamStatus","S3DestinationDescription","VpcConfigurationDescription","ElasticsearchDestinationDescription","ExtendedS3DestinationDescription","HttpEndpointDescription","HttpEndpointDestinationDescription","RedshiftDestinationDescription","SplunkDestinationDescription","DestinationDescription","KinesisStreamSourceDescription","SourceDescription","DeliveryStreamDescription","Destinations","DescribeDeliveryStreamInput","DescribeDeliveryStreamOutput","S3DestinationUpdate","ElasticsearchDestinationUpdate","ExtendedS3DestinationUpdate","ListDeliveryStreamsInput","ListDeliveryStreamsOutput","ListTagsForDeliveryStreamInput","ListTagsForDeliveryStreamOutput","_Record","PutRecordInput","PutRecordOutput","ServiceUnavailableException","PutRecordBatchInput","PutRecordBatchResponseEntry","PutRecordBatchOutput","StartDeliveryStreamEncryptionInput","StartDeliveryStreamEncryptionOutput","StopDeliveryStreamEncryptionInput","StopDeliveryStreamEncryptionOutput","TagDeliveryStreamInput","TagDeliveryStreamOutput","UntagDeliveryStreamInput","UntagDeliveryStreamOutput","HttpEndpointDestinationUpdate","RedshiftDestinationUpdate","SplunkDestinationUpdate","UpdateDestinationInput","UpdateDestinationOutput"],"sources":["C:\\Users\\jacob\\OneDrive\\College\\github\\resumeprojectsrepo\\resumeprojects\\EliteManaging\\node_modules\\@aws-sdk\\client-firehose\\models\\models_0.ts"],"sourcesContent":["import { SENSITIVE_STRING, SmithyException as __SmithyException } from \"@aws-sdk/smithy-client\";\nimport { MetadataBearer as $MetadataBearer } from \"@aws-sdk/types\";\n\n/**\n * <p>Describes hints for the buffering to perform before delivering data to the\n *          destination. These options are treated as hints, and therefore Kinesis Data Firehose might\n *          choose to use different values when it is optimal. The <code>SizeInMBs</code> and\n *             <code>IntervalInSeconds</code> parameters are optional. However, if specify a value for\n *          one of them, you must also provide a value for the other.</p>\n */\nexport interface BufferingHints {\n  /**\n   * <p>Buffer incoming data to the specified size, in MiBs, before delivering it to the\n   *          destination. The default value is 5. This parameter is optional but if you specify a value\n   *          for it, you must also specify a value for <code>IntervalInSeconds</code>, and vice\n   *          versa.</p>\n   *          <p>We recommend setting this parameter to a value greater than the amount of data you\n   *          typically ingest into the delivery stream in 10 seconds. For example, if you typically\n   *          ingest data at 1 MiB/sec, the value should be 10 MiB or higher.</p>\n   */\n  SizeInMBs?: number;\n\n  /**\n   * <p>Buffer incoming data for the specified period of time, in seconds, before delivering\n   *          it to the destination. The default value is 300. This parameter is optional but if you\n   *          specify a value for it, you must also specify a value for <code>SizeInMBs</code>, and vice\n   *          versa.</p>\n   */\n  IntervalInSeconds?: number;\n}\n\nexport namespace BufferingHints {\n  export const filterSensitiveLog = (obj: BufferingHints): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the Amazon CloudWatch logging options for your delivery stream.</p>\n */\nexport interface CloudWatchLoggingOptions {\n  /**\n   * <p>Enables or disables CloudWatch logging.</p>\n   */\n  Enabled?: boolean;\n\n  /**\n   * <p>The CloudWatch group name for logging. This value is required if CloudWatch logging\n   *          is enabled.</p>\n   */\n  LogGroupName?: string;\n\n  /**\n   * <p>The CloudWatch log stream name for logging. This value is required if CloudWatch\n   *          logging is enabled.</p>\n   */\n  LogStreamName?: string;\n}\n\nexport namespace CloudWatchLoggingOptions {\n  export const filterSensitiveLog = (obj: CloudWatchLoggingOptions): any => ({\n    ...obj,\n  });\n}\n\nexport enum CompressionFormat {\n  GZIP = \"GZIP\",\n  HADOOP_SNAPPY = \"HADOOP_SNAPPY\",\n  SNAPPY = \"Snappy\",\n  UNCOMPRESSED = \"UNCOMPRESSED\",\n  ZIP = \"ZIP\",\n}\n\n/**\n * <p>Another modification has already happened. Fetch <code>VersionId</code> again and use\n *          it to update the destination.</p>\n */\nexport interface ConcurrentModificationException extends __SmithyException, $MetadataBearer {\n  name: \"ConcurrentModificationException\";\n  $fault: \"client\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace ConcurrentModificationException {\n  export const filterSensitiveLog = (obj: ConcurrentModificationException): any => ({\n    ...obj,\n  });\n}\n\nexport enum ContentEncoding {\n  GZIP = \"GZIP\",\n  NONE = \"NONE\",\n}\n\n/**\n * <p>Describes a <code>COPY</code> command for Amazon Redshift.</p>\n */\nexport interface CopyCommand {\n  /**\n   * <p>The name of the target table. The table must already exist in the database.</p>\n   */\n  DataTableName: string | undefined;\n\n  /**\n   * <p>A comma-separated list of column names.</p>\n   */\n  DataTableColumns?: string;\n\n  /**\n   * <p>Optional parameters to use with the Amazon Redshift <code>COPY</code> command. For\n   *          more information, see the \"Optional Parameters\" section of <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\">Amazon Redshift COPY command</a>. Some possible\n   *          examples that would apply to Kinesis Data Firehose are as follows:</p>\n   *          <p>\n   *             <code>delimiter '\\t' lzop;</code> - fields are delimited with \"\\t\" (TAB character) and\n   *          compressed using lzop.</p>\n   *          <p>\n   *             <code>delimiter '|'</code> - fields are delimited with \"|\" (this is the default\n   *          delimiter).</p>\n   *          <p>\n   *             <code>delimiter '|' escape</code> - the delimiter should be escaped.</p>\n   *          <p>\n   *             <code>fixedwidth 'venueid:3,venuename:25,venuecity:12,venuestate:2,venueseats:6'</code> -\n   *          fields are fixed width in the source, with each width specified after every column in the\n   *          table.</p>\n   *          <p>\n   *             <code>JSON 's3://mybucket/jsonpaths.txt'</code> - data is in JSON format, and the path\n   *          specified is the format of the data.</p>\n   *          <p>For more examples, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_COPY_command_examples.html\">Amazon Redshift COPY command\n   *             examples</a>.</p>\n   */\n  CopyOptions?: string;\n}\n\nexport namespace CopyCommand {\n  export const filterSensitiveLog = (obj: CopyCommand): any => ({\n    ...obj,\n  });\n}\n\nexport enum KeyType {\n  AWS_OWNED_CMK = \"AWS_OWNED_CMK\",\n  CUSTOMER_MANAGED_CMK = \"CUSTOMER_MANAGED_CMK\",\n}\n\n/**\n * <p>Specifies the type and Amazon Resource Name (ARN) of the CMK to use for Server-Side\n *          Encryption (SSE). </p>\n */\nexport interface DeliveryStreamEncryptionConfigurationInput {\n  /**\n   * <p>If you set <code>KeyType</code> to <code>CUSTOMER_MANAGED_CMK</code>, you must specify\n   *          the Amazon Resource Name (ARN) of the CMK. If you set <code>KeyType</code> to\n   *             <code>AWS_OWNED_CMK</code>, Kinesis Data Firehose uses a service-account CMK.</p>\n   */\n  KeyARN?: string;\n\n  /**\n   * <p>Indicates the type of customer master key (CMK) to use for encryption. The default\n   *          setting is <code>AWS_OWNED_CMK</code>. For more information about CMKs, see <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">Customer Master Keys (CMKs)</a>. When you invoke <a>CreateDeliveryStream</a> or <a>StartDeliveryStreamEncryption</a> with\n   *             <code>KeyType</code> set to CUSTOMER_MANAGED_CMK, Kinesis Data Firehose invokes the\n   *          Amazon KMS operation <a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_CreateGrant.html\">CreateGrant</a> to create a grant that allows the Kinesis Data Firehose service to\n   *          use the customer managed CMK to perform encryption and decryption. Kinesis Data Firehose\n   *          manages that grant. </p>\n   *          <p>When you invoke <a>StartDeliveryStreamEncryption</a> to change the CMK for a\n   *          delivery stream that is encrypted with a customer managed CMK, Kinesis Data Firehose\n   *          schedules the grant it had on the old CMK for retirement.</p>\n   *          <p>You can use a CMK of type CUSTOMER_MANAGED_CMK to encrypt up to 500 delivery streams. If\n   *          a <a>CreateDeliveryStream</a> or <a>StartDeliveryStreamEncryption</a>\n   *          operation exceeds this limit, Kinesis Data Firehose throws a\n   *             <code>LimitExceededException</code>. </p>\n   *          <important>\n   *             <p>To encrypt your delivery stream, use symmetric CMKs. Kinesis Data Firehose doesn't\n   *             support asymmetric CMKs. For information about symmetric and asymmetric CMKs, see <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/symm-asymm-concepts.html\">About Symmetric and Asymmetric CMKs</a> in the AWS Key Management Service\n   *             developer guide.</p>\n   *          </important>\n   */\n  KeyType: KeyType | string | undefined;\n}\n\nexport namespace DeliveryStreamEncryptionConfigurationInput {\n  export const filterSensitiveLog = (obj: DeliveryStreamEncryptionConfigurationInput): any => ({\n    ...obj,\n  });\n}\n\nexport type DeliveryStreamType = \"DirectPut\" | \"KinesisStreamAsSource\";\n\n/**\n * <p>Describes the buffering to perform before delivering data to the Amazon ES\n *          destination.</p>\n */\nexport interface ElasticsearchBufferingHints {\n  /**\n   * <p>Buffer incoming data for the specified period of time, in seconds, before delivering\n   *          it to the destination. The default value is 300 (5 minutes).</p>\n   */\n  IntervalInSeconds?: number;\n\n  /**\n   * <p>Buffer incoming data to the specified size, in MBs, before delivering it to the\n   *          destination. The default value is 5.</p>\n   *          <p>We recommend setting this parameter to a value greater than the amount of data you\n   *          typically ingest into the delivery stream in 10 seconds. For example, if you typically\n   *          ingest data at 1 MB/sec, the value should be 10 MB or higher.</p>\n   */\n  SizeInMBs?: number;\n}\n\nexport namespace ElasticsearchBufferingHints {\n  export const filterSensitiveLog = (obj: ElasticsearchBufferingHints): any => ({\n    ...obj,\n  });\n}\n\nexport type ElasticsearchIndexRotationPeriod = \"NoRotation\" | \"OneDay\" | \"OneHour\" | \"OneMonth\" | \"OneWeek\";\n\nexport enum ProcessorParameterName {\n  BUFFER_INTERVAL_IN_SECONDS = \"BufferIntervalInSeconds\",\n  BUFFER_SIZE_IN_MB = \"BufferSizeInMBs\",\n  LAMBDA_ARN = \"LambdaArn\",\n  LAMBDA_NUMBER_OF_RETRIES = \"NumberOfRetries\",\n  ROLE_ARN = \"RoleArn\",\n}\n\n/**\n * <p>Describes the processor parameter.</p>\n */\nexport interface ProcessorParameter {\n  /**\n   * <p>The name of the parameter.</p>\n   */\n  ParameterName: ProcessorParameterName | string | undefined;\n\n  /**\n   * <p>The parameter value.</p>\n   */\n  ParameterValue: string | undefined;\n}\n\nexport namespace ProcessorParameter {\n  export const filterSensitiveLog = (obj: ProcessorParameter): any => ({\n    ...obj,\n  });\n}\n\nexport type ProcessorType = \"Lambda\";\n\n/**\n * <p>Describes a data processor.</p>\n */\nexport interface Processor {\n  /**\n   * <p>The type of processor.</p>\n   */\n  Type: ProcessorType | string | undefined;\n\n  /**\n   * <p>The processor parameters.</p>\n   */\n  Parameters?: ProcessorParameter[];\n}\n\nexport namespace Processor {\n  export const filterSensitiveLog = (obj: Processor): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes a data processing configuration.</p>\n */\nexport interface ProcessingConfiguration {\n  /**\n   * <p>Enables or disables data processing.</p>\n   */\n  Enabled?: boolean;\n\n  /**\n   * <p>The data processors.</p>\n   */\n  Processors?: Processor[];\n}\n\nexport namespace ProcessingConfiguration {\n  export const filterSensitiveLog = (obj: ProcessingConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Configures retry behavior in case Kinesis Data Firehose is unable to deliver\n *          documents to Amazon ES.</p>\n */\nexport interface ElasticsearchRetryOptions {\n  /**\n   * <p>After an initial failure to deliver to Amazon ES, the total amount of time during\n   *          which Kinesis Data Firehose retries delivery (including the first attempt). After this time\n   *          has elapsed, the failed documents are written to Amazon S3. Default value is 300 seconds (5\n   *          minutes). A value of 0 (zero) results in no retries.</p>\n   */\n  DurationInSeconds?: number;\n}\n\nexport namespace ElasticsearchRetryOptions {\n  export const filterSensitiveLog = (obj: ElasticsearchRetryOptions): any => ({\n    ...obj,\n  });\n}\n\nexport type ElasticsearchS3BackupMode = \"AllDocuments\" | \"FailedDocumentsOnly\";\n\n/**\n * <p>Describes an encryption key for a destination in Amazon S3.</p>\n */\nexport interface KMSEncryptionConfig {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the encryption key. Must belong to the same AWS\n   *          Region as the destination Amazon S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  AWSKMSKeyARN: string | undefined;\n}\n\nexport namespace KMSEncryptionConfig {\n  export const filterSensitiveLog = (obj: KMSEncryptionConfig): any => ({\n    ...obj,\n  });\n}\n\nexport type NoEncryptionConfig = \"NoEncryption\";\n\n/**\n * <p>Describes the encryption for a destination in Amazon S3.</p>\n */\nexport interface EncryptionConfiguration {\n  /**\n   * <p>Specifically override existing encryption information to ensure that no encryption is\n   *          used.</p>\n   */\n  NoEncryptionConfig?: NoEncryptionConfig | string;\n\n  /**\n   * <p>The encryption key.</p>\n   */\n  KMSEncryptionConfig?: KMSEncryptionConfig;\n}\n\nexport namespace EncryptionConfiguration {\n  export const filterSensitiveLog = (obj: EncryptionConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the configuration of a destination in Amazon S3.</p>\n */\nexport interface S3DestinationConfiguration {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN: string | undefined;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option. If no value is specified, <code>BufferingHints</code> object\n   *          default values are used.</p>\n   */\n  BufferingHints?: BufferingHints;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *             <code>UNCOMPRESSED</code>.</p>\n   *          <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified\n   *          for Amazon Redshift destinations because they are not supported by the Amazon Redshift\n   *             <code>COPY</code> operation that reads from the S3 bucket.</p>\n   */\n  CompressionFormat?: CompressionFormat | string;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration?: EncryptionConfiguration;\n\n  /**\n   * <p>The CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace S3DestinationConfiguration {\n  export const filterSensitiveLog = (obj: S3DestinationConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The details of the VPC of the Amazon ES destination.</p>\n */\nexport interface VpcConfiguration {\n  /**\n   * <p>The IDs of the subnets that you want Kinesis Data Firehose to use to create ENIs in the\n   *          VPC of the Amazon ES destination. Make sure that the routing tables and inbound and\n   *          outbound rules allow traffic to flow from the subnets whose IDs are specified here to the\n   *          subnets that have the destination Amazon ES endpoints. Kinesis Data Firehose creates at\n   *          least one ENI in each of the subnets that are specified here. Do not delete or modify these\n   *          ENIs.</p>\n   *          <p>The number of ENIs that Kinesis Data Firehose creates in the subnets specified here\n   *          scales up and down automatically based on throughput. To enable Kinesis Data Firehose to\n   *          scale up the number of ENIs to match throughput, ensure that you have sufficient quota. To\n   *          help you calculate the quota you need, assume that Kinesis Data Firehose can create up to\n   *          three ENIs for this delivery stream for each of the subnets specified here. For more\n   *          information about ENI quota, see <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-enis\">Network Interfaces </a> in the Amazon VPC Quotas topic.</p>\n   */\n  SubnetIds: string[] | undefined;\n\n  /**\n   * <p>The ARN of the IAM role that you want the delivery stream to use to create endpoints in\n   *          the destination VPC. You can use your existing Kinesis Data Firehose delivery role or you\n   *          can specify a new role. In either case, make sure that the role trusts the Kinesis Data\n   *          Firehose service principal and that it grants the following permissions:</p>\n   *          <ul>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeVpcs</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeVpcAttribute</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeSubnets</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeSecurityGroups</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeNetworkInterfaces</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:CreateNetworkInterface</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:CreateNetworkInterfacePermission</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DeleteNetworkInterface</code>\n   *                </p>\n   *             </li>\n   *          </ul>\n   *          <p>If you revoke these permissions after you create the delivery stream, Kinesis Data\n   *          Firehose can't scale out by creating more ENIs when necessary. You might therefore see a\n   *          degradation in performance.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The IDs of the security groups that you want Kinesis Data Firehose to use when it\n   *          creates ENIs in the VPC of the Amazon ES destination. You can use the same security group\n   *          that the Amazon ES domain uses or different ones. If you specify different security groups\n   *          here, ensure that they allow outbound HTTPS traffic to the Amazon ES domain's security\n   *          group. Also ensure that the Amazon ES domain's security group allows HTTPS traffic from the\n   *          security groups specified here. If you use the same security group for both your delivery\n   *          stream and the Amazon ES domain, make sure the security group inbound rule allows HTTPS\n   *          traffic. For more information about security group rules, see <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#SecurityGroupRules\">Security group rules</a> in the Amazon VPC documentation.</p>\n   */\n  SecurityGroupIds: string[] | undefined;\n}\n\nexport namespace VpcConfiguration {\n  export const filterSensitiveLog = (obj: VpcConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the configuration of a destination in Amazon ES.</p>\n */\nexport interface ElasticsearchDestinationConfiguration {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the IAM role to be assumed by Kinesis Data Firehose\n   *          for calling the Amazon ES Configuration API and for indexing documents. For more\n   *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n   *             Firehose Access to an Amazon S3 Destination</a> and <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The ARN of the Amazon ES domain. The IAM role must have permissions\n   *             for <code>DescribeElasticsearchDomain</code>, <code>DescribeElasticsearchDomains</code>,\n   *          and <code>DescribeElasticsearchDomainConfig</code> after assuming the role specified in\n   *             <b>RoleARN</b>. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   *\n   *          <p>Specify either <code>ClusterEndpoint</code> or <code>DomainARN</code>.</p>\n   */\n  DomainARN?: string;\n\n  /**\n   * <p>The endpoint to use when communicating with the cluster. Specify either this\n   *             <code>ClusterEndpoint</code> or the <code>DomainARN</code> field.</p>\n   */\n  ClusterEndpoint?: string;\n\n  /**\n   * <p>The Elasticsearch index name.</p>\n   */\n  IndexName: string | undefined;\n\n  /**\n   * <p>The Elasticsearch type name. For Elasticsearch 6.x, there can be only one type per\n   *          index. If you try to specify a new type for an existing index that already has another\n   *          type, Kinesis Data Firehose returns an error during run time.</p>\n   *\n   *          <p>For Elasticsearch 7.x, don't specify a <code>TypeName</code>.</p>\n   */\n  TypeName?: string;\n\n  /**\n   * <p>The Elasticsearch index rotation period. Index rotation appends a timestamp to the\n   *             <code>IndexName</code> to facilitate the expiration of old data. For more information,\n   *          see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-index-rotation\">Index Rotation for the\n   *             Amazon ES Destination</a>. The default value is <code>OneDay</code>.</p>\n   */\n  IndexRotationPeriod?: ElasticsearchIndexRotationPeriod | string;\n\n  /**\n   * <p>The buffering options. If no value is specified, the default values for\n   *             <code>ElasticsearchBufferingHints</code> are used.</p>\n   */\n  BufferingHints?: ElasticsearchBufferingHints;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver documents to\n   *          Amazon ES. The default value is 300 (5 minutes).</p>\n   */\n  RetryOptions?: ElasticsearchRetryOptions;\n\n  /**\n   * <p>Defines how documents should be delivered to Amazon S3. When it is set to\n   *             <code>FailedDocumentsOnly</code>, Kinesis Data Firehose writes any documents that could\n   *          not be indexed to the configured Amazon S3 destination, with\n   *             <code>elasticsearch-failed/</code> appended to the key prefix. When set to\n   *             <code>AllDocuments</code>, Kinesis Data Firehose delivers all incoming records to Amazon\n   *          S3, and also writes failed documents with <code>elasticsearch-failed/</code> appended to\n   *          the prefix. For more information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-s3-backup\">Amazon S3 Backup for the\n   *             Amazon ES Destination</a>. Default value is\n   *          <code>FailedDocumentsOnly</code>.</p>\n   *          <p>You can't change this backup mode after you create the delivery stream. </p>\n   */\n  S3BackupMode?: ElasticsearchS3BackupMode | string;\n\n  /**\n   * <p>The configuration for the backup Amazon S3 location.</p>\n   */\n  S3Configuration: S3DestinationConfiguration | undefined;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The details of the VPC of the Amazon ES destination.</p>\n   */\n  VpcConfiguration?: VpcConfiguration;\n}\n\nexport namespace ElasticsearchDestinationConfiguration {\n  export const filterSensitiveLog = (obj: ElasticsearchDestinationConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The native Hive / HCatalog JsonSerDe. Used by Kinesis Data Firehose for deserializing\n *          data, which means converting it from the JSON format in preparation for serializing it to\n *          the Parquet or ORC format. This is one of two deserializers you can choose, depending on\n *          which one offers the functionality you need. The other option is the OpenX SerDe.</p>\n */\nexport interface HiveJsonSerDe {\n  /**\n   * <p>Indicates how you want Kinesis Data Firehose to parse the date and timestamps that\n   *          may be present in your input data JSON. To specify these format strings, follow the pattern\n   *          syntax of JodaTime's DateTimeFormat format strings. For more information, see <a href=\"https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html\">Class DateTimeFormat</a>. You can also use the special value <code>millis</code> to\n   *          parse timestamps in epoch milliseconds. If you don't specify a format, Kinesis Data\n   *          Firehose uses <code>java.sql.Timestamp::valueOf</code> by default.</p>\n   */\n  TimestampFormats?: string[];\n}\n\nexport namespace HiveJsonSerDe {\n  export const filterSensitiveLog = (obj: HiveJsonSerDe): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The OpenX SerDe. Used by Kinesis Data Firehose for deserializing data, which means\n *          converting it from the JSON format in preparation for serializing it to the Parquet or ORC\n *          format. This is one of two deserializers you can choose, depending on which one offers the\n *          functionality you need. The other option is the native Hive / HCatalog JsonSerDe.</p>\n */\nexport interface OpenXJsonSerDe {\n  /**\n   * <p>When set to <code>true</code>, specifies that the names of the keys include dots and\n   *          that you want Kinesis Data Firehose to replace them with underscores. This is useful\n   *          because Apache Hive does not allow dots in column names. For example, if the JSON contains\n   *          a key whose name is \"a.b\", you can define the column name to be \"a_b\" when using this\n   *          option.</p>\n   *          <p>The default is <code>false</code>.</p>\n   */\n  ConvertDotsInJsonKeysToUnderscores?: boolean;\n\n  /**\n   * <p>When set to <code>true</code>, which is the default, Kinesis Data Firehose converts\n   *          JSON keys to lowercase before deserializing them.</p>\n   */\n  CaseInsensitive?: boolean;\n\n  /**\n   * <p>Maps column names to JSON keys that aren't identical to the column names. This is\n   *          useful when the JSON contains keys that are Hive keywords. For example,\n   *             <code>timestamp</code> is a Hive keyword. If you have a JSON key named\n   *             <code>timestamp</code>, set this parameter to <code>{\"ts\": \"timestamp\"}</code> to map\n   *          this key to a column named <code>ts</code>.</p>\n   */\n  ColumnToJsonKeyMappings?: { [key: string]: string };\n}\n\nexport namespace OpenXJsonSerDe {\n  export const filterSensitiveLog = (obj: OpenXJsonSerDe): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The deserializer you want Kinesis Data Firehose to use for converting the input data\n *          from JSON. Kinesis Data Firehose then serializes the data to its final format using the\n *             <a>Serializer</a>. Kinesis Data Firehose supports two types of deserializers:\n *          the <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-JSON\">Apache Hive JSON SerDe</a> and the <a href=\"https://github.com/rcongiu/Hive-JSON-Serde\">OpenX JSON SerDe</a>.</p>\n */\nexport interface Deserializer {\n  /**\n   * <p>The OpenX SerDe. Used by Kinesis Data Firehose for deserializing data, which means\n   *          converting it from the JSON format in preparation for serializing it to the Parquet or ORC\n   *          format. This is one of two deserializers you can choose, depending on which one offers the\n   *          functionality you need. The other option is the native Hive / HCatalog JsonSerDe.</p>\n   */\n  OpenXJsonSerDe?: OpenXJsonSerDe;\n\n  /**\n   * <p>The native Hive / HCatalog JsonSerDe. Used by Kinesis Data Firehose for deserializing\n   *          data, which means converting it from the JSON format in preparation for serializing it to\n   *          the Parquet or ORC format. This is one of two deserializers you can choose, depending on\n   *          which one offers the functionality you need. The other option is the OpenX SerDe.</p>\n   */\n  HiveJsonSerDe?: HiveJsonSerDe;\n}\n\nexport namespace Deserializer {\n  export const filterSensitiveLog = (obj: Deserializer): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies the deserializer you want to use to convert the format of the input data.\n *          This parameter is required if <code>Enabled</code> is set to true.</p>\n */\nexport interface InputFormatConfiguration {\n  /**\n   * <p>Specifies which deserializer to use. You can choose either the Apache Hive JSON SerDe\n   *          or the OpenX JSON SerDe. If both are non-null, the server rejects the request.</p>\n   */\n  Deserializer?: Deserializer;\n}\n\nexport namespace InputFormatConfiguration {\n  export const filterSensitiveLog = (obj: InputFormatConfiguration): any => ({\n    ...obj,\n  });\n}\n\nexport enum OrcCompression {\n  NONE = \"NONE\",\n  SNAPPY = \"SNAPPY\",\n  ZLIB = \"ZLIB\",\n}\n\nexport enum OrcFormatVersion {\n  V0_11 = \"V0_11\",\n  V0_12 = \"V0_12\",\n}\n\n/**\n * <p>A serializer to use for converting data to the ORC format before storing it in Amazon\n *          S3. For more information, see <a href=\"https://orc.apache.org/docs/\">Apache\n *          ORC</a>.</p>\n */\nexport interface OrcSerDe {\n  /**\n   * <p>The number of bytes in each stripe. The default is 64 MiB and the minimum is 8\n   *          MiB.</p>\n   */\n  StripeSizeBytes?: number;\n\n  /**\n   * <p>The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to\n   *          copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the\n   *          minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.</p>\n   */\n  BlockSizeBytes?: number;\n\n  /**\n   * <p>The number of rows between index entries. The default is 10,000 and the minimum is\n   *          1,000.</p>\n   */\n  RowIndexStride?: number;\n\n  /**\n   * <p>Set this to <code>true</code> to indicate that you want stripes to be padded to the HDFS\n   *          block boundaries. This is useful if you intend to copy the data from Amazon S3 to HDFS\n   *          before querying. The default is <code>false</code>.</p>\n   */\n  EnablePadding?: boolean;\n\n  /**\n   * <p>A number between 0 and 1 that defines the tolerance for block padding as a decimal\n   *          fraction of stripe size. The default value is 0.05, which means 5 percent of stripe\n   *          size.</p>\n   *          <p>For the default values of 64 MiB ORC stripes and 256 MiB HDFS blocks, the default block\n   *          padding tolerance of 5 percent reserves a maximum of 3.2 MiB for padding within the 256 MiB\n   *          block. In such a case, if the available size within the block is more than 3.2 MiB, a new,\n   *          smaller stripe is inserted to fit within that space. This ensures that no stripe crosses\n   *          block boundaries and causes remote reads within a node-local task.</p>\n   *          <p>Kinesis Data Firehose ignores this parameter when <a>OrcSerDe$EnablePadding</a> is <code>false</code>.</p>\n   */\n  PaddingTolerance?: number;\n\n  /**\n   * <p>The compression code to use over data blocks. The default is <code>SNAPPY</code>.</p>\n   */\n  Compression?: OrcCompression | string;\n\n  /**\n   * <p>The column names for which you want Kinesis Data Firehose to create bloom filters. The\n   *          default is <code>null</code>.</p>\n   */\n  BloomFilterColumns?: string[];\n\n  /**\n   * <p>The Bloom filter false positive probability (FPP). The lower the FPP, the bigger the\n   *          Bloom filter. The default value is 0.05, the minimum is 0, and the maximum is 1.</p>\n   */\n  BloomFilterFalsePositiveProbability?: number;\n\n  /**\n   * <p>Represents the fraction of the total number of non-null rows. To turn off dictionary\n   *          encoding, set this fraction to a number that is less than the number of distinct keys in a\n   *          dictionary. To always use dictionary encoding, set this threshold to 1.</p>\n   */\n  DictionaryKeyThreshold?: number;\n\n  /**\n   * <p>The version of the file to write. The possible values are <code>V0_11</code> and\n   *             <code>V0_12</code>. The default is <code>V0_12</code>.</p>\n   */\n  FormatVersion?: OrcFormatVersion | string;\n}\n\nexport namespace OrcSerDe {\n  export const filterSensitiveLog = (obj: OrcSerDe): any => ({\n    ...obj,\n  });\n}\n\nexport enum ParquetCompression {\n  GZIP = \"GZIP\",\n  SNAPPY = \"SNAPPY\",\n  UNCOMPRESSED = \"UNCOMPRESSED\",\n}\n\nexport enum ParquetWriterVersion {\n  V1 = \"V1\",\n  V2 = \"V2\",\n}\n\n/**\n * <p>A serializer to use for converting data to the Parquet format before storing it in\n *          Amazon S3. For more information, see <a href=\"https://parquet.apache.org/documentation/latest/\">Apache Parquet</a>.</p>\n */\nexport interface ParquetSerDe {\n  /**\n   * <p>The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to\n   *          copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the\n   *          minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.</p>\n   */\n  BlockSizeBytes?: number;\n\n  /**\n   * <p>The Parquet page size. Column chunks are divided into pages. A page is conceptually an\n   *          indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and\n   *          the default is 1 MiB.</p>\n   */\n  PageSizeBytes?: number;\n\n  /**\n   * <p>The compression code to use over data blocks. The possible values are\n   *             <code>UNCOMPRESSED</code>, <code>SNAPPY</code>, and <code>GZIP</code>, with the default\n   *          being <code>SNAPPY</code>. Use <code>SNAPPY</code> for higher decompression speed. Use\n   *             <code>GZIP</code> if the compression ratio is more important than speed.</p>\n   */\n  Compression?: ParquetCompression | string;\n\n  /**\n   * <p>Indicates whether to enable dictionary compression.</p>\n   */\n  EnableDictionaryCompression?: boolean;\n\n  /**\n   * <p>The maximum amount of padding to apply. This is useful if you intend to copy the data\n   *          from Amazon S3 to HDFS before querying. The default is 0.</p>\n   */\n  MaxPaddingBytes?: number;\n\n  /**\n   * <p>Indicates the version of row format to output. The possible values are <code>V1</code>\n   *          and <code>V2</code>. The default is <code>V1</code>.</p>\n   */\n  WriterVersion?: ParquetWriterVersion | string;\n}\n\nexport namespace ParquetSerDe {\n  export const filterSensitiveLog = (obj: ParquetSerDe): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The serializer that you want Kinesis Data Firehose to use to convert data to the target\n *          format before writing it to Amazon S3. Kinesis Data Firehose supports two types of\n *          serializers: the <a href=\"https://hive.apache.org/javadocs/r1.2.2/api/org/apache/hadoop/hive/ql/io/orc/OrcSerde.html\">ORC SerDe</a> and the <a href=\"https://hive.apache.org/javadocs/r1.2.2/api/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.html\">Parquet SerDe</a>.</p>\n */\nexport interface Serializer {\n  /**\n   * <p>A serializer to use for converting data to the Parquet format before storing it in\n   *          Amazon S3. For more information, see <a href=\"https://parquet.apache.org/documentation/latest/\">Apache Parquet</a>.</p>\n   */\n  ParquetSerDe?: ParquetSerDe;\n\n  /**\n   * <p>A serializer to use for converting data to the ORC format before storing it in Amazon\n   *          S3. For more information, see <a href=\"https://orc.apache.org/docs/\">Apache\n   *          ORC</a>.</p>\n   */\n  OrcSerDe?: OrcSerDe;\n}\n\nexport namespace Serializer {\n  export const filterSensitiveLog = (obj: Serializer): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies the serializer that you want Kinesis Data Firehose to use to convert the\n *          format of your data before it writes it to Amazon S3. This parameter is required if\n *             <code>Enabled</code> is set to true.</p>\n */\nexport interface OutputFormatConfiguration {\n  /**\n   * <p>Specifies which serializer to use. You can choose either the ORC SerDe or the Parquet\n   *          SerDe. If both are non-null, the server rejects the request.</p>\n   */\n  Serializer?: Serializer;\n}\n\nexport namespace OutputFormatConfiguration {\n  export const filterSensitiveLog = (obj: OutputFormatConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies the schema to which you want Kinesis Data Firehose to configure your data\n *          before it writes it to Amazon S3. This parameter is required if <code>Enabled</code> is set\n *          to true.</p>\n */\nexport interface SchemaConfiguration {\n  /**\n   * <p>The role that Kinesis Data Firehose can use to access AWS Glue. This role must be in\n   *          the same account you use for Kinesis Data Firehose. Cross-account roles aren't\n   *          allowed.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The ID of the AWS Glue Data Catalog. If you don't supply this, the AWS account ID is\n   *          used by default.</p>\n   */\n  CatalogId?: string;\n\n  /**\n   * <p>Specifies the name of the AWS Glue database that contains the schema for the output\n   *          data.</p>\n   */\n  DatabaseName?: string;\n\n  /**\n   * <p>Specifies the AWS Glue table that contains the column information that constitutes your\n   *          data schema.</p>\n   */\n  TableName?: string;\n\n  /**\n   * <p>If you don't specify an AWS Region, the default is the current Region.</p>\n   */\n  Region?: string;\n\n  /**\n   * <p>Specifies the table version for the output data schema. If you don't specify this\n   *          version ID, or if you set it to <code>LATEST</code>, Kinesis Data Firehose uses the most\n   *          recent version. This means that any updates to the table are automatically picked\n   *          up.</p>\n   */\n  VersionId?: string;\n}\n\nexport namespace SchemaConfiguration {\n  export const filterSensitiveLog = (obj: SchemaConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies that you want Kinesis Data Firehose to convert data from the JSON format to\n *          the Parquet or ORC format before writing it to Amazon S3. Kinesis Data Firehose uses the\n *          serializer and deserializer that you specify, in addition to the column information from\n *          the AWS Glue table, to deserialize your input data from JSON and then serialize it to the\n *          Parquet or ORC format. For more information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\">Kinesis Data Firehose Record Format Conversion</a>.</p>\n */\nexport interface DataFormatConversionConfiguration {\n  /**\n   * <p>Specifies the AWS Glue Data Catalog table that contains the column information. This\n   *          parameter is required if <code>Enabled</code> is set to true.</p>\n   */\n  SchemaConfiguration?: SchemaConfiguration;\n\n  /**\n   * <p>Specifies the deserializer that you want Kinesis Data Firehose to use to convert the\n   *          format of your data from JSON. This parameter is required if <code>Enabled</code> is set to\n   *          true.</p>\n   */\n  InputFormatConfiguration?: InputFormatConfiguration;\n\n  /**\n   * <p>Specifies the serializer that you want Kinesis Data Firehose to use to convert the\n   *          format of your data to the Parquet or ORC format. This parameter is required if\n   *             <code>Enabled</code> is set to true.</p>\n   */\n  OutputFormatConfiguration?: OutputFormatConfiguration;\n\n  /**\n   * <p>Defaults to <code>true</code>. Set it to <code>false</code> if you want to disable\n   *          format conversion while preserving the configuration details.</p>\n   */\n  Enabled?: boolean;\n}\n\nexport namespace DataFormatConversionConfiguration {\n  export const filterSensitiveLog = (obj: DataFormatConversionConfiguration): any => ({\n    ...obj,\n  });\n}\n\nexport type S3BackupMode = \"Disabled\" | \"Enabled\";\n\n/**\n * <p>Describes the configuration of a destination in Amazon S3.</p>\n */\nexport interface ExtendedS3DestinationConfiguration {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN: string | undefined;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option.</p>\n   */\n  BufferingHints?: BufferingHints;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *          UNCOMPRESSED.</p>\n   */\n  CompressionFormat?: CompressionFormat | string;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration?: EncryptionConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon S3 backup mode. After you create a delivery stream, you can update it to\n   *          enable Amazon S3 backup if it is disabled. If backup is enabled, you can't update the\n   *          delivery stream to disable it. </p>\n   */\n  S3BackupMode?: S3BackupMode | string;\n\n  /**\n   * <p>The configuration for backup in Amazon S3.</p>\n   */\n  S3BackupConfiguration?: S3DestinationConfiguration;\n\n  /**\n   * <p>The serializer, deserializer, and schema for converting data from the JSON format to\n   *          the Parquet or ORC format before writing it to Amazon S3.</p>\n   */\n  DataFormatConversionConfiguration?: DataFormatConversionConfiguration;\n}\n\nexport namespace ExtendedS3DestinationConfiguration {\n  export const filterSensitiveLog = (obj: ExtendedS3DestinationConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the buffering options that can be applied before data is delivered to the HTTP\n *          endpoint destination. Kinesis Data Firehose treats these options as hints, and it might\n *          choose to use more optimal values. The <code>SizeInMBs</code> and\n *             <code>IntervalInSeconds</code> parameters are optional. However, if specify a value for\n *          one of them, you must also provide a value for the other. </p>\n */\nexport interface HttpEndpointBufferingHints {\n  /**\n   * <p>Buffer incoming data to the specified size, in MBs, before delivering it to the\n   *          destination. The default value is 5. </p>\n   *          <p>We recommend setting this parameter to a value greater than the amount of data you\n   *          typically ingest into the delivery stream in 10 seconds. For example, if you typically\n   *          ingest data at 1 MB/sec, the value should be 10 MB or higher. </p>\n   */\n  SizeInMBs?: number;\n\n  /**\n   * <p>Buffer incoming data for the specified period of time, in seconds, before delivering it\n   *          to the destination. The default value is 300 (5 minutes). </p>\n   */\n  IntervalInSeconds?: number;\n}\n\nexport namespace HttpEndpointBufferingHints {\n  export const filterSensitiveLog = (obj: HttpEndpointBufferingHints): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the configuration of the HTTP endpoint to which Kinesis Firehose delivers\n *          data.</p>\n */\nexport interface HttpEndpointConfiguration {\n  /**\n   * <p>The URL of the HTTP endpoint selected as the destination.</p>\n   */\n  Url: string | undefined;\n\n  /**\n   * <p>The name of the HTTP endpoint selected as the destination.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The access key required for Kinesis Firehose to authenticate with the HTTP endpoint\n   *          selected as the destination.</p>\n   */\n  AccessKey?: string;\n}\n\nexport namespace HttpEndpointConfiguration {\n  export const filterSensitiveLog = (obj: HttpEndpointConfiguration): any => ({\n    ...obj,\n    ...(obj.Url && { Url: SENSITIVE_STRING }),\n    ...(obj.AccessKey && { AccessKey: SENSITIVE_STRING }),\n  });\n}\n\n/**\n * <p>Describes the metadata that's delivered to the specified HTTP endpoint\n *          destination.</p>\n */\nexport interface HttpEndpointCommonAttribute {\n  /**\n   * <p>The name of the HTTP endpoint common attribute.</p>\n   */\n  AttributeName: string | undefined;\n\n  /**\n   * <p>The value of the HTTP endpoint common attribute.</p>\n   */\n  AttributeValue: string | undefined;\n}\n\nexport namespace HttpEndpointCommonAttribute {\n  export const filterSensitiveLog = (obj: HttpEndpointCommonAttribute): any => ({\n    ...obj,\n    ...(obj.AttributeName && { AttributeName: SENSITIVE_STRING }),\n    ...(obj.AttributeValue && { AttributeValue: SENSITIVE_STRING }),\n  });\n}\n\n/**\n * <p>The configuration of the HTTP endpoint request.</p>\n */\nexport interface HttpEndpointRequestConfiguration {\n  /**\n   * <p>Kinesis Data Firehose uses the content encoding to compress the body of a request before\n   *          sending the request to the destination. For more information, see <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding\">Content-Encoding</a> in MDN Web Docs, the official Mozilla documentation.</p>\n   */\n  ContentEncoding?: ContentEncoding | string;\n\n  /**\n   * <p>Describes the metadata sent to the HTTP endpoint destination.</p>\n   */\n  CommonAttributes?: HttpEndpointCommonAttribute[];\n}\n\nexport namespace HttpEndpointRequestConfiguration {\n  export const filterSensitiveLog = (obj: HttpEndpointRequestConfiguration): any => ({\n    ...obj,\n    ...(obj.CommonAttributes && {\n      CommonAttributes: obj.CommonAttributes.map((item) => HttpEndpointCommonAttribute.filterSensitiveLog(item)),\n    }),\n  });\n}\n\n/**\n * <p>Describes the retry behavior in case Kinesis Data Firehose is unable to deliver data to\n *          the specified HTTP endpoint destination, or if it doesn't receive a valid acknowledgment of\n *          receipt from the specified HTTP endpoint destination.</p>\n */\nexport interface HttpEndpointRetryOptions {\n  /**\n   * <p>The total amount of time that Kinesis Data Firehose spends on retries. This duration\n   *          starts after the initial attempt to send data to the custom destination via HTTPS endpoint\n   *          fails. It doesn't include the periods during which Kinesis Data Firehose waits for\n   *          acknowledgment from the specified destination after each attempt. </p>\n   */\n  DurationInSeconds?: number;\n}\n\nexport namespace HttpEndpointRetryOptions {\n  export const filterSensitiveLog = (obj: HttpEndpointRetryOptions): any => ({\n    ...obj,\n  });\n}\n\nexport type HttpEndpointS3BackupMode = \"AllData\" | \"FailedDataOnly\";\n\n/**\n * <p>Describes the configuration of the HTTP endpoint destination.</p>\n */\nexport interface HttpEndpointDestinationConfiguration {\n  /**\n   * <p>The configuration of the HTTP endpoint selected as the destination.</p>\n   */\n  EndpointConfiguration: HttpEndpointConfiguration | undefined;\n\n  /**\n   * <p>The buffering options that can be used before data is delivered to the specified\n   *          destination. Kinesis Data Firehose treats these options as hints, and it might choose to\n   *          use more optimal values. The <code>SizeInMBs</code> and <code>IntervalInSeconds</code>\n   *          parameters are optional. However, if you specify a value for one of them, you must also\n   *          provide a value for the other. </p>\n   */\n  BufferingHints?: HttpEndpointBufferingHints;\n\n  /**\n   * <p>Describes the Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The configuration of the requeste sent to the HTTP endpoint specified as the\n   *          destination.</p>\n   */\n  RequestConfiguration?: HttpEndpointRequestConfiguration;\n\n  /**\n   * <p>Describes a data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>Kinesis Data Firehose uses this IAM role for all the permissions that the delivery\n   *          stream needs.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>Describes the retry behavior in case Kinesis Data Firehose is unable to deliver data to\n   *          the specified HTTP endpoint destination, or if it doesn't receive a valid acknowledgment of\n   *          receipt from the specified HTTP endpoint destination.</p>\n   */\n  RetryOptions?: HttpEndpointRetryOptions;\n\n  /**\n   * <p>Describes the S3 bucket backup options for the data that Kinesis Data Firehose delivers\n   *          to the HTTP endpoint destination. You can back up all documents (<code>AllData</code>) or\n   *          only the documents that Kinesis Data Firehose could not deliver to the specified HTTP\n   *          endpoint destination (<code>FailedDataOnly</code>).</p>\n   */\n  S3BackupMode?: HttpEndpointS3BackupMode | string;\n\n  /**\n   * <p>Describes the configuration of a destination in Amazon S3.</p>\n   */\n  S3Configuration: S3DestinationConfiguration | undefined;\n}\n\nexport namespace HttpEndpointDestinationConfiguration {\n  export const filterSensitiveLog = (obj: HttpEndpointDestinationConfiguration): any => ({\n    ...obj,\n    ...(obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointConfiguration.filterSensitiveLog(obj.EndpointConfiguration),\n    }),\n    ...(obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration),\n    }),\n  });\n}\n\n/**\n * <p>The stream and role Amazon Resource Names (ARNs) for a Kinesis data stream used as\n *          the source for a delivery stream.</p>\n */\nexport interface KinesisStreamSourceConfiguration {\n  /**\n   * <p>The ARN of the source Kinesis data stream. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-kinesis-streams\">Amazon Kinesis Data Streams ARN Format</a>.</p>\n   */\n  KinesisStreamARN: string | undefined;\n\n  /**\n   * <p>The ARN of the role that provides access to the source Kinesis data stream. For more\n   *          information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-iam\">AWS Identity and Access Management (IAM) ARN Format</a>.</p>\n   */\n  RoleARN: string | undefined;\n}\n\nexport namespace KinesisStreamSourceConfiguration {\n  export const filterSensitiveLog = (obj: KinesisStreamSourceConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Configures retry behavior in case Kinesis Data Firehose is unable to deliver\n *          documents to Amazon Redshift.</p>\n */\nexport interface RedshiftRetryOptions {\n  /**\n   * <p>The length of time during which Kinesis Data Firehose retries delivery after a\n   *          failure, starting from the initial request and including the first attempt. The default\n   *          value is 3600 seconds (60 minutes). Kinesis Data Firehose does not retry if the value of\n   *             <code>DurationInSeconds</code> is 0 (zero) or if the first delivery attempt takes longer\n   *          than the current value.</p>\n   */\n  DurationInSeconds?: number;\n}\n\nexport namespace RedshiftRetryOptions {\n  export const filterSensitiveLog = (obj: RedshiftRetryOptions): any => ({\n    ...obj,\n  });\n}\n\nexport type RedshiftS3BackupMode = \"Disabled\" | \"Enabled\";\n\n/**\n * <p>Describes the configuration of a destination in Amazon Redshift.</p>\n */\nexport interface RedshiftDestinationConfiguration {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The database connection string.</p>\n   */\n  ClusterJDBCURL: string | undefined;\n\n  /**\n   * <p>The <code>COPY</code> command.</p>\n   */\n  CopyCommand: CopyCommand | undefined;\n\n  /**\n   * <p>The name of the user.</p>\n   */\n  Username: string | undefined;\n\n  /**\n   * <p>The user password.</p>\n   */\n  Password: string | undefined;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver documents to\n   *          Amazon Redshift. Default value is 3600 (60 minutes).</p>\n   */\n  RetryOptions?: RedshiftRetryOptions;\n\n  /**\n   * <p>The configuration for the intermediate Amazon S3 location from which Amazon Redshift\n   *          obtains data. Restrictions are described in the topic for <a>CreateDeliveryStream</a>.</p>\n   *          <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified\n   *          in <code>RedshiftDestinationConfiguration.S3Configuration</code> because the Amazon\n   *          Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't support these\n   *          compression formats.</p>\n   */\n  S3Configuration: S3DestinationConfiguration | undefined;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon S3 backup mode. After you create a delivery stream, you can update it to\n   *          enable Amazon S3 backup if it is disabled. If backup is enabled, you can't update the\n   *          delivery stream to disable it. </p>\n   */\n  S3BackupMode?: RedshiftS3BackupMode | string;\n\n  /**\n   * <p>The configuration for backup in Amazon S3.</p>\n   */\n  S3BackupConfiguration?: S3DestinationConfiguration;\n\n  /**\n   * <p>The CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace RedshiftDestinationConfiguration {\n  export const filterSensitiveLog = (obj: RedshiftDestinationConfiguration): any => ({\n    ...obj,\n    ...(obj.Username && { Username: SENSITIVE_STRING }),\n    ...(obj.Password && { Password: SENSITIVE_STRING }),\n  });\n}\n\nexport type HECEndpointType = \"Event\" | \"Raw\";\n\n/**\n * <p>Configures retry behavior in case Kinesis Data Firehose is unable to deliver\n *          documents to Splunk, or if it doesn't receive an acknowledgment from Splunk.</p>\n */\nexport interface SplunkRetryOptions {\n  /**\n   * <p>The total amount of time that Kinesis Data Firehose spends on retries. This duration\n   *          starts after the initial attempt to send data to Splunk fails. It doesn't include the\n   *          periods during which Kinesis Data Firehose waits for acknowledgment from Splunk after each\n   *          attempt.</p>\n   */\n  DurationInSeconds?: number;\n}\n\nexport namespace SplunkRetryOptions {\n  export const filterSensitiveLog = (obj: SplunkRetryOptions): any => ({\n    ...obj,\n  });\n}\n\nexport type SplunkS3BackupMode = \"AllEvents\" | \"FailedEventsOnly\";\n\n/**\n * <p>Describes the configuration of a destination in Splunk.</p>\n */\nexport interface SplunkDestinationConfiguration {\n  /**\n   * <p>The HTTP Event Collector (HEC) endpoint to which Kinesis Data Firehose sends your\n   *          data.</p>\n   */\n  HECEndpoint: string | undefined;\n\n  /**\n   * <p>This type can be either \"Raw\" or \"Event.\"</p>\n   */\n  HECEndpointType: HECEndpointType | string | undefined;\n\n  /**\n   * <p>This is a GUID that you obtain from your Splunk cluster when you create a new HEC\n   *          endpoint.</p>\n   */\n  HECToken: string | undefined;\n\n  /**\n   * <p>The amount of time that Kinesis Data Firehose waits to receive an acknowledgment from\n   *          Splunk after it sends it data. At the end of the timeout period, Kinesis Data Firehose\n   *          either tries to send the data again or considers it an error, based on your retry\n   *          settings.</p>\n   */\n  HECAcknowledgmentTimeoutInSeconds?: number;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver data to Splunk,\n   *          or if it doesn't receive an acknowledgment of receipt from Splunk.</p>\n   */\n  RetryOptions?: SplunkRetryOptions;\n\n  /**\n   * <p>Defines how documents should be delivered to Amazon S3. When set to\n   *             <code>FailedEventsOnly</code>, Kinesis Data Firehose writes any data that could not be\n   *          indexed to the configured Amazon S3 destination. When set to <code>AllEvents</code>,\n   *          Kinesis Data Firehose delivers all incoming records to Amazon S3, and also writes failed\n   *          documents to Amazon S3. The default value is <code>FailedEventsOnly</code>.</p>\n   *          <p>You can update this backup mode from <code>FailedEventsOnly</code> to\n   *             <code>AllEvents</code>. You can't update it from <code>AllEvents</code> to\n   *             <code>FailedEventsOnly</code>.</p>\n   */\n  S3BackupMode?: SplunkS3BackupMode | string;\n\n  /**\n   * <p>The configuration for the backup Amazon S3 location.</p>\n   */\n  S3Configuration: S3DestinationConfiguration | undefined;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace SplunkDestinationConfiguration {\n  export const filterSensitiveLog = (obj: SplunkDestinationConfiguration): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Metadata that you can assign to a delivery stream, consisting of a key-value\n *          pair.</p>\n */\nexport interface Tag {\n  /**\n   * <p>A unique identifier for the tag. Maximum length: 128 characters. Valid characters:\n   *          Unicode letters, digits, white space, _ . / = + - % @</p>\n   */\n  Key: string | undefined;\n\n  /**\n   * <p>An optional string, which you can use to describe or define the tag. Maximum length:\n   *          256 characters. Valid characters: Unicode letters, digits, white space, _ . / = + - %\n   *          @</p>\n   */\n  Value?: string;\n}\n\nexport namespace Tag {\n  export const filterSensitiveLog = (obj: Tag): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream. This name must be unique per AWS account in the same\n   *          AWS Region. If the delivery streams are in different accounts or different Regions, you can\n   *          have multiple delivery streams with the same name.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>The delivery stream type. This parameter can be one of the following\n   *          values:</p>\n   *          <ul>\n   *             <li>\n   *                <p>\n   *                   <code>DirectPut</code>: Provider applications access the delivery stream\n   *                directly.</p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>KinesisStreamAsSource</code>: The delivery stream uses a Kinesis data\n   *                stream as a source.</p>\n   *             </li>\n   *          </ul>\n   */\n  DeliveryStreamType?: DeliveryStreamType | string;\n\n  /**\n   * <p>When a Kinesis data stream is used as the source for the delivery stream, a <a>KinesisStreamSourceConfiguration</a> containing the Kinesis data stream Amazon\n   *          Resource Name (ARN) and the role ARN for the source stream.</p>\n   */\n  KinesisStreamSourceConfiguration?: KinesisStreamSourceConfiguration;\n\n  /**\n   * <p>Used to specify the type and Amazon Resource Name (ARN) of the KMS key needed for\n   *          Server-Side Encryption (SSE).</p>\n   */\n  DeliveryStreamEncryptionConfigurationInput?: DeliveryStreamEncryptionConfigurationInput;\n\n  /**\n   * @deprecated\n   *\n   * <p>[Deprecated]\n   *          The destination in Amazon S3. You can specify only one destination.</p>\n   */\n  S3DestinationConfiguration?: S3DestinationConfiguration;\n\n  /**\n   * <p>The destination in Amazon S3. You can specify only one destination.</p>\n   */\n  ExtendedS3DestinationConfiguration?: ExtendedS3DestinationConfiguration;\n\n  /**\n   * <p>The destination in Amazon Redshift. You can specify only one destination.</p>\n   */\n  RedshiftDestinationConfiguration?: RedshiftDestinationConfiguration;\n\n  /**\n   * <p>The destination in Amazon ES. You can specify only one destination.</p>\n   */\n  ElasticsearchDestinationConfiguration?: ElasticsearchDestinationConfiguration;\n\n  /**\n   * <p>The destination in Splunk. You can specify only one destination.</p>\n   */\n  SplunkDestinationConfiguration?: SplunkDestinationConfiguration;\n\n  /**\n   * <p>Enables configuring Kinesis Firehose to deliver data to any HTTP endpoint destination.\n   *          You can specify only one destination.</p>\n   */\n  HttpEndpointDestinationConfiguration?: HttpEndpointDestinationConfiguration;\n\n  /**\n   * <p>A set of tags to assign to the delivery stream. A tag is a key-value pair that you can\n   *          define and assign to AWS resources. Tags are metadata. For example, you can add friendly\n   *          names and descriptions or other types of information that can help you distinguish the\n   *          delivery stream. For more information about tags, see <a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">Using Cost Allocation Tags</a> in the AWS Billing and Cost Management User\n   *          Guide.</p>\n   *\n   *          <p>You can specify up to 50 tags when creating a delivery stream.</p>\n   */\n  Tags?: Tag[];\n}\n\nexport namespace CreateDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: CreateDeliveryStreamInput): any => ({\n    ...obj,\n    ...(obj.RedshiftDestinationConfiguration && {\n      RedshiftDestinationConfiguration: RedshiftDestinationConfiguration.filterSensitiveLog(\n        obj.RedshiftDestinationConfiguration\n      ),\n    }),\n    ...(obj.HttpEndpointDestinationConfiguration && {\n      HttpEndpointDestinationConfiguration: HttpEndpointDestinationConfiguration.filterSensitiveLog(\n        obj.HttpEndpointDestinationConfiguration\n      ),\n    }),\n  });\n}\n\nexport interface CreateDeliveryStreamOutput {\n  /**\n   * <p>The ARN of the delivery stream.</p>\n   */\n  DeliveryStreamARN?: string;\n}\n\nexport namespace CreateDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: CreateDeliveryStreamOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The specified input parameter has a value that is not valid.</p>\n */\nexport interface InvalidArgumentException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidArgumentException\";\n  $fault: \"client\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace InvalidArgumentException {\n  export const filterSensitiveLog = (obj: InvalidArgumentException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Kinesis Data Firehose throws this exception when an attempt to put records or to start\n *          or stop delivery stream encryption fails. This happens when the KMS service throws one of\n *          the following exception types: <code>AccessDeniedException</code>,\n *             <code>InvalidStateException</code>, <code>DisabledException</code>, or\n *             <code>NotFoundException</code>.</p>\n */\nexport interface InvalidKMSResourceException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidKMSResourceException\";\n  $fault: \"client\";\n  code?: string;\n  message?: string;\n}\n\nexport namespace InvalidKMSResourceException {\n  export const filterSensitiveLog = (obj: InvalidKMSResourceException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>You have already reached the limit for a requested resource.</p>\n */\nexport interface LimitExceededException extends __SmithyException, $MetadataBearer {\n  name: \"LimitExceededException\";\n  $fault: \"client\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace LimitExceededException {\n  export const filterSensitiveLog = (obj: LimitExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The resource is already in use and not available for this operation.</p>\n */\nexport interface ResourceInUseException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceInUseException\";\n  $fault: \"client\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace ResourceInUseException {\n  export const filterSensitiveLog = (obj: ResourceInUseException): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>Set this to true if you want to delete the delivery stream even if Kinesis Data Firehose\n   *          is unable to retire the grant for the CMK. Kinesis Data Firehose might be unable to retire\n   *          the grant due to a customer error, such as when the CMK or the grant are in an invalid\n   *          state. If you force deletion, you can then use the <a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_RevokeGrant.html\">RevokeGrant</a> operation to revoke the grant you gave to Kinesis Data Firehose. If\n   *          a failure to retire the grant happens due to an AWS KMS issue, Kinesis Data Firehose keeps\n   *          retrying the delete operation.</p>\n   *          <p>The default value is false.</p>\n   */\n  AllowForceDelete?: boolean;\n}\n\nexport namespace DeleteDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: DeleteDeliveryStreamInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteDeliveryStreamOutput {}\n\nexport namespace DeleteDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: DeleteDeliveryStreamOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The specified resource could not be found.</p>\n */\nexport interface ResourceNotFoundException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceNotFoundException\";\n  $fault: \"client\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace ResourceNotFoundException {\n  export const filterSensitiveLog = (obj: ResourceNotFoundException): any => ({\n    ...obj,\n  });\n}\n\nexport enum DeliveryStreamFailureType {\n  CREATE_ENI_FAILED = \"CREATE_ENI_FAILED\",\n  CREATE_KMS_GRANT_FAILED = \"CREATE_KMS_GRANT_FAILED\",\n  DELETE_ENI_FAILED = \"DELETE_ENI_FAILED\",\n  DISABLED_KMS_KEY = \"DISABLED_KMS_KEY\",\n  ENI_ACCESS_DENIED = \"ENI_ACCESS_DENIED\",\n  INVALID_KMS_KEY = \"INVALID_KMS_KEY\",\n  KMS_ACCESS_DENIED = \"KMS_ACCESS_DENIED\",\n  KMS_KEY_NOT_FOUND = \"KMS_KEY_NOT_FOUND\",\n  KMS_OPT_IN_REQUIRED = \"KMS_OPT_IN_REQUIRED\",\n  RETIRE_KMS_GRANT_FAILED = \"RETIRE_KMS_GRANT_FAILED\",\n  SECURITY_GROUP_ACCESS_DENIED = \"SECURITY_GROUP_ACCESS_DENIED\",\n  SECURITY_GROUP_NOT_FOUND = \"SECURITY_GROUP_NOT_FOUND\",\n  SUBNET_ACCESS_DENIED = \"SUBNET_ACCESS_DENIED\",\n  SUBNET_NOT_FOUND = \"SUBNET_NOT_FOUND\",\n  UNKNOWN_ERROR = \"UNKNOWN_ERROR\",\n}\n\n/**\n * <p>Provides details in case one of the following operations fails due to an error related\n *          to KMS: <a>CreateDeliveryStream</a>, <a>DeleteDeliveryStream</a>,\n *             <a>StartDeliveryStreamEncryption</a>, <a>StopDeliveryStreamEncryption</a>.</p>\n */\nexport interface FailureDescription {\n  /**\n   * <p>The type of error that caused the failure.</p>\n   */\n  Type: DeliveryStreamFailureType | string | undefined;\n\n  /**\n   * <p>A message providing details about the error that caused the failure.</p>\n   */\n  Details: string | undefined;\n}\n\nexport namespace FailureDescription {\n  export const filterSensitiveLog = (obj: FailureDescription): any => ({\n    ...obj,\n  });\n}\n\nexport enum DeliveryStreamEncryptionStatus {\n  DISABLED = \"DISABLED\",\n  DISABLING = \"DISABLING\",\n  DISABLING_FAILED = \"DISABLING_FAILED\",\n  ENABLED = \"ENABLED\",\n  ENABLING = \"ENABLING\",\n  ENABLING_FAILED = \"ENABLING_FAILED\",\n}\n\n/**\n * <p>Contains information about the server-side encryption (SSE) status for the delivery\n *          stream, the type customer master key (CMK) in use, if any, and the ARN of the CMK. You can\n *          get <code>DeliveryStreamEncryptionConfiguration</code> by invoking the <a>DescribeDeliveryStream</a> operation. </p>\n */\nexport interface DeliveryStreamEncryptionConfiguration {\n  /**\n   * <p>If <code>KeyType</code> is <code>CUSTOMER_MANAGED_CMK</code>, this field contains the\n   *          ARN of the customer managed CMK. If <code>KeyType</code> is <code>AWS_OWNED_CMK</code>,\n   *             <code>DeliveryStreamEncryptionConfiguration</code> doesn't contain a value for\n   *             <code>KeyARN</code>.</p>\n   */\n  KeyARN?: string;\n\n  /**\n   * <p>Indicates the type of customer master key (CMK) that is used for encryption. The default\n   *          setting is <code>AWS_OWNED_CMK</code>. For more information about CMKs, see <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">Customer Master Keys (CMKs)</a>.</p>\n   */\n  KeyType?: KeyType | string;\n\n  /**\n   * <p>This is the server-side encryption (SSE) status for the delivery stream. For a full\n   *          description of the different values of this status, see <a>StartDeliveryStreamEncryption</a> and <a>StopDeliveryStreamEncryption</a>. If this status is <code>ENABLING_FAILED</code>\n   *          or <code>DISABLING_FAILED</code>, it is the status of the most recent attempt to enable or\n   *          disable SSE, respectively.</p>\n   */\n  Status?: DeliveryStreamEncryptionStatus | string;\n\n  /**\n   * <p>Provides details in case one of the following operations fails due to an error related\n   *          to KMS: <a>CreateDeliveryStream</a>, <a>DeleteDeliveryStream</a>,\n   *             <a>StartDeliveryStreamEncryption</a>, <a>StopDeliveryStreamEncryption</a>.</p>\n   */\n  FailureDescription?: FailureDescription;\n}\n\nexport namespace DeliveryStreamEncryptionConfiguration {\n  export const filterSensitiveLog = (obj: DeliveryStreamEncryptionConfiguration): any => ({\n    ...obj,\n  });\n}\n\nexport enum DeliveryStreamStatus {\n  ACTIVE = \"ACTIVE\",\n  CREATING = \"CREATING\",\n  CREATING_FAILED = \"CREATING_FAILED\",\n  DELETING = \"DELETING\",\n  DELETING_FAILED = \"DELETING_FAILED\",\n}\n\n/**\n * <p>Describes a destination in Amazon S3.</p>\n */\nexport interface S3DestinationDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN: string | undefined;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option. If no value is specified, <code>BufferingHints</code> object\n   *          default values are used.</p>\n   */\n  BufferingHints: BufferingHints | undefined;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *             <code>UNCOMPRESSED</code>.</p>\n   */\n  CompressionFormat: CompressionFormat | string | undefined;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration: EncryptionConfiguration | undefined;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace S3DestinationDescription {\n  export const filterSensitiveLog = (obj: S3DestinationDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The details of the VPC of the Amazon ES destination.</p>\n */\nexport interface VpcConfigurationDescription {\n  /**\n   * <p>The IDs of the subnets that Kinesis Data Firehose uses to create ENIs in the VPC of the\n   *          Amazon ES destination. Make sure that the routing tables and inbound and outbound rules\n   *          allow traffic to flow from the subnets whose IDs are specified here to the subnets that\n   *          have the destination Amazon ES endpoints. Kinesis Data Firehose creates at least one ENI in\n   *          each of the subnets that are specified here. Do not delete or modify these ENIs.</p>\n   *          <p>The number of ENIs that Kinesis Data Firehose creates in the subnets specified here\n   *          scales up and down automatically based on throughput. To enable Kinesis Data Firehose to\n   *          scale up the number of ENIs to match throughput, ensure that you have sufficient quota. To\n   *          help you calculate the quota you need, assume that Kinesis Data Firehose can create up to\n   *          three ENIs for this delivery stream for each of the subnets specified here. For more\n   *          information about ENI quota, see <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-enis\">Network Interfaces </a> in the Amazon VPC Quotas topic.</p>\n   */\n  SubnetIds: string[] | undefined;\n\n  /**\n   * <p>The ARN of the IAM role that the delivery stream uses to create endpoints in the\n   *          destination VPC. You can use your existing Kinesis Data Firehose delivery role or you can\n   *          specify a new role. In either case, make sure that the role trusts the Kinesis Data\n   *          Firehose service principal and that it grants the following permissions:</p>\n   *          <ul>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeVpcs</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeVpcAttribute</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeSubnets</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeSecurityGroups</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DescribeNetworkInterfaces</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:CreateNetworkInterface</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:CreateNetworkInterfacePermission</code>\n   *                </p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>ec2:DeleteNetworkInterface</code>\n   *                </p>\n   *             </li>\n   *          </ul>\n   *          <p>If you revoke these permissions after you create the delivery stream, Kinesis Data\n   *          Firehose can't scale out by creating more ENIs when necessary. You might therefore see a\n   *          degradation in performance.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The IDs of the security groups that Kinesis Data Firehose uses when it creates ENIs in\n   *          the VPC of the Amazon ES destination. You can use the same security group that the Amazon\n   *          ES domain uses or different ones. If you specify different security groups, ensure that\n   *          they allow outbound HTTPS traffic to the Amazon ES domain's security group. Also ensure\n   *          that the Amazon ES domain's security group allows HTTPS traffic from the security groups\n   *          specified here. If you use the same security group for both your delivery stream and the\n   *          Amazon ES domain, make sure the security group inbound rule allows HTTPS traffic. For more\n   *          information about security group rules, see <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#SecurityGroupRules\">Security group rules</a> in the Amazon VPC documentation.</p>\n   */\n  SecurityGroupIds: string[] | undefined;\n\n  /**\n   * <p>The ID of the Amazon ES destination's VPC.</p>\n   */\n  VpcId: string | undefined;\n}\n\nexport namespace VpcConfigurationDescription {\n  export const filterSensitiveLog = (obj: VpcConfigurationDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The destination description in Amazon ES.</p>\n */\nexport interface ElasticsearchDestinationDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The ARN of the Amazon ES domain. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   *\n   *          <p>Kinesis Data Firehose uses either <code>ClusterEndpoint</code> or <code>DomainARN</code>\n   *          to send data to Amazon ES.</p>\n   */\n  DomainARN?: string;\n\n  /**\n   * <p>The endpoint to use when communicating with the cluster. Kinesis Data Firehose uses\n   *          either this <code>ClusterEndpoint</code> or the <code>DomainARN</code> field to send data\n   *          to Amazon ES.</p>\n   */\n  ClusterEndpoint?: string;\n\n  /**\n   * <p>The Elasticsearch index name.</p>\n   */\n  IndexName?: string;\n\n  /**\n   * <p>The Elasticsearch type name. This applies to Elasticsearch 6.x and lower versions.\n   *          For Elasticsearch 7.x, there's no value for <code>TypeName</code>.</p>\n   */\n  TypeName?: string;\n\n  /**\n   * <p>The Elasticsearch index rotation period</p>\n   */\n  IndexRotationPeriod?: ElasticsearchIndexRotationPeriod | string;\n\n  /**\n   * <p>The buffering options.</p>\n   */\n  BufferingHints?: ElasticsearchBufferingHints;\n\n  /**\n   * <p>The Amazon ES retry options.</p>\n   */\n  RetryOptions?: ElasticsearchRetryOptions;\n\n  /**\n   * <p>The Amazon S3 backup mode.</p>\n   */\n  S3BackupMode?: ElasticsearchS3BackupMode | string;\n\n  /**\n   * <p>The Amazon S3 destination.</p>\n   */\n  S3DestinationDescription?: S3DestinationDescription;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The details of the VPC of the Amazon ES destination.</p>\n   */\n  VpcConfigurationDescription?: VpcConfigurationDescription;\n}\n\nexport namespace ElasticsearchDestinationDescription {\n  export const filterSensitiveLog = (obj: ElasticsearchDestinationDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes a destination in Amazon S3.</p>\n */\nexport interface ExtendedS3DestinationDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN: string | undefined;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option.</p>\n   */\n  BufferingHints: BufferingHints | undefined;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *             <code>UNCOMPRESSED</code>.</p>\n   */\n  CompressionFormat: CompressionFormat | string | undefined;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration: EncryptionConfiguration | undefined;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon S3 backup mode.</p>\n   */\n  S3BackupMode?: S3BackupMode | string;\n\n  /**\n   * <p>The configuration for backup in Amazon S3.</p>\n   */\n  S3BackupDescription?: S3DestinationDescription;\n\n  /**\n   * <p>The serializer, deserializer, and schema for converting data from the JSON format to\n   *          the Parquet or ORC format before writing it to Amazon S3.</p>\n   */\n  DataFormatConversionConfiguration?: DataFormatConversionConfiguration;\n}\n\nexport namespace ExtendedS3DestinationDescription {\n  export const filterSensitiveLog = (obj: ExtendedS3DestinationDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the HTTP endpoint selected as the destination. </p>\n */\nexport interface HttpEndpointDescription {\n  /**\n   * <p>The URL of the HTTP endpoint selected as the destination.</p>\n   */\n  Url?: string;\n\n  /**\n   * <p>The name of the HTTP endpoint selected as the destination.</p>\n   */\n  Name?: string;\n}\n\nexport namespace HttpEndpointDescription {\n  export const filterSensitiveLog = (obj: HttpEndpointDescription): any => ({\n    ...obj,\n    ...(obj.Url && { Url: SENSITIVE_STRING }),\n  });\n}\n\n/**\n * <p>Describes the HTTP endpoint destination.</p>\n */\nexport interface HttpEndpointDestinationDescription {\n  /**\n   * <p>The configuration of the specified HTTP endpoint destination.</p>\n   */\n  EndpointConfiguration?: HttpEndpointDescription;\n\n  /**\n   * <p>Describes buffering options that can be applied to the data before it is delivered to\n   *          the HTTPS endpoint destination. Kinesis Data Firehose teats these options as hints, and it\n   *          might choose to use more optimal values. The <code>SizeInMBs</code> and\n   *             <code>IntervalInSeconds</code> parameters are optional. However, if specify a value for\n   *          one of them, you must also provide a value for the other. </p>\n   */\n  BufferingHints?: HttpEndpointBufferingHints;\n\n  /**\n   * <p>Describes the Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The configuration of request sent to the HTTP endpoint specified as the\n   *          destination.</p>\n   */\n  RequestConfiguration?: HttpEndpointRequestConfiguration;\n\n  /**\n   * <p>Describes a data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>Kinesis Data Firehose uses this IAM role for all the permissions that the delivery\n   *          stream needs.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>Describes the retry behavior in case Kinesis Data Firehose is unable to deliver data to\n   *          the specified HTTP endpoint destination, or if it doesn't receive a valid acknowledgment of\n   *          receipt from the specified HTTP endpoint destination.</p>\n   */\n  RetryOptions?: HttpEndpointRetryOptions;\n\n  /**\n   * <p>Describes the S3 bucket backup options for the data that Kinesis Firehose delivers to\n   *          the HTTP endpoint destination. You can back up all documents (<code>AllData</code>) or only\n   *          the documents that Kinesis Data Firehose could not deliver to the specified HTTP endpoint\n   *          destination (<code>FailedDataOnly</code>).</p>\n   */\n  S3BackupMode?: HttpEndpointS3BackupMode | string;\n\n  /**\n   * <p>Describes a destination in Amazon S3.</p>\n   */\n  S3DestinationDescription?: S3DestinationDescription;\n}\n\nexport namespace HttpEndpointDestinationDescription {\n  export const filterSensitiveLog = (obj: HttpEndpointDestinationDescription): any => ({\n    ...obj,\n    ...(obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointDescription.filterSensitiveLog(obj.EndpointConfiguration),\n    }),\n    ...(obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration),\n    }),\n  });\n}\n\n/**\n * <p>Describes a destination in Amazon Redshift.</p>\n */\nexport interface RedshiftDestinationDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN: string | undefined;\n\n  /**\n   * <p>The database connection string.</p>\n   */\n  ClusterJDBCURL: string | undefined;\n\n  /**\n   * <p>The <code>COPY</code> command.</p>\n   */\n  CopyCommand: CopyCommand | undefined;\n\n  /**\n   * <p>The name of the user.</p>\n   */\n  Username: string | undefined;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver documents to\n   *          Amazon Redshift. Default value is 3600 (60 minutes).</p>\n   */\n  RetryOptions?: RedshiftRetryOptions;\n\n  /**\n   * <p>The Amazon S3 destination.</p>\n   */\n  S3DestinationDescription: S3DestinationDescription | undefined;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon S3 backup mode.</p>\n   */\n  S3BackupMode?: RedshiftS3BackupMode | string;\n\n  /**\n   * <p>The configuration for backup in Amazon S3.</p>\n   */\n  S3BackupDescription?: S3DestinationDescription;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace RedshiftDestinationDescription {\n  export const filterSensitiveLog = (obj: RedshiftDestinationDescription): any => ({\n    ...obj,\n    ...(obj.Username && { Username: SENSITIVE_STRING }),\n  });\n}\n\n/**\n * <p>Describes a destination in Splunk.</p>\n */\nexport interface SplunkDestinationDescription {\n  /**\n   * <p>The HTTP Event Collector (HEC) endpoint to which Kinesis Data Firehose sends your\n   *          data.</p>\n   */\n  HECEndpoint?: string;\n\n  /**\n   * <p>This type can be either \"Raw\" or \"Event.\"</p>\n   */\n  HECEndpointType?: HECEndpointType | string;\n\n  /**\n   * <p>A GUID you obtain from your Splunk cluster when you create a new HEC\n   *          endpoint.</p>\n   */\n  HECToken?: string;\n\n  /**\n   * <p>The amount of time that Kinesis Data Firehose waits to receive an acknowledgment from\n   *          Splunk after it sends it data. At the end of the timeout period, Kinesis Data Firehose\n   *          either tries to send the data again or considers it an error, based on your retry\n   *          settings.</p>\n   */\n  HECAcknowledgmentTimeoutInSeconds?: number;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver data to Splunk\n   *          or if it doesn't receive an acknowledgment of receipt from Splunk.</p>\n   */\n  RetryOptions?: SplunkRetryOptions;\n\n  /**\n   * <p>Defines how documents should be delivered to Amazon S3. When set to\n   *             <code>FailedDocumentsOnly</code>, Kinesis Data Firehose writes any data that could not\n   *          be indexed to the configured Amazon S3 destination. When set to <code>AllDocuments</code>,\n   *          Kinesis Data Firehose delivers all incoming records to Amazon S3, and also writes failed\n   *          documents to Amazon S3. Default value is <code>FailedDocumentsOnly</code>. </p>\n   */\n  S3BackupMode?: SplunkS3BackupMode | string;\n\n  /**\n   * <p>The Amazon S3 destination.></p>\n   */\n  S3DestinationDescription?: S3DestinationDescription;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace SplunkDestinationDescription {\n  export const filterSensitiveLog = (obj: SplunkDestinationDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the destination for a delivery stream.</p>\n */\nexport interface DestinationDescription {\n  /**\n   * <p>The ID of the destination.</p>\n   */\n  DestinationId: string | undefined;\n\n  /**\n   * <p>[Deprecated] The destination in Amazon S3.</p>\n   */\n  S3DestinationDescription?: S3DestinationDescription;\n\n  /**\n   * <p>The destination in Amazon S3.</p>\n   */\n  ExtendedS3DestinationDescription?: ExtendedS3DestinationDescription;\n\n  /**\n   * <p>The destination in Amazon Redshift.</p>\n   */\n  RedshiftDestinationDescription?: RedshiftDestinationDescription;\n\n  /**\n   * <p>The destination in Amazon ES.</p>\n   */\n  ElasticsearchDestinationDescription?: ElasticsearchDestinationDescription;\n\n  /**\n   * <p>The destination in Splunk.</p>\n   */\n  SplunkDestinationDescription?: SplunkDestinationDescription;\n\n  /**\n   * <p>Describes the specified HTTP endpoint destination.</p>\n   */\n  HttpEndpointDestinationDescription?: HttpEndpointDestinationDescription;\n}\n\nexport namespace DestinationDescription {\n  export const filterSensitiveLog = (obj: DestinationDescription): any => ({\n    ...obj,\n    ...(obj.RedshiftDestinationDescription && {\n      RedshiftDestinationDescription: RedshiftDestinationDescription.filterSensitiveLog(\n        obj.RedshiftDestinationDescription\n      ),\n    }),\n    ...(obj.HttpEndpointDestinationDescription && {\n      HttpEndpointDestinationDescription: HttpEndpointDestinationDescription.filterSensitiveLog(\n        obj.HttpEndpointDestinationDescription\n      ),\n    }),\n  });\n}\n\n/**\n * <p>Details about a Kinesis data stream used as the source for a Kinesis Data Firehose\n *          delivery stream.</p>\n */\nexport interface KinesisStreamSourceDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the source Kinesis data stream. For more\n   *          information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-kinesis-streams\">Amazon Kinesis Data Streams ARN Format</a>.</p>\n   */\n  KinesisStreamARN?: string;\n\n  /**\n   * <p>The ARN of the role used by the source Kinesis data stream. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-iam\">AWS Identity and Access Management (IAM) ARN Format</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>Kinesis Data Firehose starts retrieving records from the Kinesis data stream starting\n   *          with this timestamp.</p>\n   */\n  DeliveryStartTimestamp?: Date;\n}\n\nexport namespace KinesisStreamSourceDescription {\n  export const filterSensitiveLog = (obj: KinesisStreamSourceDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Details about a Kinesis data stream used as the source for a Kinesis Data Firehose\n *          delivery stream.</p>\n */\nexport interface SourceDescription {\n  /**\n   * <p>The <a>KinesisStreamSourceDescription</a> value for the source Kinesis\n   *          data stream.</p>\n   */\n  KinesisStreamSourceDescription?: KinesisStreamSourceDescription;\n}\n\nexport namespace SourceDescription {\n  export const filterSensitiveLog = (obj: SourceDescription): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Contains information about a delivery stream.</p>\n */\nexport interface DeliveryStreamDescription {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>The Amazon Resource Name (ARN) of the delivery stream. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  DeliveryStreamARN: string | undefined;\n\n  /**\n   * <p>The status of the delivery stream. If the status of a delivery stream is\n   *             <code>CREATING_FAILED</code>, this status doesn't change, and you can't invoke\n   *             <code>CreateDeliveryStream</code> again on it. However, you can invoke the <a>DeleteDeliveryStream</a> operation to delete it.</p>\n   */\n  DeliveryStreamStatus: DeliveryStreamStatus | string | undefined;\n\n  /**\n   * <p>Provides details in case one of the following operations fails due to an error related\n   *          to KMS: <a>CreateDeliveryStream</a>, <a>DeleteDeliveryStream</a>,\n   *             <a>StartDeliveryStreamEncryption</a>, <a>StopDeliveryStreamEncryption</a>.</p>\n   */\n  FailureDescription?: FailureDescription;\n\n  /**\n   * <p>Indicates the server-side encryption (SSE) status for the delivery stream.</p>\n   */\n  DeliveryStreamEncryptionConfiguration?: DeliveryStreamEncryptionConfiguration;\n\n  /**\n   * <p>The delivery stream type. This can be one of the following values:</p>\n   *          <ul>\n   *             <li>\n   *                <p>\n   *                   <code>DirectPut</code>: Provider applications access the delivery stream\n   *                directly.</p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>KinesisStreamAsSource</code>: The delivery stream uses a Kinesis data\n   *                stream as a source.</p>\n   *             </li>\n   *          </ul>\n   */\n  DeliveryStreamType: DeliveryStreamType | string | undefined;\n\n  /**\n   * <p>Each time the destination is updated for a delivery stream, the version ID is\n   *          changed, and the current version ID is required when updating the destination. This is so\n   *          that the service knows it is applying the changes to the correct version of the delivery\n   *          stream.</p>\n   */\n  VersionId: string | undefined;\n\n  /**\n   * <p>The date and time that the delivery stream was created.</p>\n   */\n  CreateTimestamp?: Date;\n\n  /**\n   * <p>The date and time that the delivery stream was last updated.</p>\n   */\n  LastUpdateTimestamp?: Date;\n\n  /**\n   * <p>If the <code>DeliveryStreamType</code> parameter is\n   *             <code>KinesisStreamAsSource</code>, a <a>SourceDescription</a> object\n   *          describing the source Kinesis data stream.</p>\n   */\n  Source?: SourceDescription;\n\n  /**\n   * <p>The destinations.</p>\n   */\n  Destinations: DestinationDescription[] | undefined;\n\n  /**\n   * <p>Indicates whether there are more destinations available to list.</p>\n   */\n  HasMoreDestinations: boolean | undefined;\n}\n\nexport namespace DeliveryStreamDescription {\n  export const filterSensitiveLog = (obj: DeliveryStreamDescription): any => ({\n    ...obj,\n    ...(obj.Destinations && {\n      Destinations: obj.Destinations.map((item) => DestinationDescription.filterSensitiveLog(item)),\n    }),\n  });\n}\n\nexport interface DescribeDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>The limit on the number of destinations to return. You can have one destination per\n   *          delivery stream.</p>\n   */\n  Limit?: number;\n\n  /**\n   * <p>The ID of the destination to start returning the destination information. Kinesis\n   *          Data Firehose supports one destination per delivery stream.</p>\n   */\n  ExclusiveStartDestinationId?: string;\n}\n\nexport namespace DescribeDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: DescribeDeliveryStreamInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeDeliveryStreamOutput {\n  /**\n   * <p>Information about the delivery stream.</p>\n   */\n  DeliveryStreamDescription: DeliveryStreamDescription | undefined;\n}\n\nexport namespace DescribeDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: DescribeDeliveryStreamOutput): any => ({\n    ...obj,\n    ...(obj.DeliveryStreamDescription && {\n      DeliveryStreamDescription: DeliveryStreamDescription.filterSensitiveLog(obj.DeliveryStreamDescription),\n    }),\n  });\n}\n\n/**\n * <p>Describes an update for a destination in Amazon S3.</p>\n */\nexport interface S3DestinationUpdate {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN?: string;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option. If no value is specified, <code>BufferingHints</code> object\n   *          default values are used.</p>\n   */\n  BufferingHints?: BufferingHints;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *             <code>UNCOMPRESSED</code>.</p>\n   *          <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified\n   *          for Amazon Redshift destinations because they are not supported by the Amazon Redshift\n   *             <code>COPY</code> operation that reads from the S3 bucket.</p>\n   */\n  CompressionFormat?: CompressionFormat | string;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration?: EncryptionConfiguration;\n\n  /**\n   * <p>The CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace S3DestinationUpdate {\n  export const filterSensitiveLog = (obj: S3DestinationUpdate): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes an update for a destination in Amazon ES.</p>\n */\nexport interface ElasticsearchDestinationUpdate {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the IAM role to be assumed by Kinesis Data Firehose\n   *          for calling the Amazon ES Configuration API and for indexing documents. For more\n   *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n   *             Firehose Access to an Amazon S3 Destination</a> and <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The ARN of the Amazon ES domain. The IAM role must have permissions\n   *             for <code>DescribeElasticsearchDomain</code>, <code>DescribeElasticsearchDomains</code>,\n   *          and <code>DescribeElasticsearchDomainConfig</code> after assuming the IAM role specified in\n   *             <code>RoleARN</code>. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   *\n   *          <p>Specify either <code>ClusterEndpoint</code> or <code>DomainARN</code>.</p>\n   */\n  DomainARN?: string;\n\n  /**\n   * <p>The endpoint to use when communicating with the cluster. Specify either this\n   *             <code>ClusterEndpoint</code> or the <code>DomainARN</code> field.</p>\n   */\n  ClusterEndpoint?: string;\n\n  /**\n   * <p>The Elasticsearch index name.</p>\n   */\n  IndexName?: string;\n\n  /**\n   * <p>The Elasticsearch type name. For Elasticsearch 6.x, there can be only one type per\n   *          index. If you try to specify a new type for an existing index that already has another\n   *          type, Kinesis Data Firehose returns an error during runtime.</p>\n   *\n   *          <p>If you upgrade Elasticsearch from 6.x to 7.x and don’t update your delivery stream,\n   *          Kinesis Data Firehose still delivers data to Elasticsearch with the old index name and type\n   *          name. If you want to update your delivery stream with a new index name, provide an empty\n   *          string for <code>TypeName</code>. </p>\n   */\n  TypeName?: string;\n\n  /**\n   * <p>The Elasticsearch index rotation period. Index rotation appends a timestamp to\n   *             <code>IndexName</code> to facilitate the expiration of old data. For more information,\n   *          see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#es-index-rotation\">Index Rotation for the\n   *             Amazon ES Destination</a>. Default value is <code>OneDay</code>.</p>\n   */\n  IndexRotationPeriod?: ElasticsearchIndexRotationPeriod | string;\n\n  /**\n   * <p>The buffering options. If no value is specified,\n   *             <code>ElasticsearchBufferingHints</code> object default values are used. </p>\n   */\n  BufferingHints?: ElasticsearchBufferingHints;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver documents to\n   *          Amazon ES. The default value is 300 (5 minutes).</p>\n   */\n  RetryOptions?: ElasticsearchRetryOptions;\n\n  /**\n   * <p>The Amazon S3 destination.</p>\n   */\n  S3Update?: S3DestinationUpdate;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace ElasticsearchDestinationUpdate {\n  export const filterSensitiveLog = (obj: ElasticsearchDestinationUpdate): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes an update for a destination in Amazon S3.</p>\n */\nexport interface ExtendedS3DestinationUpdate {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The ARN of the S3 bucket. For more information, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon\n   *             Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  BucketARN?: string;\n\n  /**\n   * <p>The \"YYYY/MM/DD/HH\" time format prefix is automatically used for delivered Amazon S3\n   *          files. You can also specify a custom prefix, as described in <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  Prefix?: string;\n\n  /**\n   * <p>A prefix that Kinesis Data Firehose evaluates and adds to failed records before writing\n   *          them to S3. This prefix appears immediately following the bucket name. For information\n   *          about how to specify this prefix, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html\">Custom Prefixes\n   *             for Amazon S3 Objects</a>.</p>\n   */\n  ErrorOutputPrefix?: string;\n\n  /**\n   * <p>The buffering option.</p>\n   */\n  BufferingHints?: BufferingHints;\n\n  /**\n   * <p>The compression format. If no value is specified, the default is\n   *             <code>UNCOMPRESSED</code>. </p>\n   */\n  CompressionFormat?: CompressionFormat | string;\n\n  /**\n   * <p>The encryption configuration. If no value is specified, the default is no\n   *          encryption.</p>\n   */\n  EncryptionConfiguration?: EncryptionConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>You can update a delivery stream to enable Amazon S3 backup if it is disabled. If\n   *          backup is enabled, you can't update the delivery stream to disable it. </p>\n   */\n  S3BackupMode?: S3BackupMode | string;\n\n  /**\n   * <p>The Amazon S3 destination for backup.</p>\n   */\n  S3BackupUpdate?: S3DestinationUpdate;\n\n  /**\n   * <p>The serializer, deserializer, and schema for converting data from the JSON format to\n   *          the Parquet or ORC format before writing it to Amazon S3.</p>\n   */\n  DataFormatConversionConfiguration?: DataFormatConversionConfiguration;\n}\n\nexport namespace ExtendedS3DestinationUpdate {\n  export const filterSensitiveLog = (obj: ExtendedS3DestinationUpdate): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListDeliveryStreamsInput {\n  /**\n   * <p>The maximum number of delivery streams to list. The default value is 10.</p>\n   */\n  Limit?: number;\n\n  /**\n   * <p>The delivery stream type. This can be one of the following values:</p>\n   *          <ul>\n   *             <li>\n   *                <p>\n   *                   <code>DirectPut</code>: Provider applications access the delivery stream\n   *                directly.</p>\n   *             </li>\n   *             <li>\n   *                <p>\n   *                   <code>KinesisStreamAsSource</code>: The delivery stream uses a Kinesis data\n   *                stream as a source.</p>\n   *             </li>\n   *          </ul>\n   *          <p>This parameter is optional. If this parameter is omitted, delivery streams of all\n   *          types are returned.</p>\n   */\n  DeliveryStreamType?: DeliveryStreamType | string;\n\n  /**\n   * <p>The list of delivery streams returned by this call to\n   *             <code>ListDeliveryStreams</code> will start with the delivery stream whose name comes\n   *          alphabetically immediately after the name you specify in\n   *             <code>ExclusiveStartDeliveryStreamName</code>.</p>\n   */\n  ExclusiveStartDeliveryStreamName?: string;\n}\n\nexport namespace ListDeliveryStreamsInput {\n  export const filterSensitiveLog = (obj: ListDeliveryStreamsInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListDeliveryStreamsOutput {\n  /**\n   * <p>The names of the delivery streams.</p>\n   */\n  DeliveryStreamNames: string[] | undefined;\n\n  /**\n   * <p>Indicates whether there are more delivery streams available to list.</p>\n   */\n  HasMoreDeliveryStreams: boolean | undefined;\n}\n\nexport namespace ListDeliveryStreamsOutput {\n  export const filterSensitiveLog = (obj: ListDeliveryStreamsOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListTagsForDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream whose tags you want to list.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>The key to use as the starting point for the list of tags. If you set this parameter,\n   *             <code>ListTagsForDeliveryStream</code> gets all tags that occur after\n   *             <code>ExclusiveStartTagKey</code>.</p>\n   */\n  ExclusiveStartTagKey?: string;\n\n  /**\n   * <p>The number of tags to return. If this number is less than the total number of tags\n   *          associated with the delivery stream, <code>HasMoreTags</code> is set to <code>true</code>\n   *          in the response. To list additional tags, set <code>ExclusiveStartTagKey</code> to the last\n   *          key in the response. </p>\n   */\n  Limit?: number;\n}\n\nexport namespace ListTagsForDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: ListTagsForDeliveryStreamInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListTagsForDeliveryStreamOutput {\n  /**\n   * <p>A list of tags associated with <code>DeliveryStreamName</code>, starting with the\n   *          first tag after <code>ExclusiveStartTagKey</code> and up to the specified\n   *             <code>Limit</code>.</p>\n   */\n  Tags: Tag[] | undefined;\n\n  /**\n   * <p>If this is <code>true</code> in the response, more tags are available. To list the\n   *          remaining tags, set <code>ExclusiveStartTagKey</code> to the key of the last tag returned\n   *          and call <code>ListTagsForDeliveryStream</code> again.</p>\n   */\n  HasMoreTags: boolean | undefined;\n}\n\nexport namespace ListTagsForDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: ListTagsForDeliveryStreamOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The unit of data in a delivery stream.</p>\n */\nexport interface _Record {\n  /**\n   * <p>The data blob, which is base64-encoded when the blob is serialized. The maximum size\n   *          of the data blob, before base64-encoding, is 1,000 KiB.</p>\n   */\n  Data: Uint8Array | undefined;\n}\n\nexport namespace _Record {\n  export const filterSensitiveLog = (obj: _Record): any => ({\n    ...obj,\n  });\n}\n\nexport interface PutRecordInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>The record.</p>\n   */\n  Record: _Record | undefined;\n}\n\nexport namespace PutRecordInput {\n  export const filterSensitiveLog = (obj: PutRecordInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface PutRecordOutput {\n  /**\n   * <p>The ID of the record.</p>\n   */\n  RecordId: string | undefined;\n\n  /**\n   * <p>Indicates whether server-side encryption (SSE) was enabled during this operation.</p>\n   */\n  Encrypted?: boolean;\n}\n\nexport namespace PutRecordOutput {\n  export const filterSensitiveLog = (obj: PutRecordOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The service is unavailable. Back off and retry the operation. If you continue to see\n *          the exception, throughput limits for the delivery stream may have been exceeded. For more\n *          information about limits and how to request an increase, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/limits.html\">Amazon Kinesis Data Firehose\n *          Limits</a>.</p>\n */\nexport interface ServiceUnavailableException extends __SmithyException, $MetadataBearer {\n  name: \"ServiceUnavailableException\";\n  $fault: \"server\";\n  /**\n   * <p>A message that provides information about the error.</p>\n   */\n  message?: string;\n}\n\nexport namespace ServiceUnavailableException {\n  export const filterSensitiveLog = (obj: ServiceUnavailableException): any => ({\n    ...obj,\n  });\n}\n\nexport interface PutRecordBatchInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>One or more records.</p>\n   */\n  Records: _Record[] | undefined;\n}\n\nexport namespace PutRecordBatchInput {\n  export const filterSensitiveLog = (obj: PutRecordBatchInput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Contains the result for an individual record from a <a>PutRecordBatch</a>\n *          request. If the record is successfully added to your delivery stream, it receives a record\n *          ID. If the record fails to be added to your delivery stream, the result includes an error\n *          code and an error message.</p>\n */\nexport interface PutRecordBatchResponseEntry {\n  /**\n   * <p>The ID of the record.</p>\n   */\n  RecordId?: string;\n\n  /**\n   * <p>The error code for an individual record result.</p>\n   */\n  ErrorCode?: string;\n\n  /**\n   * <p>The error message for an individual record result.</p>\n   */\n  ErrorMessage?: string;\n}\n\nexport namespace PutRecordBatchResponseEntry {\n  export const filterSensitiveLog = (obj: PutRecordBatchResponseEntry): any => ({\n    ...obj,\n  });\n}\n\nexport interface PutRecordBatchOutput {\n  /**\n   * <p>The number of records that might have failed processing. This number might be greater\n   *          than 0 even if the <a>PutRecordBatch</a> call succeeds. Check\n   *             <code>FailedPutCount</code> to determine whether there are records that you need to\n   *          resend.</p>\n   */\n  FailedPutCount: number | undefined;\n\n  /**\n   * <p>Indicates whether server-side encryption (SSE) was enabled during this operation.</p>\n   */\n  Encrypted?: boolean;\n\n  /**\n   * <p>The results array. For each record, the index of the response element is the same as\n   *          the index used in the request array.</p>\n   */\n  RequestResponses: PutRecordBatchResponseEntry[] | undefined;\n}\n\nexport namespace PutRecordBatchOutput {\n  export const filterSensitiveLog = (obj: PutRecordBatchOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartDeliveryStreamEncryptionInput {\n  /**\n   * <p>The name of the delivery stream for which you want to enable server-side encryption\n   *          (SSE).</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>Used to specify the type and Amazon Resource Name (ARN) of the KMS key needed for\n   *          Server-Side Encryption (SSE).</p>\n   */\n  DeliveryStreamEncryptionConfigurationInput?: DeliveryStreamEncryptionConfigurationInput;\n}\n\nexport namespace StartDeliveryStreamEncryptionInput {\n  export const filterSensitiveLog = (obj: StartDeliveryStreamEncryptionInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartDeliveryStreamEncryptionOutput {}\n\nexport namespace StartDeliveryStreamEncryptionOutput {\n  export const filterSensitiveLog = (obj: StartDeliveryStreamEncryptionOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopDeliveryStreamEncryptionInput {\n  /**\n   * <p>The name of the delivery stream for which you want to disable server-side encryption\n   *          (SSE).</p>\n   */\n  DeliveryStreamName: string | undefined;\n}\n\nexport namespace StopDeliveryStreamEncryptionInput {\n  export const filterSensitiveLog = (obj: StopDeliveryStreamEncryptionInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopDeliveryStreamEncryptionOutput {}\n\nexport namespace StopDeliveryStreamEncryptionOutput {\n  export const filterSensitiveLog = (obj: StopDeliveryStreamEncryptionOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface TagDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream to which you want to add the tags.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>A set of key-value pairs to use to create the tags.</p>\n   */\n  Tags: Tag[] | undefined;\n}\n\nexport namespace TagDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: TagDeliveryStreamInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface TagDeliveryStreamOutput {}\n\nexport namespace TagDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: TagDeliveryStreamOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface UntagDeliveryStreamInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>A list of tag keys. Each corresponding tag is removed from the delivery\n   *          stream.</p>\n   */\n  TagKeys: string[] | undefined;\n}\n\nexport namespace UntagDeliveryStreamInput {\n  export const filterSensitiveLog = (obj: UntagDeliveryStreamInput): any => ({\n    ...obj,\n  });\n}\n\nexport interface UntagDeliveryStreamOutput {}\n\nexport namespace UntagDeliveryStreamOutput {\n  export const filterSensitiveLog = (obj: UntagDeliveryStreamOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Updates the specified HTTP endpoint destination.</p>\n */\nexport interface HttpEndpointDestinationUpdate {\n  /**\n   * <p>Describes the configuration of the HTTP endpoint destination.</p>\n   */\n  EndpointConfiguration?: HttpEndpointConfiguration;\n\n  /**\n   * <p>Describes buffering options that can be applied to the data before it is delivered to\n   *          the HTTPS endpoint destination. Kinesis Data Firehose teats these options as hints, and it\n   *          might choose to use more optimal values. The <code>SizeInMBs</code> and\n   *             <code>IntervalInSeconds</code> parameters are optional. However, if specify a value for\n   *          one of them, you must also provide a value for the other. </p>\n   */\n  BufferingHints?: HttpEndpointBufferingHints;\n\n  /**\n   * <p>Describes the Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n\n  /**\n   * <p>The configuration of the request sent to the HTTP endpoint specified as the\n   *          destination.</p>\n   */\n  RequestConfiguration?: HttpEndpointRequestConfiguration;\n\n  /**\n   * <p>Describes a data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>Kinesis Data Firehose uses this IAM role for all the permissions that the delivery\n   *          stream needs.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>Describes the retry behavior in case Kinesis Data Firehose is unable to deliver data to\n   *          the specified HTTP endpoint destination, or if it doesn't receive a valid acknowledgment of\n   *          receipt from the specified HTTP endpoint destination.</p>\n   */\n  RetryOptions?: HttpEndpointRetryOptions;\n\n  /**\n   * <p>Describes the S3 bucket backup options for the data that Kinesis Firehose delivers to\n   *          the HTTP endpoint destination. You can back up all documents (<code>AllData</code>) or only\n   *          the documents that Kinesis Data Firehose could not deliver to the specified HTTP endpoint\n   *          destination (<code>FailedDataOnly</code>).</p>\n   */\n  S3BackupMode?: HttpEndpointS3BackupMode | string;\n\n  /**\n   * <p>Describes an update for a destination in Amazon S3.</p>\n   */\n  S3Update?: S3DestinationUpdate;\n}\n\nexport namespace HttpEndpointDestinationUpdate {\n  export const filterSensitiveLog = (obj: HttpEndpointDestinationUpdate): any => ({\n    ...obj,\n    ...(obj.EndpointConfiguration && {\n      EndpointConfiguration: HttpEndpointConfiguration.filterSensitiveLog(obj.EndpointConfiguration),\n    }),\n    ...(obj.RequestConfiguration && {\n      RequestConfiguration: HttpEndpointRequestConfiguration.filterSensitiveLog(obj.RequestConfiguration),\n    }),\n  });\n}\n\n/**\n * <p>Describes an update for a destination in Amazon Redshift.</p>\n */\nexport interface RedshiftDestinationUpdate {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the AWS credentials. For more information, see\n   *             <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Names (ARNs) and AWS Service Namespaces</a>.</p>\n   */\n  RoleARN?: string;\n\n  /**\n   * <p>The database connection string.</p>\n   */\n  ClusterJDBCURL?: string;\n\n  /**\n   * <p>The <code>COPY</code> command.</p>\n   */\n  CopyCommand?: CopyCommand;\n\n  /**\n   * <p>The name of the user.</p>\n   */\n  Username?: string;\n\n  /**\n   * <p>The user password.</p>\n   */\n  Password?: string;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver documents to\n   *          Amazon Redshift. Default value is 3600 (60 minutes).</p>\n   */\n  RetryOptions?: RedshiftRetryOptions;\n\n  /**\n   * <p>The Amazon S3 destination.</p>\n   *          <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be specified\n   *          in <code>RedshiftDestinationUpdate.S3Update</code> because the Amazon Redshift\n   *             <code>COPY</code> operation that reads from the S3 bucket doesn't support these\n   *          compression formats.</p>\n   */\n  S3Update?: S3DestinationUpdate;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>You can update a delivery stream to enable Amazon S3 backup if it is disabled. If\n   *          backup is enabled, you can't update the delivery stream to disable it. </p>\n   */\n  S3BackupMode?: RedshiftS3BackupMode | string;\n\n  /**\n   * <p>The Amazon S3 destination for backup.</p>\n   */\n  S3BackupUpdate?: S3DestinationUpdate;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace RedshiftDestinationUpdate {\n  export const filterSensitiveLog = (obj: RedshiftDestinationUpdate): any => ({\n    ...obj,\n    ...(obj.Username && { Username: SENSITIVE_STRING }),\n    ...(obj.Password && { Password: SENSITIVE_STRING }),\n  });\n}\n\n/**\n * <p>Describes an update for a destination in Splunk.</p>\n */\nexport interface SplunkDestinationUpdate {\n  /**\n   * <p>The HTTP Event Collector (HEC) endpoint to which Kinesis Data Firehose sends your\n   *          data.</p>\n   */\n  HECEndpoint?: string;\n\n  /**\n   * <p>This type can be either \"Raw\" or \"Event.\"</p>\n   */\n  HECEndpointType?: HECEndpointType | string;\n\n  /**\n   * <p>A GUID that you obtain from your Splunk cluster when you create a new HEC\n   *          endpoint.</p>\n   */\n  HECToken?: string;\n\n  /**\n   * <p>The amount of time that Kinesis Data Firehose waits to receive an acknowledgment from\n   *          Splunk after it sends data. At the end of the timeout period, Kinesis Data Firehose either\n   *          tries to send the data again or considers it an error, based on your retry\n   *          settings.</p>\n   */\n  HECAcknowledgmentTimeoutInSeconds?: number;\n\n  /**\n   * <p>The retry behavior in case Kinesis Data Firehose is unable to deliver data to Splunk\n   *          or if it doesn't receive an acknowledgment of receipt from Splunk.</p>\n   */\n  RetryOptions?: SplunkRetryOptions;\n\n  /**\n   * <p>Specifies how you want Kinesis Data Firehose to back up documents to Amazon S3. When\n   *          set to <code>FailedDocumentsOnly</code>, Kinesis Data Firehose writes any data that could\n   *          not be indexed to the configured Amazon S3 destination. When set to <code>AllEvents</code>,\n   *          Kinesis Data Firehose delivers all incoming records to Amazon S3, and also writes failed\n   *          documents to Amazon S3. The default value is <code>FailedEventsOnly</code>.</p>\n   *          <p>You can update this backup mode from <code>FailedEventsOnly</code> to\n   *             <code>AllEvents</code>. You can't update it from <code>AllEvents</code> to\n   *             <code>FailedEventsOnly</code>.</p>\n   */\n  S3BackupMode?: SplunkS3BackupMode | string;\n\n  /**\n   * <p>Your update to the configuration of the backup Amazon S3 location.</p>\n   */\n  S3Update?: S3DestinationUpdate;\n\n  /**\n   * <p>The data processing configuration.</p>\n   */\n  ProcessingConfiguration?: ProcessingConfiguration;\n\n  /**\n   * <p>The Amazon CloudWatch logging options for your delivery stream.</p>\n   */\n  CloudWatchLoggingOptions?: CloudWatchLoggingOptions;\n}\n\nexport namespace SplunkDestinationUpdate {\n  export const filterSensitiveLog = (obj: SplunkDestinationUpdate): any => ({\n    ...obj,\n  });\n}\n\nexport interface UpdateDestinationInput {\n  /**\n   * <p>The name of the delivery stream.</p>\n   */\n  DeliveryStreamName: string | undefined;\n\n  /**\n   * <p>Obtain this value from the <code>VersionId</code> result of <a>DeliveryStreamDescription</a>. This value is required, and helps the service\n   *          perform conditional operations. For example, if there is an interleaving update and this\n   *          value is null, then the update destination fails. After the update is successful, the\n   *             <code>VersionId</code> value is updated. The service then performs a merge of the old\n   *          configuration with the new configuration.</p>\n   */\n  CurrentDeliveryStreamVersionId: string | undefined;\n\n  /**\n   * <p>The ID of the destination.</p>\n   */\n  DestinationId: string | undefined;\n\n  /**\n   * @deprecated\n   *\n   * <p>[Deprecated] Describes an update for a destination in Amazon S3.</p>\n   */\n  S3DestinationUpdate?: S3DestinationUpdate;\n\n  /**\n   * <p>Describes an update for a destination in Amazon S3.</p>\n   */\n  ExtendedS3DestinationUpdate?: ExtendedS3DestinationUpdate;\n\n  /**\n   * <p>Describes an update for a destination in Amazon Redshift.</p>\n   */\n  RedshiftDestinationUpdate?: RedshiftDestinationUpdate;\n\n  /**\n   * <p>Describes an update for a destination in Amazon ES.</p>\n   */\n  ElasticsearchDestinationUpdate?: ElasticsearchDestinationUpdate;\n\n  /**\n   * <p>Describes an update for a destination in Splunk.</p>\n   */\n  SplunkDestinationUpdate?: SplunkDestinationUpdate;\n\n  /**\n   * <p>Describes an update to the specified HTTP endpoint destination.</p>\n   */\n  HttpEndpointDestinationUpdate?: HttpEndpointDestinationUpdate;\n}\n\nexport namespace UpdateDestinationInput {\n  export const filterSensitiveLog = (obj: UpdateDestinationInput): any => ({\n    ...obj,\n    ...(obj.RedshiftDestinationUpdate && {\n      RedshiftDestinationUpdate: RedshiftDestinationUpdate.filterSensitiveLog(obj.RedshiftDestinationUpdate),\n    }),\n    ...(obj.HttpEndpointDestinationUpdate && {\n      HttpEndpointDestinationUpdate: HttpEndpointDestinationUpdate.filterSensitiveLog(\n        obj.HttpEndpointDestinationUpdate\n      ),\n    }),\n  });\n}\n\nexport interface UpdateDestinationOutput {}\n\nexport namespace UpdateDestinationOutput {\n  export const filterSensitiveLog = (obj: UpdateDestinationOutput): any => ({\n    ...obj,\n  });\n}\n"]},"metadata":{},"sourceType":"module"}