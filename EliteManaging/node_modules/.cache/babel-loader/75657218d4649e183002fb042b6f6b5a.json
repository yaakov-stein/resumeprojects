{"ast":null,"code":"import { __assign } from \"tslib\";\nexport var AccessDeniedException;\n\n(function (AccessDeniedException) {\n  AccessDeniedException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AccessDeniedException || (AccessDeniedException = {}));\n\nexport var AgeRange;\n\n(function (AgeRange) {\n  AgeRange.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AgeRange || (AgeRange = {}));\n\nexport var S3Object;\n\n(function (S3Object) {\n  S3Object.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(S3Object || (S3Object = {}));\n\nexport var GroundTruthManifest;\n\n(function (GroundTruthManifest) {\n  GroundTruthManifest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GroundTruthManifest || (GroundTruthManifest = {}));\n\nexport var Asset;\n\n(function (Asset) {\n  Asset.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Asset || (Asset = {}));\n\nexport var Attribute;\n\n(function (Attribute) {\n  Attribute[\"ALL\"] = \"ALL\";\n  Attribute[\"DEFAULT\"] = \"DEFAULT\";\n})(Attribute || (Attribute = {}));\n\nexport var AudioMetadata;\n\n(function (AudioMetadata) {\n  AudioMetadata.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AudioMetadata || (AudioMetadata = {}));\n\nexport var Beard;\n\n(function (Beard) {\n  Beard.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Beard || (Beard = {}));\n\nexport var BodyPart;\n\n(function (BodyPart) {\n  BodyPart[\"FACE\"] = \"FACE\";\n  BodyPart[\"HEAD\"] = \"HEAD\";\n  BodyPart[\"LEFT_HAND\"] = \"LEFT_HAND\";\n  BodyPart[\"RIGHT_HAND\"] = \"RIGHT_HAND\";\n})(BodyPart || (BodyPart = {}));\n\nexport var BoundingBox;\n\n(function (BoundingBox) {\n  BoundingBox.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(BoundingBox || (BoundingBox = {}));\n\nexport var CoversBodyPart;\n\n(function (CoversBodyPart) {\n  CoversBodyPart.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CoversBodyPart || (CoversBodyPart = {}));\n\nexport var ProtectiveEquipmentType;\n\n(function (ProtectiveEquipmentType) {\n  ProtectiveEquipmentType[\"FACE_COVER\"] = \"FACE_COVER\";\n  ProtectiveEquipmentType[\"HAND_COVER\"] = \"HAND_COVER\";\n  ProtectiveEquipmentType[\"HEAD_COVER\"] = \"HEAD_COVER\";\n})(ProtectiveEquipmentType || (ProtectiveEquipmentType = {}));\n\nexport var EquipmentDetection;\n\n(function (EquipmentDetection) {\n  EquipmentDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EquipmentDetection || (EquipmentDetection = {}));\n\nexport var ProtectiveEquipmentBodyPart;\n\n(function (ProtectiveEquipmentBodyPart) {\n  ProtectiveEquipmentBodyPart.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentBodyPart || (ProtectiveEquipmentBodyPart = {}));\n\nexport var LandmarkType;\n\n(function (LandmarkType) {\n  LandmarkType[\"chinBottom\"] = \"chinBottom\";\n  LandmarkType[\"eyeLeft\"] = \"eyeLeft\";\n  LandmarkType[\"eyeRight\"] = \"eyeRight\";\n  LandmarkType[\"leftEyeBrowLeft\"] = \"leftEyeBrowLeft\";\n  LandmarkType[\"leftEyeBrowRight\"] = \"leftEyeBrowRight\";\n  LandmarkType[\"leftEyeBrowUp\"] = \"leftEyeBrowUp\";\n  LandmarkType[\"leftEyeDown\"] = \"leftEyeDown\";\n  LandmarkType[\"leftEyeLeft\"] = \"leftEyeLeft\";\n  LandmarkType[\"leftEyeRight\"] = \"leftEyeRight\";\n  LandmarkType[\"leftEyeUp\"] = \"leftEyeUp\";\n  LandmarkType[\"leftPupil\"] = \"leftPupil\";\n  LandmarkType[\"midJawlineLeft\"] = \"midJawlineLeft\";\n  LandmarkType[\"midJawlineRight\"] = \"midJawlineRight\";\n  LandmarkType[\"mouthDown\"] = \"mouthDown\";\n  LandmarkType[\"mouthLeft\"] = \"mouthLeft\";\n  LandmarkType[\"mouthRight\"] = \"mouthRight\";\n  LandmarkType[\"mouthUp\"] = \"mouthUp\";\n  LandmarkType[\"nose\"] = \"nose\";\n  LandmarkType[\"noseLeft\"] = \"noseLeft\";\n  LandmarkType[\"noseRight\"] = \"noseRight\";\n  LandmarkType[\"rightEyeBrowLeft\"] = \"rightEyeBrowLeft\";\n  LandmarkType[\"rightEyeBrowRight\"] = \"rightEyeBrowRight\";\n  LandmarkType[\"rightEyeBrowUp\"] = \"rightEyeBrowUp\";\n  LandmarkType[\"rightEyeDown\"] = \"rightEyeDown\";\n  LandmarkType[\"rightEyeLeft\"] = \"rightEyeLeft\";\n  LandmarkType[\"rightEyeRight\"] = \"rightEyeRight\";\n  LandmarkType[\"rightEyeUp\"] = \"rightEyeUp\";\n  LandmarkType[\"rightPupil\"] = \"rightPupil\";\n  LandmarkType[\"upperJawlineLeft\"] = \"upperJawlineLeft\";\n  LandmarkType[\"upperJawlineRight\"] = \"upperJawlineRight\";\n})(LandmarkType || (LandmarkType = {}));\n\nexport var Landmark;\n\n(function (Landmark) {\n  Landmark.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Landmark || (Landmark = {}));\n\nexport var Pose;\n\n(function (Pose) {\n  Pose.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Pose || (Pose = {}));\n\nexport var ImageQuality;\n\n(function (ImageQuality) {\n  ImageQuality.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ImageQuality || (ImageQuality = {}));\n\nexport var ComparedFace;\n\n(function (ComparedFace) {\n  ComparedFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ComparedFace || (ComparedFace = {}));\n\nexport var Celebrity;\n\n(function (Celebrity) {\n  Celebrity.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Celebrity || (Celebrity = {}));\n\nexport var Emotion;\n\n(function (Emotion) {\n  Emotion.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Emotion || (Emotion = {}));\n\nexport var Eyeglasses;\n\n(function (Eyeglasses) {\n  Eyeglasses.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Eyeglasses || (Eyeglasses = {}));\n\nexport var EyeOpen;\n\n(function (EyeOpen) {\n  EyeOpen.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EyeOpen || (EyeOpen = {}));\n\nexport var GenderType;\n\n(function (GenderType) {\n  GenderType[\"Female\"] = \"Female\";\n  GenderType[\"Male\"] = \"Male\";\n})(GenderType || (GenderType = {}));\n\nexport var Gender;\n\n(function (Gender) {\n  Gender.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Gender || (Gender = {}));\n\nexport var MouthOpen;\n\n(function (MouthOpen) {\n  MouthOpen.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(MouthOpen || (MouthOpen = {}));\n\nexport var Mustache;\n\n(function (Mustache) {\n  Mustache.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Mustache || (Mustache = {}));\n\nexport var Smile;\n\n(function (Smile) {\n  Smile.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Smile || (Smile = {}));\n\nexport var Sunglasses;\n\n(function (Sunglasses) {\n  Sunglasses.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Sunglasses || (Sunglasses = {}));\n\nexport var FaceDetail;\n\n(function (FaceDetail) {\n  FaceDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceDetail || (FaceDetail = {}));\n\nexport var CelebrityDetail;\n\n(function (CelebrityDetail) {\n  CelebrityDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CelebrityDetail || (CelebrityDetail = {}));\n\nexport var CelebrityRecognition;\n\n(function (CelebrityRecognition) {\n  CelebrityRecognition.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CelebrityRecognition || (CelebrityRecognition = {}));\n\nexport var CelebrityRecognitionSortBy;\n\n(function (CelebrityRecognitionSortBy) {\n  CelebrityRecognitionSortBy[\"ID\"] = \"ID\";\n  CelebrityRecognitionSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(CelebrityRecognitionSortBy || (CelebrityRecognitionSortBy = {}));\n\nexport var ComparedSourceImageFace;\n\n(function (ComparedSourceImageFace) {\n  ComparedSourceImageFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ComparedSourceImageFace || (ComparedSourceImageFace = {}));\n\nexport var QualityFilter;\n\n(function (QualityFilter) {\n  QualityFilter[\"AUTO\"] = \"AUTO\";\n  QualityFilter[\"HIGH\"] = \"HIGH\";\n  QualityFilter[\"LOW\"] = \"LOW\";\n  QualityFilter[\"MEDIUM\"] = \"MEDIUM\";\n  QualityFilter[\"NONE\"] = \"NONE\";\n})(QualityFilter || (QualityFilter = {}));\n\nexport var Image;\n\n(function (Image) {\n  Image.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Image || (Image = {}));\n\nexport var CompareFacesRequest;\n\n(function (CompareFacesRequest) {\n  CompareFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesRequest || (CompareFacesRequest = {}));\n\nexport var CompareFacesMatch;\n\n(function (CompareFacesMatch) {\n  CompareFacesMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesMatch || (CompareFacesMatch = {}));\n\nexport var OrientationCorrection;\n\n(function (OrientationCorrection) {\n  OrientationCorrection[\"ROTATE_0\"] = \"ROTATE_0\";\n  OrientationCorrection[\"ROTATE_180\"] = \"ROTATE_180\";\n  OrientationCorrection[\"ROTATE_270\"] = \"ROTATE_270\";\n  OrientationCorrection[\"ROTATE_90\"] = \"ROTATE_90\";\n})(OrientationCorrection || (OrientationCorrection = {}));\n\nexport var CompareFacesResponse;\n\n(function (CompareFacesResponse) {\n  CompareFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesResponse || (CompareFacesResponse = {}));\n\nexport var ImageTooLargeException;\n\n(function (ImageTooLargeException) {\n  ImageTooLargeException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ImageTooLargeException || (ImageTooLargeException = {}));\n\nexport var InternalServerError;\n\n(function (InternalServerError) {\n  InternalServerError.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InternalServerError || (InternalServerError = {}));\n\nexport var InvalidImageFormatException;\n\n(function (InvalidImageFormatException) {\n  InvalidImageFormatException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidImageFormatException || (InvalidImageFormatException = {}));\n\nexport var InvalidParameterException;\n\n(function (InvalidParameterException) {\n  InvalidParameterException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidParameterException || (InvalidParameterException = {}));\n\nexport var InvalidS3ObjectException;\n\n(function (InvalidS3ObjectException) {\n  InvalidS3ObjectException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidS3ObjectException || (InvalidS3ObjectException = {}));\n\nexport var ProvisionedThroughputExceededException;\n\n(function (ProvisionedThroughputExceededException) {\n  ProvisionedThroughputExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProvisionedThroughputExceededException || (ProvisionedThroughputExceededException = {}));\n\nexport var ThrottlingException;\n\n(function (ThrottlingException) {\n  ThrottlingException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ThrottlingException || (ThrottlingException = {}));\n\nexport var ContentClassifier;\n\n(function (ContentClassifier) {\n  ContentClassifier[\"FREE_OF_ADULT_CONTENT\"] = \"FreeOfAdultContent\";\n  ContentClassifier[\"FREE_OF_PERSONALLY_IDENTIFIABLE_INFORMATION\"] = \"FreeOfPersonallyIdentifiableInformation\";\n})(ContentClassifier || (ContentClassifier = {}));\n\nexport var ModerationLabel;\n\n(function (ModerationLabel) {\n  ModerationLabel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ModerationLabel || (ModerationLabel = {}));\n\nexport var ContentModerationDetection;\n\n(function (ContentModerationDetection) {\n  ContentModerationDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ContentModerationDetection || (ContentModerationDetection = {}));\n\nexport var ContentModerationSortBy;\n\n(function (ContentModerationSortBy) {\n  ContentModerationSortBy[\"NAME\"] = \"NAME\";\n  ContentModerationSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(ContentModerationSortBy || (ContentModerationSortBy = {}));\n\nexport var CreateCollectionRequest;\n\n(function (CreateCollectionRequest) {\n  CreateCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateCollectionRequest || (CreateCollectionRequest = {}));\n\nexport var CreateCollectionResponse;\n\n(function (CreateCollectionResponse) {\n  CreateCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateCollectionResponse || (CreateCollectionResponse = {}));\n\nexport var ResourceAlreadyExistsException;\n\n(function (ResourceAlreadyExistsException) {\n  ResourceAlreadyExistsException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceAlreadyExistsException || (ResourceAlreadyExistsException = {}));\n\nexport var CreateProjectRequest;\n\n(function (CreateProjectRequest) {\n  CreateProjectRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectRequest || (CreateProjectRequest = {}));\n\nexport var CreateProjectResponse;\n\n(function (CreateProjectResponse) {\n  CreateProjectResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectResponse || (CreateProjectResponse = {}));\n\nexport var LimitExceededException;\n\n(function (LimitExceededException) {\n  LimitExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(LimitExceededException || (LimitExceededException = {}));\n\nexport var ResourceInUseException;\n\n(function (ResourceInUseException) {\n  ResourceInUseException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceInUseException || (ResourceInUseException = {}));\n\nexport var OutputConfig;\n\n(function (OutputConfig) {\n  OutputConfig.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(OutputConfig || (OutputConfig = {}));\n\nexport var TestingData;\n\n(function (TestingData) {\n  TestingData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TestingData || (TestingData = {}));\n\nexport var TrainingData;\n\n(function (TrainingData) {\n  TrainingData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TrainingData || (TrainingData = {}));\n\nexport var CreateProjectVersionRequest;\n\n(function (CreateProjectVersionRequest) {\n  CreateProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectVersionRequest || (CreateProjectVersionRequest = {}));\n\nexport var CreateProjectVersionResponse;\n\n(function (CreateProjectVersionResponse) {\n  CreateProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectVersionResponse || (CreateProjectVersionResponse = {}));\n\nexport var ResourceNotFoundException;\n\n(function (ResourceNotFoundException) {\n  ResourceNotFoundException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceNotFoundException || (ResourceNotFoundException = {}));\n\nexport var KinesisVideoStream;\n\n(function (KinesisVideoStream) {\n  KinesisVideoStream.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisVideoStream || (KinesisVideoStream = {}));\n\nexport var StreamProcessorInput;\n\n(function (StreamProcessorInput) {\n  StreamProcessorInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorInput || (StreamProcessorInput = {}));\n\nexport var KinesisDataStream;\n\n(function (KinesisDataStream) {\n  KinesisDataStream.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisDataStream || (KinesisDataStream = {}));\n\nexport var StreamProcessorOutput;\n\n(function (StreamProcessorOutput) {\n  StreamProcessorOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorOutput || (StreamProcessorOutput = {}));\n\nexport var FaceSearchSettings;\n\n(function (FaceSearchSettings) {\n  FaceSearchSettings.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceSearchSettings || (FaceSearchSettings = {}));\n\nexport var StreamProcessorSettings;\n\n(function (StreamProcessorSettings) {\n  StreamProcessorSettings.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorSettings || (StreamProcessorSettings = {}));\n\nexport var CreateStreamProcessorRequest;\n\n(function (CreateStreamProcessorRequest) {\n  CreateStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateStreamProcessorRequest || (CreateStreamProcessorRequest = {}));\n\nexport var CreateStreamProcessorResponse;\n\n(function (CreateStreamProcessorResponse) {\n  CreateStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateStreamProcessorResponse || (CreateStreamProcessorResponse = {}));\n\nexport var Point;\n\n(function (Point) {\n  Point.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Point || (Point = {}));\n\nexport var Geometry;\n\n(function (Geometry) {\n  Geometry.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Geometry || (Geometry = {}));\n\nexport var CustomLabel;\n\n(function (CustomLabel) {\n  CustomLabel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CustomLabel || (CustomLabel = {}));\n\nexport var DeleteCollectionRequest;\n\n(function (DeleteCollectionRequest) {\n  DeleteCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteCollectionRequest || (DeleteCollectionRequest = {}));\n\nexport var DeleteCollectionResponse;\n\n(function (DeleteCollectionResponse) {\n  DeleteCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteCollectionResponse || (DeleteCollectionResponse = {}));\n\nexport var DeleteFacesRequest;\n\n(function (DeleteFacesRequest) {\n  DeleteFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteFacesRequest || (DeleteFacesRequest = {}));\n\nexport var DeleteFacesResponse;\n\n(function (DeleteFacesResponse) {\n  DeleteFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteFacesResponse || (DeleteFacesResponse = {}));\n\nexport var DeleteProjectRequest;\n\n(function (DeleteProjectRequest) {\n  DeleteProjectRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectRequest || (DeleteProjectRequest = {}));\n\nexport var ProjectStatus;\n\n(function (ProjectStatus) {\n  ProjectStatus[\"CREATED\"] = \"CREATED\";\n  ProjectStatus[\"CREATING\"] = \"CREATING\";\n  ProjectStatus[\"DELETING\"] = \"DELETING\";\n})(ProjectStatus || (ProjectStatus = {}));\n\nexport var DeleteProjectResponse;\n\n(function (DeleteProjectResponse) {\n  DeleteProjectResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectResponse || (DeleteProjectResponse = {}));\n\nexport var DeleteProjectVersionRequest;\n\n(function (DeleteProjectVersionRequest) {\n  DeleteProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectVersionRequest || (DeleteProjectVersionRequest = {}));\n\nexport var ProjectVersionStatus;\n\n(function (ProjectVersionStatus) {\n  ProjectVersionStatus[\"DELETING\"] = \"DELETING\";\n  ProjectVersionStatus[\"FAILED\"] = \"FAILED\";\n  ProjectVersionStatus[\"RUNNING\"] = \"RUNNING\";\n  ProjectVersionStatus[\"STARTING\"] = \"STARTING\";\n  ProjectVersionStatus[\"STOPPED\"] = \"STOPPED\";\n  ProjectVersionStatus[\"STOPPING\"] = \"STOPPING\";\n  ProjectVersionStatus[\"TRAINING_COMPLETED\"] = \"TRAINING_COMPLETED\";\n  ProjectVersionStatus[\"TRAINING_FAILED\"] = \"TRAINING_FAILED\";\n  ProjectVersionStatus[\"TRAINING_IN_PROGRESS\"] = \"TRAINING_IN_PROGRESS\";\n})(ProjectVersionStatus || (ProjectVersionStatus = {}));\n\nexport var DeleteProjectVersionResponse;\n\n(function (DeleteProjectVersionResponse) {\n  DeleteProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectVersionResponse || (DeleteProjectVersionResponse = {}));\n\nexport var DeleteStreamProcessorRequest;\n\n(function (DeleteStreamProcessorRequest) {\n  DeleteStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteStreamProcessorRequest || (DeleteStreamProcessorRequest = {}));\n\nexport var DeleteStreamProcessorResponse;\n\n(function (DeleteStreamProcessorResponse) {\n  DeleteStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteStreamProcessorResponse || (DeleteStreamProcessorResponse = {}));\n\nexport var DescribeCollectionRequest;\n\n(function (DescribeCollectionRequest) {\n  DescribeCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeCollectionRequest || (DescribeCollectionRequest = {}));\n\nexport var DescribeCollectionResponse;\n\n(function (DescribeCollectionResponse) {\n  DescribeCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeCollectionResponse || (DescribeCollectionResponse = {}));\n\nexport var DescribeProjectsRequest;\n\n(function (DescribeProjectsRequest) {\n  DescribeProjectsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectsRequest || (DescribeProjectsRequest = {}));\n\nexport var ProjectDescription;\n\n(function (ProjectDescription) {\n  ProjectDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProjectDescription || (ProjectDescription = {}));\n\nexport var DescribeProjectsResponse;\n\n(function (DescribeProjectsResponse) {\n  DescribeProjectsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectsResponse || (DescribeProjectsResponse = {}));\n\nexport var InvalidPaginationTokenException;\n\n(function (InvalidPaginationTokenException) {\n  InvalidPaginationTokenException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidPaginationTokenException || (InvalidPaginationTokenException = {}));\n\nexport var DescribeProjectVersionsRequest;\n\n(function (DescribeProjectVersionsRequest) {\n  DescribeProjectVersionsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectVersionsRequest || (DescribeProjectVersionsRequest = {}));\n\nexport var Summary;\n\n(function (Summary) {\n  Summary.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Summary || (Summary = {}));\n\nexport var EvaluationResult;\n\n(function (EvaluationResult) {\n  EvaluationResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EvaluationResult || (EvaluationResult = {}));\n\nexport var ValidationData;\n\n(function (ValidationData) {\n  ValidationData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ValidationData || (ValidationData = {}));\n\nexport var TestingDataResult;\n\n(function (TestingDataResult) {\n  TestingDataResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TestingDataResult || (TestingDataResult = {}));\n\nexport var TrainingDataResult;\n\n(function (TrainingDataResult) {\n  TrainingDataResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TrainingDataResult || (TrainingDataResult = {}));\n\nexport var ProjectVersionDescription;\n\n(function (ProjectVersionDescription) {\n  ProjectVersionDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProjectVersionDescription || (ProjectVersionDescription = {}));\n\nexport var DescribeProjectVersionsResponse;\n\n(function (DescribeProjectVersionsResponse) {\n  DescribeProjectVersionsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectVersionsResponse || (DescribeProjectVersionsResponse = {}));\n\nexport var DescribeStreamProcessorRequest;\n\n(function (DescribeStreamProcessorRequest) {\n  DescribeStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeStreamProcessorRequest || (DescribeStreamProcessorRequest = {}));\n\nexport var StreamProcessorStatus;\n\n(function (StreamProcessorStatus) {\n  StreamProcessorStatus[\"FAILED\"] = \"FAILED\";\n  StreamProcessorStatus[\"RUNNING\"] = \"RUNNING\";\n  StreamProcessorStatus[\"STARTING\"] = \"STARTING\";\n  StreamProcessorStatus[\"STOPPED\"] = \"STOPPED\";\n  StreamProcessorStatus[\"STOPPING\"] = \"STOPPING\";\n})(StreamProcessorStatus || (StreamProcessorStatus = {}));\n\nexport var DescribeStreamProcessorResponse;\n\n(function (DescribeStreamProcessorResponse) {\n  DescribeStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeStreamProcessorResponse || (DescribeStreamProcessorResponse = {}));\n\nexport var DetectCustomLabelsRequest;\n\n(function (DetectCustomLabelsRequest) {\n  DetectCustomLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectCustomLabelsRequest || (DetectCustomLabelsRequest = {}));\n\nexport var DetectCustomLabelsResponse;\n\n(function (DetectCustomLabelsResponse) {\n  DetectCustomLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectCustomLabelsResponse || (DetectCustomLabelsResponse = {}));\n\nexport var ResourceNotReadyException;\n\n(function (ResourceNotReadyException) {\n  ResourceNotReadyException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceNotReadyException || (ResourceNotReadyException = {}));\n\nexport var DetectFacesRequest;\n\n(function (DetectFacesRequest) {\n  DetectFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectFacesRequest || (DetectFacesRequest = {}));\n\nexport var DetectFacesResponse;\n\n(function (DetectFacesResponse) {\n  DetectFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectFacesResponse || (DetectFacesResponse = {}));\n\nexport var DetectionFilter;\n\n(function (DetectionFilter) {\n  DetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectionFilter || (DetectionFilter = {}));\n\nexport var DetectLabelsRequest;\n\n(function (DetectLabelsRequest) {\n  DetectLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectLabelsRequest || (DetectLabelsRequest = {}));\n\nexport var Instance;\n\n(function (Instance) {\n  Instance.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Instance || (Instance = {}));\n\nexport var Parent;\n\n(function (Parent) {\n  Parent.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Parent || (Parent = {}));\n\nexport var Label;\n\n(function (Label) {\n  Label.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Label || (Label = {}));\n\nexport var DetectLabelsResponse;\n\n(function (DetectLabelsResponse) {\n  DetectLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectLabelsResponse || (DetectLabelsResponse = {}));\n\nexport var HumanLoopDataAttributes;\n\n(function (HumanLoopDataAttributes) {\n  HumanLoopDataAttributes.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopDataAttributes || (HumanLoopDataAttributes = {}));\n\nexport var HumanLoopConfig;\n\n(function (HumanLoopConfig) {\n  HumanLoopConfig.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopConfig || (HumanLoopConfig = {}));\n\nexport var DetectModerationLabelsRequest;\n\n(function (DetectModerationLabelsRequest) {\n  DetectModerationLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectModerationLabelsRequest || (DetectModerationLabelsRequest = {}));\n\nexport var HumanLoopActivationOutput;\n\n(function (HumanLoopActivationOutput) {\n  HumanLoopActivationOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopActivationOutput || (HumanLoopActivationOutput = {}));\n\nexport var DetectModerationLabelsResponse;\n\n(function (DetectModerationLabelsResponse) {\n  DetectModerationLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectModerationLabelsResponse || (DetectModerationLabelsResponse = {}));\n\nexport var HumanLoopQuotaExceededException;\n\n(function (HumanLoopQuotaExceededException) {\n  HumanLoopQuotaExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopQuotaExceededException || (HumanLoopQuotaExceededException = {}));\n\nexport var ProtectiveEquipmentSummarizationAttributes;\n\n(function (ProtectiveEquipmentSummarizationAttributes) {\n  ProtectiveEquipmentSummarizationAttributes.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentSummarizationAttributes || (ProtectiveEquipmentSummarizationAttributes = {}));\n\nexport var DetectProtectiveEquipmentRequest;\n\n(function (DetectProtectiveEquipmentRequest) {\n  DetectProtectiveEquipmentRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectProtectiveEquipmentRequest || (DetectProtectiveEquipmentRequest = {}));\n\nexport var ProtectiveEquipmentPerson;\n\n(function (ProtectiveEquipmentPerson) {\n  ProtectiveEquipmentPerson.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentPerson || (ProtectiveEquipmentPerson = {}));\n\nexport var ProtectiveEquipmentSummary;\n\n(function (ProtectiveEquipmentSummary) {\n  ProtectiveEquipmentSummary.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentSummary || (ProtectiveEquipmentSummary = {}));\n\nexport var DetectProtectiveEquipmentResponse;\n\n(function (DetectProtectiveEquipmentResponse) {\n  DetectProtectiveEquipmentResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectProtectiveEquipmentResponse || (DetectProtectiveEquipmentResponse = {}));\n\nexport var RegionOfInterest;\n\n(function (RegionOfInterest) {\n  RegionOfInterest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RegionOfInterest || (RegionOfInterest = {}));\n\nexport var DetectTextFilters;\n\n(function (DetectTextFilters) {\n  DetectTextFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextFilters || (DetectTextFilters = {}));\n\nexport var DetectTextRequest;\n\n(function (DetectTextRequest) {\n  DetectTextRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextRequest || (DetectTextRequest = {}));\n\nexport var TextTypes;\n\n(function (TextTypes) {\n  TextTypes[\"LINE\"] = \"LINE\";\n  TextTypes[\"WORD\"] = \"WORD\";\n})(TextTypes || (TextTypes = {}));\n\nexport var TextDetection;\n\n(function (TextDetection) {\n  TextDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TextDetection || (TextDetection = {}));\n\nexport var DetectTextResponse;\n\n(function (DetectTextResponse) {\n  DetectTextResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextResponse || (DetectTextResponse = {}));\n\nexport var Face;\n\n(function (Face) {\n  Face.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Face || (Face = {}));\n\nexport var FaceAttributes;\n\n(function (FaceAttributes) {\n  FaceAttributes[\"ALL\"] = \"ALL\";\n  FaceAttributes[\"DEFAULT\"] = \"DEFAULT\";\n})(FaceAttributes || (FaceAttributes = {}));\n\nexport var FaceDetection;\n\n(function (FaceDetection) {\n  FaceDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceDetection || (FaceDetection = {}));\n\nexport var FaceMatch;\n\n(function (FaceMatch) {\n  FaceMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceMatch || (FaceMatch = {}));\n\nexport var FaceRecord;\n\n(function (FaceRecord) {\n  FaceRecord.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceRecord || (FaceRecord = {}));\n\nexport var FaceSearchSortBy;\n\n(function (FaceSearchSortBy) {\n  FaceSearchSortBy[\"INDEX\"] = \"INDEX\";\n  FaceSearchSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(FaceSearchSortBy || (FaceSearchSortBy = {}));\n\nexport var GetCelebrityInfoRequest;\n\n(function (GetCelebrityInfoRequest) {\n  GetCelebrityInfoRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityInfoRequest || (GetCelebrityInfoRequest = {}));\n\nexport var GetCelebrityInfoResponse;\n\n(function (GetCelebrityInfoResponse) {\n  GetCelebrityInfoResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityInfoResponse || (GetCelebrityInfoResponse = {}));\n\nexport var GetCelebrityRecognitionRequest;\n\n(function (GetCelebrityRecognitionRequest) {\n  GetCelebrityRecognitionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityRecognitionRequest || (GetCelebrityRecognitionRequest = {}));\n\nexport var VideoJobStatus;\n\n(function (VideoJobStatus) {\n  VideoJobStatus[\"FAILED\"] = \"FAILED\";\n  VideoJobStatus[\"IN_PROGRESS\"] = \"IN_PROGRESS\";\n  VideoJobStatus[\"SUCCEEDED\"] = \"SUCCEEDED\";\n})(VideoJobStatus || (VideoJobStatus = {}));\n\nexport var VideoMetadata;\n\n(function (VideoMetadata) {\n  VideoMetadata.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VideoMetadata || (VideoMetadata = {}));\n\nexport var GetCelebrityRecognitionResponse;\n\n(function (GetCelebrityRecognitionResponse) {\n  GetCelebrityRecognitionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityRecognitionResponse || (GetCelebrityRecognitionResponse = {}));\n\nexport var GetContentModerationRequest;\n\n(function (GetContentModerationRequest) {\n  GetContentModerationRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetContentModerationRequest || (GetContentModerationRequest = {}));\n\nexport var GetContentModerationResponse;\n\n(function (GetContentModerationResponse) {\n  GetContentModerationResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetContentModerationResponse || (GetContentModerationResponse = {}));\n\nexport var GetFaceDetectionRequest;\n\n(function (GetFaceDetectionRequest) {\n  GetFaceDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceDetectionRequest || (GetFaceDetectionRequest = {}));\n\nexport var GetFaceDetectionResponse;\n\n(function (GetFaceDetectionResponse) {\n  GetFaceDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceDetectionResponse || (GetFaceDetectionResponse = {}));\n\nexport var GetFaceSearchRequest;\n\n(function (GetFaceSearchRequest) {\n  GetFaceSearchRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceSearchRequest || (GetFaceSearchRequest = {}));\n\nexport var PersonDetail;\n\n(function (PersonDetail) {\n  PersonDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonDetail || (PersonDetail = {}));\n\nexport var PersonMatch;\n\n(function (PersonMatch) {\n  PersonMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonMatch || (PersonMatch = {}));\n\nexport var GetFaceSearchResponse;\n\n(function (GetFaceSearchResponse) {\n  GetFaceSearchResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceSearchResponse || (GetFaceSearchResponse = {}));\n\nexport var LabelDetectionSortBy;\n\n(function (LabelDetectionSortBy) {\n  LabelDetectionSortBy[\"NAME\"] = \"NAME\";\n  LabelDetectionSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(LabelDetectionSortBy || (LabelDetectionSortBy = {}));\n\nexport var GetLabelDetectionRequest;\n\n(function (GetLabelDetectionRequest) {\n  GetLabelDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetLabelDetectionRequest || (GetLabelDetectionRequest = {}));\n\nexport var LabelDetection;\n\n(function (LabelDetection) {\n  LabelDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(LabelDetection || (LabelDetection = {}));\n\nexport var GetLabelDetectionResponse;\n\n(function (GetLabelDetectionResponse) {\n  GetLabelDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetLabelDetectionResponse || (GetLabelDetectionResponse = {}));\n\nexport var PersonTrackingSortBy;\n\n(function (PersonTrackingSortBy) {\n  PersonTrackingSortBy[\"INDEX\"] = \"INDEX\";\n  PersonTrackingSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(PersonTrackingSortBy || (PersonTrackingSortBy = {}));\n\nexport var GetPersonTrackingRequest;\n\n(function (GetPersonTrackingRequest) {\n  GetPersonTrackingRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetPersonTrackingRequest || (GetPersonTrackingRequest = {}));\n\nexport var PersonDetection;\n\n(function (PersonDetection) {\n  PersonDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonDetection || (PersonDetection = {}));\n\nexport var GetPersonTrackingResponse;\n\n(function (GetPersonTrackingResponse) {\n  GetPersonTrackingResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetPersonTrackingResponse || (GetPersonTrackingResponse = {}));\n\nexport var GetSegmentDetectionRequest;\n\n(function (GetSegmentDetectionRequest) {\n  GetSegmentDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetSegmentDetectionRequest || (GetSegmentDetectionRequest = {}));\n\nexport var ShotSegment;\n\n(function (ShotSegment) {\n  ShotSegment.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ShotSegment || (ShotSegment = {}));\n\nexport var TechnicalCueType;\n\n(function (TechnicalCueType) {\n  TechnicalCueType[\"BLACK_FRAMES\"] = \"BlackFrames\";\n  TechnicalCueType[\"COLOR_BARS\"] = \"ColorBars\";\n  TechnicalCueType[\"END_CREDITS\"] = \"EndCredits\";\n})(TechnicalCueType || (TechnicalCueType = {}));\n\nexport var TechnicalCueSegment;\n\n(function (TechnicalCueSegment) {\n  TechnicalCueSegment.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TechnicalCueSegment || (TechnicalCueSegment = {}));\n\nexport var SegmentType;\n\n(function (SegmentType) {\n  SegmentType[\"SHOT\"] = \"SHOT\";\n  SegmentType[\"TECHNICAL_CUE\"] = \"TECHNICAL_CUE\";\n})(SegmentType || (SegmentType = {}));\n\nexport var SegmentDetection;\n\n(function (SegmentDetection) {\n  SegmentDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SegmentDetection || (SegmentDetection = {}));\n\nexport var SegmentTypeInfo;\n\n(function (SegmentTypeInfo) {\n  SegmentTypeInfo.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SegmentTypeInfo || (SegmentTypeInfo = {}));\n\nexport var GetSegmentDetectionResponse;\n\n(function (GetSegmentDetectionResponse) {\n  GetSegmentDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetSegmentDetectionResponse || (GetSegmentDetectionResponse = {}));\n\nexport var GetTextDetectionRequest;\n\n(function (GetTextDetectionRequest) {\n  GetTextDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetTextDetectionRequest || (GetTextDetectionRequest = {}));\n\nexport var TextDetectionResult;\n\n(function (TextDetectionResult) {\n  TextDetectionResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TextDetectionResult || (TextDetectionResult = {}));\n\nexport var GetTextDetectionResponse;\n\n(function (GetTextDetectionResponse) {\n  GetTextDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetTextDetectionResponse || (GetTextDetectionResponse = {}));\n\nexport var IdempotentParameterMismatchException;\n\n(function (IdempotentParameterMismatchException) {\n  IdempotentParameterMismatchException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IdempotentParameterMismatchException || (IdempotentParameterMismatchException = {}));\n\nexport var IndexFacesRequest;\n\n(function (IndexFacesRequest) {\n  IndexFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IndexFacesRequest || (IndexFacesRequest = {}));\n\nexport var Reason;\n\n(function (Reason) {\n  Reason[\"EXCEEDS_MAX_FACES\"] = \"EXCEEDS_MAX_FACES\";\n  Reason[\"EXTREME_POSE\"] = \"EXTREME_POSE\";\n  Reason[\"LOW_BRIGHTNESS\"] = \"LOW_BRIGHTNESS\";\n  Reason[\"LOW_CONFIDENCE\"] = \"LOW_CONFIDENCE\";\n  Reason[\"LOW_FACE_QUALITY\"] = \"LOW_FACE_QUALITY\";\n  Reason[\"LOW_SHARPNESS\"] = \"LOW_SHARPNESS\";\n  Reason[\"SMALL_BOUNDING_BOX\"] = \"SMALL_BOUNDING_BOX\";\n})(Reason || (Reason = {}));\n\nexport var UnindexedFace;\n\n(function (UnindexedFace) {\n  UnindexedFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(UnindexedFace || (UnindexedFace = {}));\n\nexport var IndexFacesResponse;\n\n(function (IndexFacesResponse) {\n  IndexFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IndexFacesResponse || (IndexFacesResponse = {}));\n\nexport var ServiceQuotaExceededException;\n\n(function (ServiceQuotaExceededException) {\n  ServiceQuotaExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ServiceQuotaExceededException || (ServiceQuotaExceededException = {}));\n\nexport var ListCollectionsRequest;\n\n(function (ListCollectionsRequest) {\n  ListCollectionsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListCollectionsRequest || (ListCollectionsRequest = {}));\n\nexport var ListCollectionsResponse;\n\n(function (ListCollectionsResponse) {\n  ListCollectionsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListCollectionsResponse || (ListCollectionsResponse = {}));\n\nexport var ListFacesRequest;\n\n(function (ListFacesRequest) {\n  ListFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListFacesRequest || (ListFacesRequest = {}));\n\nexport var ListFacesResponse;\n\n(function (ListFacesResponse) {\n  ListFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListFacesResponse || (ListFacesResponse = {}));\n\nexport var ListStreamProcessorsRequest;\n\n(function (ListStreamProcessorsRequest) {\n  ListStreamProcessorsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListStreamProcessorsRequest || (ListStreamProcessorsRequest = {}));\n\nexport var StreamProcessor;\n\n(function (StreamProcessor) {\n  StreamProcessor.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessor || (StreamProcessor = {}));\n\nexport var ListStreamProcessorsResponse;\n\n(function (ListStreamProcessorsResponse) {\n  ListStreamProcessorsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListStreamProcessorsResponse || (ListStreamProcessorsResponse = {}));\n\nexport var NotificationChannel;\n\n(function (NotificationChannel) {\n  NotificationChannel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(NotificationChannel || (NotificationChannel = {}));\n\nexport var RecognizeCelebritiesRequest;\n\n(function (RecognizeCelebritiesRequest) {\n  RecognizeCelebritiesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RecognizeCelebritiesRequest || (RecognizeCelebritiesRequest = {}));\n\nexport var RecognizeCelebritiesResponse;\n\n(function (RecognizeCelebritiesResponse) {\n  RecognizeCelebritiesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RecognizeCelebritiesResponse || (RecognizeCelebritiesResponse = {}));\n\nexport var SearchFacesRequest;\n\n(function (SearchFacesRequest) {\n  SearchFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesRequest || (SearchFacesRequest = {}));\n\nexport var SearchFacesResponse;\n\n(function (SearchFacesResponse) {\n  SearchFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesResponse || (SearchFacesResponse = {}));\n\nexport var SearchFacesByImageRequest;\n\n(function (SearchFacesByImageRequest) {\n  SearchFacesByImageRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesByImageRequest || (SearchFacesByImageRequest = {}));\n\nexport var SearchFacesByImageResponse;\n\n(function (SearchFacesByImageResponse) {\n  SearchFacesByImageResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesByImageResponse || (SearchFacesByImageResponse = {}));\n\nexport var Video;\n\n(function (Video) {\n  Video.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Video || (Video = {}));\n\nexport var StartCelebrityRecognitionRequest;\n\n(function (StartCelebrityRecognitionRequest) {\n  StartCelebrityRecognitionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartCelebrityRecognitionRequest || (StartCelebrityRecognitionRequest = {}));\n\nexport var StartCelebrityRecognitionResponse;\n\n(function (StartCelebrityRecognitionResponse) {\n  StartCelebrityRecognitionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartCelebrityRecognitionResponse || (StartCelebrityRecognitionResponse = {}));\n\nexport var VideoTooLargeException;\n\n(function (VideoTooLargeException) {\n  VideoTooLargeException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VideoTooLargeException || (VideoTooLargeException = {}));\n\nexport var StartContentModerationRequest;\n\n(function (StartContentModerationRequest) {\n  StartContentModerationRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartContentModerationRequest || (StartContentModerationRequest = {}));\n\nexport var StartContentModerationResponse;\n\n(function (StartContentModerationResponse) {\n  StartContentModerationResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartContentModerationResponse || (StartContentModerationResponse = {}));\n\nexport var StartFaceDetectionRequest;\n\n(function (StartFaceDetectionRequest) {\n  StartFaceDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceDetectionRequest || (StartFaceDetectionRequest = {}));\n\nexport var StartFaceDetectionResponse;\n\n(function (StartFaceDetectionResponse) {\n  StartFaceDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceDetectionResponse || (StartFaceDetectionResponse = {}));\n\nexport var StartFaceSearchRequest;\n\n(function (StartFaceSearchRequest) {\n  StartFaceSearchRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceSearchRequest || (StartFaceSearchRequest = {}));\n\nexport var StartFaceSearchResponse;\n\n(function (StartFaceSearchResponse) {\n  StartFaceSearchResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceSearchResponse || (StartFaceSearchResponse = {}));\n\nexport var StartLabelDetectionRequest;\n\n(function (StartLabelDetectionRequest) {\n  StartLabelDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartLabelDetectionRequest || (StartLabelDetectionRequest = {}));\n\nexport var StartLabelDetectionResponse;\n\n(function (StartLabelDetectionResponse) {\n  StartLabelDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartLabelDetectionResponse || (StartLabelDetectionResponse = {}));\n\nexport var StartPersonTrackingRequest;\n\n(function (StartPersonTrackingRequest) {\n  StartPersonTrackingRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartPersonTrackingRequest || (StartPersonTrackingRequest = {}));\n\nexport var StartPersonTrackingResponse;\n\n(function (StartPersonTrackingResponse) {\n  StartPersonTrackingResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartPersonTrackingResponse || (StartPersonTrackingResponse = {}));\n\nexport var StartProjectVersionRequest;\n\n(function (StartProjectVersionRequest) {\n  StartProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartProjectVersionRequest || (StartProjectVersionRequest = {}));\n\nexport var StartProjectVersionResponse;\n\n(function (StartProjectVersionResponse) {\n  StartProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartProjectVersionResponse || (StartProjectVersionResponse = {}));\n\nexport var StartShotDetectionFilter;\n\n(function (StartShotDetectionFilter) {\n  StartShotDetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartShotDetectionFilter || (StartShotDetectionFilter = {}));\n\nexport var StartTechnicalCueDetectionFilter;\n\n(function (StartTechnicalCueDetectionFilter) {\n  StartTechnicalCueDetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTechnicalCueDetectionFilter || (StartTechnicalCueDetectionFilter = {}));\n\nexport var StartSegmentDetectionFilters;\n\n(function (StartSegmentDetectionFilters) {\n  StartSegmentDetectionFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionFilters || (StartSegmentDetectionFilters = {}));\n\nexport var StartSegmentDetectionRequest;\n\n(function (StartSegmentDetectionRequest) {\n  StartSegmentDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionRequest || (StartSegmentDetectionRequest = {}));\n\nexport var StartSegmentDetectionResponse;\n\n(function (StartSegmentDetectionResponse) {\n  StartSegmentDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionResponse || (StartSegmentDetectionResponse = {}));\n\nexport var StartStreamProcessorRequest;\n\n(function (StartStreamProcessorRequest) {\n  StartStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartStreamProcessorRequest || (StartStreamProcessorRequest = {}));\n\nexport var StartStreamProcessorResponse;\n\n(function (StartStreamProcessorResponse) {\n  StartStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartStreamProcessorResponse || (StartStreamProcessorResponse = {}));\n\nexport var StartTextDetectionFilters;\n\n(function (StartTextDetectionFilters) {\n  StartTextDetectionFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionFilters || (StartTextDetectionFilters = {}));\n\nexport var StartTextDetectionRequest;\n\n(function (StartTextDetectionRequest) {\n  StartTextDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionRequest || (StartTextDetectionRequest = {}));\n\nexport var StartTextDetectionResponse;\n\n(function (StartTextDetectionResponse) {\n  StartTextDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionResponse || (StartTextDetectionResponse = {}));\n\nexport var StopProjectVersionRequest;\n\n(function (StopProjectVersionRequest) {\n  StopProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopProjectVersionRequest || (StopProjectVersionRequest = {}));\n\nexport var StopProjectVersionResponse;\n\n(function (StopProjectVersionResponse) {\n  StopProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopProjectVersionResponse || (StopProjectVersionResponse = {}));\n\nexport var StopStreamProcessorRequest;\n\n(function (StopStreamProcessorRequest) {\n  StopStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopStreamProcessorRequest || (StopStreamProcessorRequest = {}));\n\nexport var StopStreamProcessorResponse;\n\n(function (StopStreamProcessorResponse) {\n  StopStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopStreamProcessorResponse || (StopStreamProcessorResponse = {}));","map":{"version":3,"mappings":";AAiBA,OAAM,IAAWA,qBAAX;;AAAN,WAAiBA,qBAAjB,EAAsC;EACvBA,2CAAqB,UAACC,GAAD,EAA2B;IAAU,oBAClEA,GADkE;EAErE,CAFW;AAGd,CAJD,EAAiBD,qBAAqB,KAArBA,qBAAqB,MAAtC;;AAwBA,OAAM,IAAWE,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAACD,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiBC,QAAQ,KAARA,QAAQ,MAAzB;;AAgCA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAACF,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiBE,QAAQ,KAARA,QAAQ,MAAzB;;AAuBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACH,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBG,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAmBA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAACJ,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiBI,KAAK,KAALA,KAAK,MAAtB;;AAMA,WAAYC,SAAZ;;AAAA,WAAYA,SAAZ,EAAqB;EACnBA;EACAA;AACD,CAHD,EAAYA,SAAS,KAATA,SAAS,MAArB;;AA+BA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAACN,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiBM,aAAa,KAAbA,aAAa,MAA9B;;AAsBA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAACP,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiBO,KAAK,KAALA,KAAK,MAAtB;;AAMA,WAAYC,QAAZ;;AAAA,WAAYA,QAAZ,EAAoB;EAClBA;EACAA;EACAA;EACAA;AACD,CALD,EAAYA,QAAQ,KAARA,QAAQ,MAApB;;AAiDA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAACT,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiBS,WAAW,KAAXA,WAAW,MAA5B;;AAsBA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAACV,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiBU,cAAc,KAAdA,cAAc,MAA/B;;AAMA,WAAYC,uBAAZ;;AAAA,WAAYA,uBAAZ,EAAmC;EACjCA;EACAA;EACAA;AACD,CAJD,EAAYA,uBAAuB,KAAvBA,uBAAuB,MAAnC;;AAiCA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACZ,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBY,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AA6BA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACb,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBa,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAMA,WAAYC,YAAZ;;AAAA,WAAYA,YAAZ,EAAwB;EACtBA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;AACD,CA/BD,EAAYA,YAAY,KAAZA,YAAY,MAAxB;;AAyDA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAACf,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiBe,QAAQ,KAARA,QAAQ,MAAzB;;AA0BA,OAAM,IAAWC,IAAX;;AAAN,WAAiBA,IAAjB,EAAqB;EACNA,0BAAqB,UAAChB,GAAD,EAAU;IAAU,oBACjDA,GADiD;EAEpD,CAFW;AAGd,CAJD,EAAiBgB,IAAI,KAAJA,IAAI,MAArB;;AAuBA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAACjB,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiBiB,YAAY,KAAZA,YAAY,MAA7B;;AAqCA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAAClB,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiBkB,YAAY,KAAZA,YAAY,MAA7B;;AAuCA,OAAM,IAAWC,SAAX;;AAAN,WAAiBA,SAAjB,EAA0B;EACXA,+BAAqB,UAACnB,GAAD,EAAe;IAAU,oBACtDA,GADsD;EAEzD,CAFW;AAGd,CAJD,EAAiBmB,SAAS,KAATA,SAAS,MAA1B;;AAmCA,OAAM,IAAWC,OAAX;;AAAN,WAAiBA,OAAjB,EAAwB;EACTA,6BAAqB,UAACpB,GAAD,EAAa;IAAU,oBACpDA,GADoD;EAEvD,CAFW;AAGd,CAJD,EAAiBoB,OAAO,KAAPA,OAAO,MAAxB;;AAsBA,OAAM,IAAWC,UAAX;;AAAN,WAAiBA,UAAjB,EAA2B;EACZA,gCAAqB,UAACrB,GAAD,EAAgB;IAAU,oBACvDA,GADuD;EAE1D,CAFW;AAGd,CAJD,EAAiBqB,UAAU,KAAVA,UAAU,MAA3B;;AAsBA,OAAM,IAAWC,OAAX;;AAAN,WAAiBA,OAAjB,EAAwB;EACTA,6BAAqB,UAACtB,GAAD,EAAa;IAAU,oBACpDA,GADoD;EAEvD,CAFW;AAGd,CAJD,EAAiBsB,OAAO,KAAPA,OAAO,MAAxB;;AAMA,WAAYC,UAAZ;;AAAA,WAAYA,UAAZ,EAAsB;EACpBA;EACAA;AACD,CAHD,EAAYA,UAAU,KAAVA,UAAU,MAAtB;;AAiCA,OAAM,IAAWC,MAAX;;AAAN,WAAiBA,MAAjB,EAAuB;EACRA,4BAAqB,UAACxB,GAAD,EAAY;IAAU,oBACnDA,GADmD;EAEtD,CAFW;AAGd,CAJD,EAAiBwB,MAAM,KAANA,MAAM,MAAvB;;AAsBA,OAAM,IAAWC,SAAX;;AAAN,WAAiBA,SAAjB,EAA0B;EACXA,+BAAqB,UAACzB,GAAD,EAAe;IAAU,oBACtDA,GADsD;EAEzD,CAFW;AAGd,CAJD,EAAiByB,SAAS,KAATA,SAAS,MAA1B;;AAsBA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAAC1B,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiB0B,QAAQ,KAARA,QAAQ,MAAzB;;AAsBA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAAC3B,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiB2B,KAAK,KAALA,KAAK,MAAtB;;AAsBA,OAAM,IAAWC,UAAX;;AAAN,WAAiBA,UAAjB,EAA2B;EACZA,gCAAqB,UAAC5B,GAAD,EAAgB;IAAU,oBACvDA,GADuD;EAE1D,CAFW;AAGd,CAJD,EAAiB4B,UAAU,KAAVA,UAAU,MAA3B;;AAyHA,OAAM,IAAWC,UAAX;;AAAN,WAAiBA,UAAjB,EAA2B;EACZA,gCAAqB,UAAC7B,GAAD,EAAgB;IAAU,oBACvDA,GADuD;EAE1D,CAFW;AAGd,CAJD,EAAiB6B,UAAU,KAAVA,UAAU,MAA3B;;AAyCA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAAC9B,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiB8B,eAAe,KAAfA,eAAe,MAAhC;;AAsBA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAAC/B,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiB+B,oBAAoB,KAApBA,oBAAoB,MAArC;;AAMA,WAAYC,0BAAZ;;AAAA,WAAYA,0BAAZ,EAAsC;EACpCA;EACAA;AACD,CAHD,EAAYA,0BAA0B,KAA1BA,0BAA0B,MAAtC;;AAuBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACjC,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBiC,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAMA,WAAYC,aAAZ;;AAAA,WAAYA,aAAZ,EAAyB;EACvBA;EACAA;EACAA;EACAA;EACAA;AACD,CAND,EAAYA,aAAa,KAAbA,aAAa,MAAzB;;AA6CA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAACnC,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiBmC,KAAK,KAALA,KAAK,MAAtB;;AAmDA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACpC,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBoC,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAyBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAACrC,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBqC,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAMA,WAAYC,qBAAZ;;AAAA,WAAYA,qBAAZ,EAAiC;EAC/BA;EACAA;EACAA;EACAA;AACD,CALD,EAAYA,qBAAqB,KAArBA,qBAAqB,MAAjC;;AAwDA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACvC,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBuC,oBAAoB,KAApBA,oBAAoB,MAArC;;AAqBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACxC,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiBwC,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAoBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACzC,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiByC,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAoBA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC1C,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB0C,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAqBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC3C,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB2C,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAoBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC5C,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB4C,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAqBA,OAAM,IAAWC,sCAAX;;AAAN,WAAiBA,sCAAjB,EAAuD;EACxCA,4DAAqB,UAAC7C,GAAD,EAA4C;IAAU,oBACnFA,GADmF;EAEtF,CAFW;AAGd,CAJD,EAAiB6C,sCAAsC,KAAtCA,sCAAsC,MAAvD;;AAoBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC9C,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB8C,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAMA,WAAYC,iBAAZ;;AAAA,WAAYA,iBAAZ,EAA6B;EAC3BA;EACAA;AACD,CAHD,EAAYA,iBAAiB,KAAjBA,iBAAiB,MAA7B;;AAgCA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAAChD,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiBgD,eAAe,KAAfA,eAAe,MAAhC;;AAqBA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACjD,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBiD,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAMA,WAAYC,uBAAZ;;AAAA,WAAYA,uBAAZ,EAAmC;EACjCA;EACAA;AACD,CAHD,EAAYA,uBAAuB,KAAvBA,uBAAuB,MAAnC;;AAYA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACnD,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBmD,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAwBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACpD,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiBoD,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAoBA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACrD,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBqD,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAaA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACtD,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBsD,oBAAoB,KAApBA,oBAAoB,MAArC;;AAcA,OAAM,IAAWC,qBAAX;;AAAN,WAAiBA,qBAAjB,EAAsC;EACvBA,2CAAqB,UAACvD,GAAD,EAA2B;IAAU,oBAClEA,GADkE;EAErE,CAFW;AAGd,CAJD,EAAiBuD,qBAAqB,KAArBA,qBAAqB,MAAtC;;AAsBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACxD,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiBwD,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAoBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACzD,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiByD,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAqBA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAAC1D,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiB0D,YAAY,KAAZA,YAAY,MAA7B;;AAsBA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAAC3D,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiB2D,WAAW,KAAXA,WAAW,MAA5B;;AAgBA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAAC5D,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiB4D,YAAY,KAAZA,YAAY,MAA7B;;AAkCA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC7D,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB6D,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAcA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAAC9D,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiB8D,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAoBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC/D,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB+D,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAiBA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAChE,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBgE,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAgBA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACjE,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBiE,oBAAoB,KAApBA,oBAAoB,MAArC;;AAiBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAAClE,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBkE,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAiBA,OAAM,IAAWC,qBAAX;;AAAN,WAAiBA,qBAAjB,EAAsC;EACvBA,2CAAqB,UAACnE,GAAD,EAA2B;IAAU,oBAClEA,GADkE;EAErE,CAFW;AAGd,CAJD,EAAiBmE,qBAAqB,KAArBA,qBAAqB,MAAtC;;AAuBA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACpE,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBoE,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAgBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACrE,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBqE,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAqCA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACtE,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBsE,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAaA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAACvE,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBuE,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AA4BA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAACxE,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiBwE,KAAK,KAALA,KAAK,MAAtB;;AAuBA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAACzE,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiByE,QAAQ,KAARA,QAAQ,MAAzB;;AA6BA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAAC1E,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiB0E,WAAW,KAAXA,WAAW,MAA5B;;AAaA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAC3E,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiB2E,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAaA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC5E,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB4E,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAkBA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAC7E,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiB6E,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAaA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC9E,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB8E,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAaA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAAC/E,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiB+E,oBAAoB,KAApBA,oBAAoB,MAArC;;AAMA,WAAYC,aAAZ;;AAAA,WAAYA,aAAZ,EAAyB;EACvBA;EACAA;EACAA;AACD,CAJD,EAAYA,aAAa,KAAbA,aAAa,MAAzB;;AAaA,OAAM,IAAWC,qBAAX;;AAAN,WAAiBA,qBAAjB,EAAsC;EACvBA,2CAAqB,UAACjF,GAAD,EAA2B;IAAU,oBAClEA,GADkE;EAErE,CAFW;AAGd,CAJD,EAAiBiF,qBAAqB,KAArBA,qBAAqB,MAAtC;;AAaA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAClF,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBkF,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAMA,WAAYC,oBAAZ;;AAAA,WAAYA,oBAAZ,EAAgC;EAC9BA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;AACD,CAVD,EAAYA,oBAAoB,KAApBA,oBAAoB,MAAhC;;AAmBA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACpF,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBoF,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAaA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACrF,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBqF,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAQA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAACtF,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBsF,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AAaA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACvF,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBuF,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAiCA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACxF,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBwF,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAsBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACzF,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiByF,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AA0BA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAC1F,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiB0F,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAoBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC3F,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB2F,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAoBA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAAC5F,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiB4F,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAmCA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAAC7F,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiB6F,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AA0BA,OAAM,IAAWC,OAAX;;AAAN,WAAiBA,OAAjB,EAAwB;EACTA,6BAAqB,UAAC9F,GAAD,EAAa;IAAU,oBACpDA,GADoD;EAEvD,CAFW;AAGd,CAJD,EAAiB8F,OAAO,KAAPA,OAAO,MAAxB;;AAyBA,OAAM,IAAWC,gBAAX;;AAAN,WAAiBA,gBAAjB,EAAiC;EAClBA,sCAAqB,UAAC/F,GAAD,EAAsB;IAAU,oBAC7DA,GAD6D;EAEhE,CAFW;AAGd,CAJD,EAAiB+F,gBAAgB,KAAhBA,gBAAgB,MAAjC;;AA2BA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAAChG,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiBgG,cAAc,KAAdA,cAAc,MAA/B;;AA2BA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAACjG,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBiG,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AA0BA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAClG,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBkG,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AA0EA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACnG,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBmG,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAqBA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAACpG,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiBoG,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAaA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACrG,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBqG,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAMA,WAAYC,qBAAZ;;AAAA,WAAYA,qBAAZ,EAAiC;EAC/BA;EACAA;EACAA;EACAA;EACAA;AACD,CAND,EAAYA,qBAAqB,KAArBA,qBAAqB,MAAjC;;AA+DA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAACvG,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiBuG,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAsDA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACxG,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBwG,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAaA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACzG,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiByG,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAsBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC1G,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB0G,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AA6BA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAC3G,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiB2G,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AA2BA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC5G,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB4G,mBAAmB,KAAnBA,mBAAmB,MAApC;;AA8BA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAAC7G,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiB6G,eAAe,KAAfA,eAAe,MAAhC;;AAgCA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC9G,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB8G,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAsBA,OAAM,IAAWC,QAAX;;AAAN,WAAiBA,QAAjB,EAAyB;EACVA,8BAAqB,UAAC/G,GAAD,EAAc;IAAU,oBACrDA,GADqD;EAExD,CAFW;AAGd,CAJD,EAAiB+G,QAAQ,KAARA,QAAQ,MAAzB;;AAgBA,OAAM,IAAWC,MAAX;;AAAN,WAAiBA,MAAjB,EAAuB;EACRA,4BAAqB,UAAChH,GAAD,EAAY;IAAU,oBACnDA,GADmD;EAEtD,CAFW;AAGd,CAJD,EAAiBgH,MAAM,KAANA,MAAM,MAAvB;;AAmCA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAACjH,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiBiH,KAAK,KAALA,KAAK,MAAtB;;AAgCA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAAClH,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBkH,oBAAoB,KAApBA,oBAAoB,MAArC;;AAiBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACnH,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBmH,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AA6BA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAACpH,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiBoH,eAAe,KAAfA,eAAe,MAAhC;;AAgCA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAACrH,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBqH,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AA4BA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACtH,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBsH,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAwBA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACvH,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBuH,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAmCA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAACxH,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiBwH,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAsCA,OAAM,IAAWC,0CAAX;;AAAN,WAAiBA,0CAAjB,EAA2D;EAC5CA,gEAAqB,UAACzH,GAAD,EAAgD;IAAU,oBACvFA,GADuF;EAE1F,CAFW;AAGd,CAJD,EAAiByH,0CAA0C,KAA1CA,0CAA0C,MAA3D;;AAmBA,OAAM,IAAWC,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAAC1H,GAAD,EAAsC;IAAU,oBAC7EA,GAD6E;EAEhF,CAFW;AAGd,CAJD,EAAiB0H,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAkCA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC3H,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB2H,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AA6CA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC5H,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB4H,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAwBA,OAAM,IAAWC,iCAAX;;AAAN,WAAiBA,iCAAjB,EAAkD;EACnCA,uDAAqB,UAAC7H,GAAD,EAAuC;IAAU,oBAC9EA,GAD8E;EAEjF,CAFW;AAGd,CAJD,EAAiB6H,iCAAiC,KAAjCA,iCAAiC,MAAlD;;AAoBA,OAAM,IAAWC,gBAAX;;AAAN,WAAiBA,gBAAjB,EAAiC;EAClBA,sCAAqB,UAAC9H,GAAD,EAAsB;IAAU,oBAC7DA,GAD6D;EAEhE,CAFW;AAGd,CAJD,EAAiB8H,gBAAgB,KAAhBA,gBAAgB,MAAjC;;AAyBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAAC/H,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiB+H,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAsBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAAChI,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBgI,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAMA,WAAYC,SAAZ;;AAAA,WAAYA,SAAZ,EAAqB;EACnBA;EACAA;AACD,CAHD,EAAYA,SAAS,KAATA,SAAS,MAArB;;AAsDA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAAClI,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiBkI,aAAa,KAAbA,aAAa,MAA9B;;AAkBA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAACnI,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiBmI,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAsCA,OAAM,IAAWC,IAAX;;AAAN,WAAiBA,IAAjB,EAAqB;EACNA,0BAAqB,UAACpI,GAAD,EAAU;IAAU,oBACjDA,GADiD;EAEpD,CAFW;AAGd,CAJD,EAAiBoI,IAAI,KAAJA,IAAI,MAArB;;AAMA,WAAYC,cAAZ;;AAAA,WAAYA,cAAZ,EAA0B;EACxBA;EACAA;AACD,CAHD,EAAYA,cAAc,KAAdA,cAAc,MAA1B;;AAoBA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAACtI,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiBsI,aAAa,KAAbA,aAAa,MAA9B;;AAuBA,OAAM,IAAWC,SAAX;;AAAN,WAAiBA,SAAjB,EAA0B;EACXA,+BAAqB,UAACvI,GAAD,EAAe;IAAU,oBACtDA,GADsD;EAEzD,CAFW;AAGd,CAJD,EAAiBuI,SAAS,KAATA,SAAS,MAA1B;;AAuBA,OAAM,IAAWC,UAAX;;AAAN,WAAiBA,UAAjB,EAA2B;EACZA,gCAAqB,UAACxI,GAAD,EAAgB;IAAU,oBACvDA,GADuD;EAE1D,CAFW;AAGd,CAJD,EAAiBwI,UAAU,KAAVA,UAAU,MAA3B;;AAMA,WAAYC,gBAAZ;;AAAA,WAAYA,gBAAZ,EAA4B;EAC1BA;EACAA;AACD,CAHD,EAAYA,gBAAgB,KAAhBA,gBAAgB,MAA5B;;AAaA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAC1I,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiB0I,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAkBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC3I,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB2I,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAiCA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAAC5I,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiB4I,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AAMA,WAAYC,cAAZ;;AAAA,WAAYA,cAAZ,EAA0B;EACxBA;EACAA;EACAA;AACD,CAJD,EAAYA,cAAc,KAAdA,cAAc,MAA1B;;AA0CA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAAC9I,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiB8I,aAAa,KAAbA,aAAa,MAA9B;;AAmCA,OAAM,IAAWC,+BAAX;;AAAN,WAAiBA,+BAAjB,EAAgD;EACjCA,qDAAqB,UAAC/I,GAAD,EAAqC;IAAU,oBAC5EA,GAD4E;EAE/E,CAFW;AAGd,CAJD,EAAiB+I,+BAA+B,KAA/BA,+BAA+B,MAAhD;;AAqCA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAChJ,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBgJ,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAwCA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACjJ,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBiJ,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AA0BA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAClJ,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBkJ,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAmCA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACnJ,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiBmJ,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAgCA,OAAM,IAAWC,oBAAX;;AAAN,WAAiBA,oBAAjB,EAAqC;EACtBA,0CAAqB,UAACpJ,GAAD,EAA0B;IAAU,oBACjEA,GADiE;EAEpE,CAFW;AAGd,CAJD,EAAiBoJ,oBAAoB,KAApBA,oBAAoB,MAArC;;AA0BA,OAAM,IAAWC,YAAX;;AAAN,WAAiBA,YAAjB,EAA6B;EACdA,kCAAqB,UAACrJ,GAAD,EAAkB;IAAU,oBACzDA,GADyD;EAE5D,CAFW;AAGd,CAJD,EAAiBqJ,YAAY,KAAZA,YAAY,MAA7B;;AA6BA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAACtJ,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiBsJ,WAAW,KAAXA,WAAW,MAA5B;;AAwCA,OAAM,IAAWC,qBAAX;;AAAN,WAAiBA,qBAAjB,EAAsC;EACvBA,2CAAqB,UAACvJ,GAAD,EAA2B;IAAU,oBAClEA,GADkE;EAErE,CAFW;AAGd,CAJD,EAAiBuJ,qBAAqB,KAArBA,qBAAqB,MAAtC;;AAMA,WAAYC,oBAAZ;;AAAA,WAAYA,oBAAZ,EAAgC;EAC9BA;EACAA;AACD,CAHD,EAAYA,oBAAoB,KAApBA,oBAAoB,MAAhC;;AAmCA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAACzJ,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiByJ,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAqBA,OAAM,IAAWC,cAAX;;AAAN,WAAiBA,cAAjB,EAA+B;EAChBA,oCAAqB,UAAC1J,GAAD,EAAoB;IAAU,oBAC3DA,GAD2D;EAE9D,CAFW;AAGd,CAJD,EAAiB0J,cAAc,KAAdA,cAAc,MAA/B;;AAyCA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC3J,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB2J,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAMA,WAAYC,oBAAZ;;AAAA,WAAYA,oBAAZ,EAAgC;EAC9BA;EACAA;AACD,CAHD,EAAYA,oBAAoB,KAApBA,oBAAoB,MAAhC;;AAkCA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC7J,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB6J,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAyBA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAAC9J,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiB8J,eAAe,KAAfA,eAAe,MAAhC;;AAmCA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC/J,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB+J,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAyBA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAChK,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBgK,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAsBA,OAAM,IAAWC,WAAX;;AAAN,WAAiBA,WAAjB,EAA4B;EACbA,iCAAqB,UAACjK,GAAD,EAAiB;IAAU,oBACxDA,GADwD;EAE3D,CAFW;AAGd,CAJD,EAAiBiK,WAAW,KAAXA,WAAW,MAA5B;;AAMA,WAAYC,gBAAZ;;AAAA,WAAYA,gBAAZ,EAA4B;EAC1BA;EACAA;EACAA;AACD,CAJD,EAAYA,gBAAgB,KAAhBA,gBAAgB,MAA5B;;AAqBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACnK,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBmK,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAMA,WAAYC,WAAZ;;AAAA,WAAYA,WAAZ,EAAuB;EACrBA;EACAA;AACD,CAHD,EAAYA,WAAW,KAAXA,WAAW,MAAvB;;AAiEA,OAAM,IAAWC,gBAAX;;AAAN,WAAiBA,gBAAjB,EAAiC;EAClBA,sCAAqB,UAACrK,GAAD,EAAsB;IAAU,oBAC7DA,GAD6D;EAEhE,CAFW;AAGd,CAJD,EAAiBqK,gBAAgB,KAAhBA,gBAAgB,MAAjC;;AAsBA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAACtK,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiBsK,eAAe,KAAfA,eAAe,MAAhC;;AAyDA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACvK,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBuK,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAyBA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACxK,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBwK,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAsBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACzK,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiByK,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAyCA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC1K,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB0K,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAqBA,OAAM,IAAWC,oCAAX;;AAAN,WAAiBA,oCAAjB,EAAqD;EACtCA,0DAAqB,UAAC3K,GAAD,EAA0C;IAAU,oBACjFA,GADiF;EAEpF,CAFW;AAGd,CAJD,EAAiB2K,oCAAoC,KAApCA,oCAAoC,MAArD;;AA0EA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAAC5K,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiB4K,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAMA,WAAYC,MAAZ;;AAAA,WAAYA,MAAZ,EAAkB;EAChBA;EACAA;EACAA;EACAA;EACAA;EACAA;EACAA;AACD,CARD,EAAYA,MAAM,KAANA,MAAM,MAAlB;;AAkDA,OAAM,IAAWC,aAAX;;AAAN,WAAiBA,aAAjB,EAA8B;EACfA,mCAAqB,UAAC9K,GAAD,EAAmB;IAAU,oBAC1DA,GAD0D;EAE7D,CAFW;AAGd,CAJD,EAAiB8K,aAAa,KAAbA,aAAa,MAA9B;;AA4DA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAC/K,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiB+K,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAwBA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAAChL,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBgL,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AAkBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACjL,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiBiL,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AA0BA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAAClL,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBkL,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AAyBA,OAAM,IAAWC,gBAAX;;AAAN,WAAiBA,gBAAjB,EAAiC;EAClBA,sCAAqB,UAACnL,GAAD,EAAsB;IAAU,oBAC7DA,GAD6D;EAEhE,CAFW;AAGd,CAJD,EAAiBmL,gBAAgB,KAAhBA,gBAAgB,MAAjC;;AAwBA,OAAM,IAAWC,iBAAX;;AAAN,WAAiBA,iBAAjB,EAAkC;EACnBA,uCAAqB,UAACpL,GAAD,EAAuB;IAAU,oBAC9DA,GAD8D;EAEjE,CAFW;AAGd,CAJD,EAAiBoL,iBAAiB,KAAjBA,iBAAiB,MAAlC;;AAmBA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACrL,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBqL,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAwBA,OAAM,IAAWC,eAAX;;AAAN,WAAiBA,eAAjB,EAAgC;EACjBA,qCAAqB,UAACtL,GAAD,EAAqB;IAAU,oBAC5DA,GAD4D;EAE/D,CAFW;AAGd,CAJD,EAAiBsL,eAAe,KAAfA,eAAe,MAAhC;;AAmBA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACvL,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBuL,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAsBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAACxL,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiBwL,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAiBA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACzL,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiByL,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAmCA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAAC1L,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiB0L,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAgCA,OAAM,IAAWC,kBAAX;;AAAN,WAAiBA,kBAAjB,EAAmC;EACpBA,wCAAqB,UAAC3L,GAAD,EAAwB;IAAU,oBAC/DA,GAD+D;EAElE,CAFW;AAGd,CAJD,EAAiB2L,kBAAkB,KAAlBA,kBAAkB,MAAnC;;AAwBA,OAAM,IAAWC,mBAAX;;AAAN,WAAiBA,mBAAjB,EAAoC;EACrBA,yCAAqB,UAAC5L,GAAD,EAAyB;IAAU,oBAChEA,GADgE;EAEnE,CAFW;AAGd,CAJD,EAAiB4L,mBAAmB,KAAnBA,mBAAmB,MAApC;;AAqDA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAAC7L,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiB6L,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AA+BA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC9L,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB8L,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAiBA,OAAM,IAAWC,KAAX;;AAAN,WAAiBA,KAAjB,EAAsB;EACPA,2BAAqB,UAAC/L,GAAD,EAAW;IAAU,oBAClDA,GADkD;EAErD,CAFW;AAGd,CAJD,EAAiB+L,KAAK,KAALA,KAAK,MAAtB;;AAiCA,OAAM,IAAWC,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAAChM,GAAD,EAAsC;IAAU,oBAC7EA,GAD6E;EAEhF,CAFW;AAGd,CAJD,EAAiBgM,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAcA,OAAM,IAAWC,iCAAX;;AAAN,WAAiBA,iCAAjB,EAAkD;EACnCA,uDAAqB,UAACjM,GAAD,EAAuC;IAAU,oBAC9EA,GAD8E;EAEjF,CAFW;AAGd,CAJD,EAAiBiM,iCAAiC,KAAjCA,iCAAiC,MAAlD;;AAqBA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAAClM,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiBkM,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AA0CA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAACnM,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBmM,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AAcA,OAAM,IAAWC,8BAAX;;AAAN,WAAiBA,8BAAjB,EAA+C;EAChCA,oDAAqB,UAACpM,GAAD,EAAoC;IAAU,oBAC3EA,GAD2E;EAE9E,CAFW;AAGd,CAJD,EAAiBoM,8BAA8B,KAA9BA,8BAA8B,MAA/C;;AA0CA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACrM,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBqM,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAcA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACtM,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBsM,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AA0CA,OAAM,IAAWC,sBAAX;;AAAN,WAAiBA,sBAAjB,EAAuC;EACxBA,4CAAqB,UAACvM,GAAD,EAA4B;IAAU,oBACnEA,GADmE;EAEtE,CAFW;AAGd,CAJD,EAAiBuM,sBAAsB,KAAtBA,sBAAsB,MAAvC;;AAaA,OAAM,IAAWC,uBAAX;;AAAN,WAAiBA,uBAAjB,EAAwC;EACzBA,6CAAqB,UAACxM,GAAD,EAA6B;IAAU,oBACpEA,GADoE;EAEvE,CAFW;AAGd,CAJD,EAAiBwM,uBAAuB,KAAvBA,uBAAuB,MAAxC;;AA2CA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACzM,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiByM,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAcA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC1M,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB0M,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAiCA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC3M,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB2M,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAcA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC5M,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB4M,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAsBA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC7M,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB6M,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAaA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC9M,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB8M,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAsBA,OAAM,IAAWC,wBAAX;;AAAN,WAAiBA,wBAAjB,EAAyC;EAC1BA,8CAAqB,UAAC/M,GAAD,EAA8B;IAAU,oBACrEA,GADqE;EAExE,CAFW;AAGd,CAJD,EAAiB+M,wBAAwB,KAAxBA,wBAAwB,MAAzC;;AAsBA,OAAM,IAAWC,gCAAX;;AAAN,WAAiBA,gCAAjB,EAAiD;EAClCA,sDAAqB,UAAChN,GAAD,EAAsC;IAAU,oBAC7EA,GAD6E;EAEhF,CAFW;AAGd,CAJD,EAAiBgN,gCAAgC,KAAhCA,gCAAgC,MAAjD;;AAuBA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACjN,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBiN,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AA2CA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAAClN,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBkN,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAcA,OAAM,IAAWC,6BAAX;;AAAN,WAAiBA,6BAAjB,EAA8C;EAC/BA,mDAAqB,UAACnN,GAAD,EAAmC;IAAU,oBAC1EA,GAD0E;EAE7E,CAFW;AAGd,CAJD,EAAiBmN,6BAA6B,KAA7BA,6BAA6B,MAA9C;;AAaA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAACpN,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiBoN,2BAA2B,KAA3BA,2BAA2B,MAA5C;;AAQA,OAAM,IAAWC,4BAAX;;AAAN,WAAiBA,4BAAjB,EAA6C;EAC9BA,kDAAqB,UAACrN,GAAD,EAAkC;IAAU,oBACzEA,GADyE;EAE5E,CAFW;AAGd,CAJD,EAAiBqN,4BAA4B,KAA5BA,4BAA4B,MAA7C;;AAwBA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACtN,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBsN,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAsCA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACvN,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiBuN,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAaA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAACxN,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiBwN,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAcA,OAAM,IAAWC,yBAAX;;AAAN,WAAiBA,yBAAjB,EAA0C;EAC3BA,+CAAqB,UAACzN,GAAD,EAA+B;IAAU,oBACtEA,GADsE;EAEzE,CAFW;AAGd,CAJD,EAAiByN,yBAAyB,KAAzBA,yBAAyB,MAA1C;;AAaA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC1N,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB0N,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAaA,OAAM,IAAWC,0BAAX;;AAAN,WAAiBA,0BAAjB,EAA2C;EAC5BA,gDAAqB,UAAC3N,GAAD,EAAgC;IAAU,oBACvEA,GADuE;EAE1E,CAFW;AAGd,CAJD,EAAiB2N,0BAA0B,KAA1BA,0BAA0B,MAA3C;;AAQA,OAAM,IAAWC,2BAAX;;AAAN,WAAiBA,2BAAjB,EAA4C;EAC7BA,iDAAqB,UAAC5N,GAAD,EAAiC;IAAU,oBACxEA,GADwE;EAE3E,CAFW;AAGd,CAJD,EAAiB4N,2BAA2B,KAA3BA,2BAA2B,MAA5C","names":["AccessDeniedException","obj","AgeRange","S3Object","GroundTruthManifest","Asset","Attribute","AudioMetadata","Beard","BodyPart","BoundingBox","CoversBodyPart","ProtectiveEquipmentType","EquipmentDetection","ProtectiveEquipmentBodyPart","LandmarkType","Landmark","Pose","ImageQuality","ComparedFace","Celebrity","Emotion","Eyeglasses","EyeOpen","GenderType","Gender","MouthOpen","Mustache","Smile","Sunglasses","FaceDetail","CelebrityDetail","CelebrityRecognition","CelebrityRecognitionSortBy","ComparedSourceImageFace","QualityFilter","Image","CompareFacesRequest","CompareFacesMatch","OrientationCorrection","CompareFacesResponse","ImageTooLargeException","InternalServerError","InvalidImageFormatException","InvalidParameterException","InvalidS3ObjectException","ProvisionedThroughputExceededException","ThrottlingException","ContentClassifier","ModerationLabel","ContentModerationDetection","ContentModerationSortBy","CreateCollectionRequest","CreateCollectionResponse","ResourceAlreadyExistsException","CreateProjectRequest","CreateProjectResponse","LimitExceededException","ResourceInUseException","OutputConfig","TestingData","TrainingData","CreateProjectVersionRequest","CreateProjectVersionResponse","ResourceNotFoundException","KinesisVideoStream","StreamProcessorInput","KinesisDataStream","StreamProcessorOutput","FaceSearchSettings","StreamProcessorSettings","CreateStreamProcessorRequest","CreateStreamProcessorResponse","Point","Geometry","CustomLabel","DeleteCollectionRequest","DeleteCollectionResponse","DeleteFacesRequest","DeleteFacesResponse","DeleteProjectRequest","ProjectStatus","DeleteProjectResponse","DeleteProjectVersionRequest","ProjectVersionStatus","DeleteProjectVersionResponse","DeleteStreamProcessorRequest","DeleteStreamProcessorResponse","DescribeCollectionRequest","DescribeCollectionResponse","DescribeProjectsRequest","ProjectDescription","DescribeProjectsResponse","InvalidPaginationTokenException","DescribeProjectVersionsRequest","Summary","EvaluationResult","ValidationData","TestingDataResult","TrainingDataResult","ProjectVersionDescription","DescribeProjectVersionsResponse","DescribeStreamProcessorRequest","StreamProcessorStatus","DescribeStreamProcessorResponse","DetectCustomLabelsRequest","DetectCustomLabelsResponse","ResourceNotReadyException","DetectFacesRequest","DetectFacesResponse","DetectionFilter","DetectLabelsRequest","Instance","Parent","Label","DetectLabelsResponse","HumanLoopDataAttributes","HumanLoopConfig","DetectModerationLabelsRequest","HumanLoopActivationOutput","DetectModerationLabelsResponse","HumanLoopQuotaExceededException","ProtectiveEquipmentSummarizationAttributes","DetectProtectiveEquipmentRequest","ProtectiveEquipmentPerson","ProtectiveEquipmentSummary","DetectProtectiveEquipmentResponse","RegionOfInterest","DetectTextFilters","DetectTextRequest","TextTypes","TextDetection","DetectTextResponse","Face","FaceAttributes","FaceDetection","FaceMatch","FaceRecord","FaceSearchSortBy","GetCelebrityInfoRequest","GetCelebrityInfoResponse","GetCelebrityRecognitionRequest","VideoJobStatus","VideoMetadata","GetCelebrityRecognitionResponse","GetContentModerationRequest","GetContentModerationResponse","GetFaceDetectionRequest","GetFaceDetectionResponse","GetFaceSearchRequest","PersonDetail","PersonMatch","GetFaceSearchResponse","LabelDetectionSortBy","GetLabelDetectionRequest","LabelDetection","GetLabelDetectionResponse","PersonTrackingSortBy","GetPersonTrackingRequest","PersonDetection","GetPersonTrackingResponse","GetSegmentDetectionRequest","ShotSegment","TechnicalCueType","TechnicalCueSegment","SegmentType","SegmentDetection","SegmentTypeInfo","GetSegmentDetectionResponse","GetTextDetectionRequest","TextDetectionResult","GetTextDetectionResponse","IdempotentParameterMismatchException","IndexFacesRequest","Reason","UnindexedFace","IndexFacesResponse","ServiceQuotaExceededException","ListCollectionsRequest","ListCollectionsResponse","ListFacesRequest","ListFacesResponse","ListStreamProcessorsRequest","StreamProcessor","ListStreamProcessorsResponse","NotificationChannel","RecognizeCelebritiesRequest","RecognizeCelebritiesResponse","SearchFacesRequest","SearchFacesResponse","SearchFacesByImageRequest","SearchFacesByImageResponse","Video","StartCelebrityRecognitionRequest","StartCelebrityRecognitionResponse","VideoTooLargeException","StartContentModerationRequest","StartContentModerationResponse","StartFaceDetectionRequest","StartFaceDetectionResponse","StartFaceSearchRequest","StartFaceSearchResponse","StartLabelDetectionRequest","StartLabelDetectionResponse","StartPersonTrackingRequest","StartPersonTrackingResponse","StartProjectVersionRequest","StartProjectVersionResponse","StartShotDetectionFilter","StartTechnicalCueDetectionFilter","StartSegmentDetectionFilters","StartSegmentDetectionRequest","StartSegmentDetectionResponse","StartStreamProcessorRequest","StartStreamProcessorResponse","StartTextDetectionFilters","StartTextDetectionRequest","StartTextDetectionResponse","StopProjectVersionRequest","StopProjectVersionResponse","StopStreamProcessorRequest","StopStreamProcessorResponse"],"sources":["C:\\Users\\jacob\\OneDrive\\College\\Freshman_2021-2022\\Summer_2022\\COM4570H\\newcrm\\new-crm\\node_modules\\@aws-sdk\\client-rekognition\\models\\models_0.ts"],"sourcesContent":["import { LazyJsonString as __LazyJsonString, SmithyException as __SmithyException } from \"@aws-sdk/smithy-client\";\nimport { MetadataBearer as $MetadataBearer } from \"@aws-sdk/types\";\n\n/**\n * <p>You are not authorized to perform the action.</p>\n */\nexport interface AccessDeniedException extends __SmithyException, $MetadataBearer {\n  name: \"AccessDeniedException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace AccessDeniedException {\n  export const filterSensitiveLog = (obj: AccessDeniedException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing the estimated age range, in years, for a face.</p>\n *          <p>Amazon Rekognition estimates an age range for faces detected in the input image. Estimated age\n *       ranges can overlap. A face of a 5-year-old might have an estimated range of 4-6, while the\n *       face of a 6-year-old might have an estimated range of 4-8.</p>\n */\nexport interface AgeRange {\n  /**\n   * <p>The lowest estimated age.</p>\n   */\n  Low?: number;\n\n  /**\n   * <p>The highest estimated age.</p>\n   */\n  High?: number;\n}\n\nexport namespace AgeRange {\n  export const filterSensitiveLog = (obj: AgeRange): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides the S3 bucket name and object name.</p>\n *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n *       Amazon Rekognition operations.</p>\n *\n *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n *       Developer Guide. </p>\n */\nexport interface S3Object {\n  /**\n   * <p>Name of the S3 bucket.</p>\n   */\n  Bucket?: string;\n\n  /**\n   * <p>S3 object key name.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>If the bucket is versioning enabled, you can specify the object version. </p>\n   */\n  Version?: string;\n}\n\nexport namespace S3Object {\n  export const filterSensitiveLog = (obj: S3Object): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file.\n * </p>\n */\nexport interface GroundTruthManifest {\n  /**\n   * <p>Provides the S3 bucket name and object name.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n   *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n   *       Developer Guide. </p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace GroundTruthManifest {\n  export const filterSensitiveLog = (obj: GroundTruthManifest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Assets are the images that you use to train and evaluate a model version.\n *          Assets can also contain validation information that you use to debug a failed model training.\n *          </p>\n */\nexport interface Asset {\n  /**\n   * <p>The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file.\n   * </p>\n   */\n  GroundTruthManifest?: GroundTruthManifest;\n}\n\nexport namespace Asset {\n  export const filterSensitiveLog = (obj: Asset): any => ({\n    ...obj,\n  });\n}\n\nexport enum Attribute {\n  ALL = \"ALL\",\n  DEFAULT = \"DEFAULT\",\n}\n\n/**\n * <p>Metadata information about an audio stream. An array of <code>AudioMetadata</code> objects\n *       for the audio streams found in a stored video is returned by <a>GetSegmentDetection</a>. </p>\n */\nexport interface AudioMetadata {\n  /**\n   * <p>The audio codec used to encode or decode the audio stream. </p>\n   */\n  Codec?: string;\n\n  /**\n   * <p>The duration of the audio stream in milliseconds.</p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>The sample rate for the audio stream.</p>\n   */\n  SampleRate?: number;\n\n  /**\n   * <p>The number of audio channels in the segment.</p>\n   */\n  NumberOfChannels?: number;\n}\n\nexport namespace AudioMetadata {\n  export const filterSensitiveLog = (obj: AudioMetadata): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face has a beard, and the confidence level in the\n *       determination.</p>\n */\nexport interface Beard {\n  /**\n   * <p>Boolean value that indicates whether the face has beard or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Beard {\n  export const filterSensitiveLog = (obj: Beard): any => ({\n    ...obj,\n  });\n}\n\nexport enum BodyPart {\n  FACE = \"FACE\",\n  HEAD = \"HEAD\",\n  LEFT_HAND = \"LEFT_HAND\",\n  RIGHT_HAND = \"RIGHT_HAND\",\n}\n\n/**\n * <p>Identifies the bounding box around the label, face, text or personal protective equipment.\n *       The <code>left</code> (x-coordinate) and <code>top</code> (y-coordinate) are coordinates representing the top and\n *       left sides of the bounding box. Note that the upper-left corner of the image is the origin\n *       (0,0). </p>\n *          <p>The <code>top</code> and <code>left</code> values returned are ratios of the overall\n *       image size. For example, if the input image is 700x200 pixels, and the top-left coordinate of\n *       the bounding box is 350x50 pixels, the API returns a <code>left</code> value of 0.5 (350/700)\n *       and a <code>top</code> value of 0.25 (50/200).</p>\n *          <p>The <code>width</code> and <code>height</code> values represent the dimensions of the\n *       bounding box as a ratio of the overall image dimension. For example, if the input image is\n *       700x200 pixels, and the bounding box width is 70 pixels, the width returned is 0.1. </p>\n *          <note>\n *             <p> The bounding box coordinates can have negative values. For example, if Amazon Rekognition is\n *         able to detect a face that is at the image edge and is only partially visible, the service\n *         can return coordinates that are outside the image bounds and, depending on the image edge,\n *         you might get negative values or values greater than 1 for the <code>left</code> or\n *           <code>top</code> values. </p>\n *          </note>\n */\nexport interface BoundingBox {\n  /**\n   * <p>Width of the bounding box as a ratio of the overall image width.</p>\n   */\n  Width?: number;\n\n  /**\n   * <p>Height of the bounding box as a ratio of the overall image height.</p>\n   */\n  Height?: number;\n\n  /**\n   * <p>Left coordinate of the bounding box as a ratio of overall image width.</p>\n   */\n  Left?: number;\n\n  /**\n   * <p>Top coordinate of the bounding box as a ratio of overall image height.</p>\n   */\n  Top?: number;\n}\n\nexport namespace BoundingBox {\n  export const filterSensitiveLog = (obj: BoundingBox): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about an item of Personal Protective Equipment covering a corresponding body part. For more\n *          information, see <a>DetectProtectiveEquipment</a>.</p>\n */\nexport interface CoversBodyPart {\n  /**\n   * <p>The confidence that Amazon Rekognition has in the value of <code>Value</code>.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>True if the PPE covers the corresponding body part, otherwise false.</p>\n   */\n  Value?: boolean;\n}\n\nexport namespace CoversBodyPart {\n  export const filterSensitiveLog = (obj: CoversBodyPart): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProtectiveEquipmentType {\n  FACE_COVER = \"FACE_COVER\",\n  HAND_COVER = \"HAND_COVER\",\n  HEAD_COVER = \"HEAD_COVER\",\n}\n\n/**\n * <p>Information about an item of Personal Protective Equipment (PPE) detected by\n *          <a>DetectProtectiveEquipment</a>. For more\n *          information, see <a>DetectProtectiveEquipment</a>.</p>\n */\nexport interface EquipmentDetection {\n  /**\n   * <p>A bounding box surrounding the item of detected PPE.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has that the bounding box (<code>BoundingBox</code>) contains an item of PPE.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The type of detected PPE.</p>\n   */\n  Type?: ProtectiveEquipmentType | string;\n\n  /**\n   * <p>Information about the body part covered by the detected PPE.</p>\n   */\n  CoversBodyPart?: CoversBodyPart;\n}\n\nexport namespace EquipmentDetection {\n  export const filterSensitiveLog = (obj: EquipmentDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a body part detected by <a>DetectProtectiveEquipment</a> that contains PPE.\n *          An array of <code>ProtectiveEquipmentBodyPart</code> objects is returned for each person detected by\n *          <code>DetectProtectiveEquipment</code>. </p>\n */\nexport interface ProtectiveEquipmentBodyPart {\n  /**\n   * <p>The detected body part.</p>\n   */\n  Name?: BodyPart | string;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the detection accuracy of the detected body part.\n   *       </p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>An array of Personal Protective Equipment items detected around a body part.</p>\n   */\n  EquipmentDetections?: EquipmentDetection[];\n}\n\nexport namespace ProtectiveEquipmentBodyPart {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentBodyPart): any => ({\n    ...obj,\n  });\n}\n\nexport enum LandmarkType {\n  chinBottom = \"chinBottom\",\n  eyeLeft = \"eyeLeft\",\n  eyeRight = \"eyeRight\",\n  leftEyeBrowLeft = \"leftEyeBrowLeft\",\n  leftEyeBrowRight = \"leftEyeBrowRight\",\n  leftEyeBrowUp = \"leftEyeBrowUp\",\n  leftEyeDown = \"leftEyeDown\",\n  leftEyeLeft = \"leftEyeLeft\",\n  leftEyeRight = \"leftEyeRight\",\n  leftEyeUp = \"leftEyeUp\",\n  leftPupil = \"leftPupil\",\n  midJawlineLeft = \"midJawlineLeft\",\n  midJawlineRight = \"midJawlineRight\",\n  mouthDown = \"mouthDown\",\n  mouthLeft = \"mouthLeft\",\n  mouthRight = \"mouthRight\",\n  mouthUp = \"mouthUp\",\n  nose = \"nose\",\n  noseLeft = \"noseLeft\",\n  noseRight = \"noseRight\",\n  rightEyeBrowLeft = \"rightEyeBrowLeft\",\n  rightEyeBrowRight = \"rightEyeBrowRight\",\n  rightEyeBrowUp = \"rightEyeBrowUp\",\n  rightEyeDown = \"rightEyeDown\",\n  rightEyeLeft = \"rightEyeLeft\",\n  rightEyeRight = \"rightEyeRight\",\n  rightEyeUp = \"rightEyeUp\",\n  rightPupil = \"rightPupil\",\n  upperJawlineLeft = \"upperJawlineLeft\",\n  upperJawlineRight = \"upperJawlineRight\",\n}\n\n/**\n * <p>Indicates the location of the landmark on the face.</p>\n */\nexport interface Landmark {\n  /**\n   * <p>Type of landmark.</p>\n   */\n  Type?: LandmarkType | string;\n\n  /**\n   * <p>The x-coordinate of the landmark expressed as a ratio of the width of the image.\n   *       The x-coordinate is measured from the left-side of the image.\n   *       For example, if the image is 700 pixels wide and the x-coordinate of the landmark is at 350 pixels, this value is 0.5. </p>\n   */\n  X?: number;\n\n  /**\n   * <p>The y-coordinate of the landmark expressed as a ratio of the height of the image.\n   *       The y-coordinate is measured from the top of the image.\n   *       For example, if the image height is 200 pixels and the y-coordinate of the landmark is at 50 pixels, this value is 0.25.</p>\n   */\n  Y?: number;\n}\n\nexport namespace Landmark {\n  export const filterSensitiveLog = (obj: Landmark): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw.</p>\n */\nexport interface Pose {\n  /**\n   * <p>Value representing the face rotation on the roll axis.</p>\n   */\n  Roll?: number;\n\n  /**\n   * <p>Value representing the face rotation on the yaw axis.</p>\n   */\n  Yaw?: number;\n\n  /**\n   * <p>Value representing the face rotation on the pitch axis.</p>\n   */\n  Pitch?: number;\n}\n\nexport namespace Pose {\n  export const filterSensitiveLog = (obj: Pose): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Identifies face image brightness and sharpness. </p>\n */\nexport interface ImageQuality {\n  /**\n   * <p>Value representing brightness of the face. The service returns a value between 0 and\n   *       100 (inclusive). A higher value indicates a brighter face image.</p>\n   */\n  Brightness?: number;\n\n  /**\n   * <p>Value representing sharpness of the face. The service returns a value between 0 and 100\n   *       (inclusive). A higher value indicates a sharper face image.</p>\n   */\n  Sharpness?: number;\n}\n\nexport namespace ImageQuality {\n  export const filterSensitiveLog = (obj: ImageQuality): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides face metadata for target image faces that are analyzed by\n *         <code>CompareFaces</code> and <code>RecognizeCelebrities</code>.</p>\n */\nexport interface ComparedFace {\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Level of confidence that what the bounding box contains is a face.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>An array of facial landmarks.</p>\n   */\n  Landmarks?: Landmark[];\n\n  /**\n   * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw.</p>\n   */\n  Pose?: Pose;\n\n  /**\n   * <p>Identifies face image brightness and sharpness. </p>\n   */\n  Quality?: ImageQuality;\n}\n\nexport namespace ComparedFace {\n  export const filterSensitiveLog = (obj: ComparedFace): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides information about a celebrity recognized by the <a>RecognizeCelebrities</a> operation.</p>\n */\nexport interface Celebrity {\n  /**\n   * <p>An array of URLs pointing to additional information about the celebrity. If there is no\n   *       additional information about the celebrity, this list is empty.</p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>A unique identifier for the celebrity. </p>\n   */\n  Id?: string;\n\n  /**\n   * <p>Provides information about the celebrity's face, such as its location on the\n   *       image.</p>\n   */\n  Face?: ComparedFace;\n\n  /**\n   * <p>The confidence, in percentage, that Amazon Rekognition has that the recognized face is the\n   *       celebrity.</p>\n   */\n  MatchConfidence?: number;\n}\n\nexport namespace Celebrity {\n  export const filterSensitiveLog = (obj: Celebrity): any => ({\n    ...obj,\n  });\n}\n\nexport type EmotionName =\n  | \"ANGRY\"\n  | \"CALM\"\n  | \"CONFUSED\"\n  | \"DISGUSTED\"\n  | \"FEAR\"\n  | \"HAPPY\"\n  | \"SAD\"\n  | \"SURPRISED\"\n  | \"UNKNOWN\";\n\n/**\n * <p>The emotions that appear to be expressed on the face, and the confidence level in the determination.\n *       The API is only making a determination of the physical appearance of a person's face. It is not a determination\n *       of the persons internal emotional state and should not be used in such a way. For example, a person pretending to have\n *       a sad face might not be sad emotionally.</p>\n */\nexport interface Emotion {\n  /**\n   * <p>Type of emotion detected.</p>\n   */\n  Type?: EmotionName | string;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Emotion {\n  export const filterSensitiveLog = (obj: Emotion): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is wearing eye glasses, and the confidence level in\n *       the determination.</p>\n */\nexport interface Eyeglasses {\n  /**\n   * <p>Boolean value that indicates whether the face is wearing eye glasses or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Eyeglasses {\n  export const filterSensitiveLog = (obj: Eyeglasses): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the eyes on the face are open, and the confidence level in the\n *       determination.</p>\n */\nexport interface EyeOpen {\n  /**\n   * <p>Boolean value that indicates whether the eyes on the face are open.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace EyeOpen {\n  export const filterSensitiveLog = (obj: EyeOpen): any => ({\n    ...obj,\n  });\n}\n\nexport enum GenderType {\n  Female = \"Female\",\n  Male = \"Male\",\n}\n\n/**\n * <p>The predicted gender of a detected face.\n *           </p>\n *\n *\n *          <p>Amazon Rekognition makes gender binary (male/female) predictions based on the physical appearance\n *       of a face in a particular image. This kind of prediction is not designed to categorize a persons gender\n *       identity, and you shouldn't use Amazon Rekognition to make such a determination. For example, a male actor\n *       wearing a long-haired wig and earrings for a role might be predicted as female.</p>\n *\n *          <p>Using Amazon Rekognition to make gender binary predictions is best suited for use cases where aggregate gender distribution statistics need to be\n *       analyzed without identifying specific users. For example, the percentage of female users compared to male users on a social media platform. </p>\n *\n *          <p>We don't recommend using gender binary predictions to make decisions that impact an individual's rights, privacy, or access to services.</p>\n */\nexport interface Gender {\n  /**\n   * <p>The predicted gender of the face.</p>\n   */\n  Value?: GenderType | string;\n\n  /**\n   * <p>Level of confidence in the prediction.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Gender {\n  export const filterSensitiveLog = (obj: Gender): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the mouth on the face is open, and the confidence level in the\n *       determination.</p>\n */\nexport interface MouthOpen {\n  /**\n   * <p>Boolean value that indicates whether the mouth on the face is open or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace MouthOpen {\n  export const filterSensitiveLog = (obj: MouthOpen): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face has a mustache, and the confidence level in the\n *       determination.</p>\n */\nexport interface Mustache {\n  /**\n   * <p>Boolean value that indicates whether the face has mustache or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Mustache {\n  export const filterSensitiveLog = (obj: Mustache): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is smiling, and the confidence level in the\n *       determination.</p>\n */\nexport interface Smile {\n  /**\n   * <p>Boolean value that indicates whether the face is smiling or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Smile {\n  export const filterSensitiveLog = (obj: Smile): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is wearing sunglasses, and the confidence level in\n *       the determination.</p>\n */\nexport interface Sunglasses {\n  /**\n   * <p>Boolean value that indicates whether the face is wearing sunglasses or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Sunglasses {\n  export const filterSensitiveLog = (obj: Sunglasses): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing attributes of the face that the algorithm detected.</p>\n *          <p>A <code>FaceDetail</code> object contains either the default facial attributes or all facial attributes.\n *       The default attributes are <code>BoundingBox</code>, <code>Confidence</code>, <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>.</p>\n *          <p>\n *             <a>GetFaceDetection</a> is the only Amazon Rekognition Video stored video operation that can return a <code>FaceDetail</code> object with all attributes.\n *       To specify which attributes to return, use the <code>FaceAttributes</code> input parameter for <a>StartFaceDetection</a>.\n *       The following Amazon Rekognition Video operations return only the default attributes. The corresponding Start operations\n *         don't have a <code>FaceAttributes</code> input parameter.</p>\n *          <ul>\n *             <li>\n *                <p>GetCelebrityRecognition</p>\n *             </li>\n *             <li>\n *                <p>GetPersonTracking</p>\n *             </li>\n *             <li>\n *                <p>GetFaceSearch</p>\n *             </li>\n *          </ul>\n *          <p>The Amazon Rekognition Image <a>DetectFaces</a> and <a>IndexFaces</a> operations\n *       can return all facial attributes. To specify which attributes to return, use the\n *       <code>Attributes</code> input parameter for <code>DetectFaces</code>. For <code>IndexFaces</code>, use the\n *       <code>DetectAttributes</code> input parameter.</p>\n */\nexport interface FaceDetail {\n  /**\n   * <p>Bounding box of the face. Default attribute.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The estimated age range, in years, for the face. Low represents the lowest estimated\n   *       age and High represents the highest estimated age.</p>\n   */\n  AgeRange?: AgeRange;\n\n  /**\n   * <p>Indicates whether or not the face is smiling, and the confidence level in the\n   *       determination.</p>\n   */\n  Smile?: Smile;\n\n  /**\n   * <p>Indicates whether or not the face is wearing eye glasses, and the confidence level in\n   *       the determination.</p>\n   */\n  Eyeglasses?: Eyeglasses;\n\n  /**\n   * <p>Indicates whether or not the face is wearing sunglasses, and the confidence level in\n   *       the determination.</p>\n   */\n  Sunglasses?: Sunglasses;\n\n  /**\n   * <p>The predicted gender of a detected face.\n   *     </p>\n   */\n  Gender?: Gender;\n\n  /**\n   * <p>Indicates whether or not the face has a beard, and the confidence level in the\n   *       determination.</p>\n   */\n  Beard?: Beard;\n\n  /**\n   * <p>Indicates whether or not the face has a mustache, and the confidence level in the\n   *       determination.</p>\n   */\n  Mustache?: Mustache;\n\n  /**\n   * <p>Indicates whether or not the eyes on the face are open, and the confidence level in the\n   *       determination.</p>\n   */\n  EyesOpen?: EyeOpen;\n\n  /**\n   * <p>Indicates whether or not the mouth on the face is open, and the confidence level in the\n   *       determination.</p>\n   */\n  MouthOpen?: MouthOpen;\n\n  /**\n   * <p>The emotions that appear to be expressed on the face, and the confidence level in the determination.\n   *       The API is only making a determination of the physical appearance of a person's face. It is not a determination\n   *       of the persons internal emotional state and should not be used in such a way. For example, a person pretending to have\n   *       a sad face might not be sad emotionally.</p>\n   */\n  Emotions?: Emotion[];\n\n  /**\n   * <p>Indicates the location of landmarks on the face. Default attribute.</p>\n   */\n  Landmarks?: Landmark[];\n\n  /**\n   * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw. Default attribute.</p>\n   */\n  Pose?: Pose;\n\n  /**\n   * <p>Identifies image brightness and sharpness. Default attribute.</p>\n   */\n  Quality?: ImageQuality;\n\n  /**\n   * <p>Confidence level that the bounding box contains a face (and not a different object such\n   *       as a tree). Default attribute.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace FaceDetail {\n  export const filterSensitiveLog = (obj: FaceDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a recognized celebrity.</p>\n */\nexport interface CelebrityDetail {\n  /**\n   * <p>An array of URLs pointing to additional celebrity information. </p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The unique identifier for the celebrity. </p>\n   */\n  Id?: string;\n\n  /**\n   * <p>The confidence, in percentage, that Amazon Rekognition has that the recognized face is the celebrity. </p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>Bounding box around the body of a celebrity.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Face details for the recognized celebrity.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace CelebrityDetail {\n  export const filterSensitiveLog = (obj: CelebrityDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a detected celebrity and the time the celebrity was detected in a stored video.\n *         For more information, see GetCelebrityRecognition in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface CelebrityRecognition {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the celebrity was recognized.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Information about a recognized celebrity.</p>\n   */\n  Celebrity?: CelebrityDetail;\n}\n\nexport namespace CelebrityRecognition {\n  export const filterSensitiveLog = (obj: CelebrityRecognition): any => ({\n    ...obj,\n  });\n}\n\nexport enum CelebrityRecognitionSortBy {\n  ID = \"ID\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\n/**\n * <p>Type that describes the face Amazon Rekognition chose to compare with the faces in the target.\n *       This contains a bounding box for the selected face and confidence level that the bounding box\n *       contains a face. Note that Amazon Rekognition selects the largest face in the source image for this\n *       comparison. </p>\n */\nexport interface ComparedSourceImageFace {\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Confidence level that the selected bounding box contains a face.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace ComparedSourceImageFace {\n  export const filterSensitiveLog = (obj: ComparedSourceImageFace): any => ({\n    ...obj,\n  });\n}\n\nexport enum QualityFilter {\n  AUTO = \"AUTO\",\n  HIGH = \"HIGH\",\n  LOW = \"LOW\",\n  MEDIUM = \"MEDIUM\",\n  NONE = \"NONE\",\n}\n\n/**\n * <p>Provides the input image either as bytes or an S3 object.</p>\n *          <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code>\n *       property. For example, you would use the <code>Bytes</code> property to pass an image loaded\n *       from a local file system. Image bytes passed by using the <code>Bytes</code> property must be\n *       base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to\n *       call Amazon Rekognition API operations. </p>\n *\n *          <p>For more information, see Analyzing an Image Loaded from a Local File System\n *       in the Amazon Rekognition Developer Guide.</p>\n *          <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the\n *         <code>S3Object</code> property. Images stored in an S3 bucket do not need to be\n *       base64-encoded.</p>\n *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n *       Amazon Rekognition operations.</p>\n *          <p>If you use the\n *       AWS\n *       CLI to call Amazon Rekognition operations, passing image bytes using the Bytes\n *       property is not supported. You must first upload the image to an Amazon S3 bucket and then\n *       call the operation using the S3Object property.</p>\n *\n *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3\n *       object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide.\n *     </p>\n */\nexport interface Image {\n  /**\n   * <p>Blob of image bytes up to 5 MBs.</p>\n   */\n  Bytes?: Uint8Array;\n\n  /**\n   * <p>Identifies an S3 object as the image source.</p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Image {\n  export const filterSensitiveLog = (obj: Image): any => ({\n    ...obj,\n  });\n}\n\nexport interface CompareFacesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  SourceImage: Image | undefined;\n\n  /**\n   * <p>The target image as base64-encoded bytes or an S3 object. If you use the AWS CLI to\n   *       call Amazon Rekognition operations, passing base64-encoded image bytes is not supported.\n   *     </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  TargetImage: Image | undefined;\n\n  /**\n   * <p>The minimum level of confidence in the face matches that a match must meet to be\n   *       included in the <code>FaceMatches</code> array.</p>\n   */\n  SimilarityThreshold?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *       Filtered faces aren't compared. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.\n   *       If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       dont meet the chosen quality bar.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed. The default value is <code>NONE</code>.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace CompareFacesRequest {\n  export const filterSensitiveLog = (obj: CompareFacesRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides information about a face in a target image that matches the source image face\n *       analyzed by <code>CompareFaces</code>. The <code>Face</code> property contains the bounding\n *       box of the face in the target image. The <code>Similarity</code> property is the confidence\n *       that the source image face matches the face in the bounding box.</p>\n */\nexport interface CompareFacesMatch {\n  /**\n   * <p>Level of confidence that the faces match.</p>\n   */\n  Similarity?: number;\n\n  /**\n   * <p>Provides face metadata (bounding box and confidence that the bounding box actually\n   *       contains a face).</p>\n   */\n  Face?: ComparedFace;\n}\n\nexport namespace CompareFacesMatch {\n  export const filterSensitiveLog = (obj: CompareFacesMatch): any => ({\n    ...obj,\n  });\n}\n\nexport enum OrientationCorrection {\n  ROTATE_0 = \"ROTATE_0\",\n  ROTATE_180 = \"ROTATE_180\",\n  ROTATE_270 = \"ROTATE_270\",\n  ROTATE_90 = \"ROTATE_90\",\n}\n\nexport interface CompareFacesResponse {\n  /**\n   * <p>The face in the source image that was used for comparison.</p>\n   */\n  SourceImageFace?: ComparedSourceImageFace;\n\n  /**\n   * <p>An array of faces in the target image that match the source image face. Each\n   *         <code>CompareFacesMatch</code> object provides the bounding box, the confidence level that\n   *       the bounding box contains a face, and the similarity score for the face in the bounding box\n   *       and the face in the source image.</p>\n   */\n  FaceMatches?: CompareFacesMatch[];\n\n  /**\n   * <p>An array of faces in the target image that did not match the source image\n   *       face.</p>\n   */\n  UnmatchedFaces?: ComparedFace[];\n\n  /**\n   * <p>The value of <code>SourceImageOrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesnt perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  SourceImageOrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>The value of <code>TargetImageOrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesnt perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  TargetImageOrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace CompareFacesResponse {\n  export const filterSensitiveLog = (obj: CompareFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The input image size exceeds the allowed limit. For more information, see\n *       Limits in Amazon Rekognition in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface ImageTooLargeException extends __SmithyException, $MetadataBearer {\n  name: \"ImageTooLargeException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ImageTooLargeException {\n  export const filterSensitiveLog = (obj: ImageTooLargeException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition experienced a service issue. Try your call again.</p>\n */\nexport interface InternalServerError extends __SmithyException, $MetadataBearer {\n  name: \"InternalServerError\";\n  $fault: \"server\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InternalServerError {\n  export const filterSensitiveLog = (obj: InternalServerError): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The provided image format is not supported. </p>\n */\nexport interface InvalidImageFormatException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidImageFormatException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidImageFormatException {\n  export const filterSensitiveLog = (obj: InvalidImageFormatException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input parameter violated a constraint. Validate your parameter before calling the API\n *       operation again.</p>\n */\nexport interface InvalidParameterException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidParameterException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidParameterException {\n  export const filterSensitiveLog = (obj: InvalidParameterException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition is unable to access the S3 object specified in the request.</p>\n */\nexport interface InvalidS3ObjectException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidS3ObjectException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidS3ObjectException {\n  export const filterSensitiveLog = (obj: InvalidS3ObjectException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The number of requests exceeded your throughput limit. If you want to increase this\n *       limit, contact Amazon Rekognition.</p>\n */\nexport interface ProvisionedThroughputExceededException extends __SmithyException, $MetadataBearer {\n  name: \"ProvisionedThroughputExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ProvisionedThroughputExceededException {\n  export const filterSensitiveLog = (obj: ProvisionedThroughputExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition is temporarily unable to process the request. Try your call again.</p>\n */\nexport interface ThrottlingException extends __SmithyException, $MetadataBearer {\n  name: \"ThrottlingException\";\n  $fault: \"server\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ThrottlingException {\n  export const filterSensitiveLog = (obj: ThrottlingException): any => ({\n    ...obj,\n  });\n}\n\nexport enum ContentClassifier {\n  FREE_OF_ADULT_CONTENT = \"FreeOfAdultContent\",\n  FREE_OF_PERSONALLY_IDENTIFIABLE_INFORMATION = \"FreeOfPersonallyIdentifiableInformation\",\n}\n\n/**\n * <p>Provides information about a single type of unsafe content found in an image or video. Each type of\n *       moderated content has a label within a hierarchical taxonomy. For more information, see\n *       Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface ModerationLabel {\n  /**\n   * <p>Specifies the confidence that Amazon Rekognition has that the label has been correctly\n   *       identified.</p>\n   *          <p>If you don't specify the <code>MinConfidence</code> parameter in the call to\n   *         <code>DetectModerationLabels</code>, the operation returns labels with a confidence value\n   *       greater than or equal to 50 percent.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The label name for the type of unsafe content detected in the image.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The name for the parent label. Labels at the top level of the hierarchy have the parent\n   *       label <code>\"\"</code>.</p>\n   */\n  ParentName?: string;\n}\n\nexport namespace ModerationLabel {\n  export const filterSensitiveLog = (obj: ModerationLabel): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about an unsafe content label detection in a stored video.</p>\n */\nexport interface ContentModerationDetection {\n  /**\n   * <p>Time, in milliseconds from the beginning of the video, that the unsafe content label was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>The unsafe content label detected by in the stored video.</p>\n   */\n  ModerationLabel?: ModerationLabel;\n}\n\nexport namespace ContentModerationDetection {\n  export const filterSensitiveLog = (obj: ContentModerationDetection): any => ({\n    ...obj,\n  });\n}\n\nexport enum ContentModerationSortBy {\n  NAME = \"NAME\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface CreateCollectionRequest {\n  /**\n   * <p>ID for the collection that you are creating.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace CreateCollectionRequest {\n  export const filterSensitiveLog = (obj: CreateCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateCollectionResponse {\n  /**\n   * <p>HTTP status code indicating the result of the operation.</p>\n   */\n  StatusCode?: number;\n\n  /**\n   * <p>Amazon Resource Name (ARN) of the collection. You can use this to manage permissions on\n   *       your resources. </p>\n   */\n  CollectionArn?: string;\n\n  /**\n   * <p>Version number of the face detection model associated with the collection you are creating.</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace CreateCollectionResponse {\n  export const filterSensitiveLog = (obj: CreateCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A collection with the specified ID already exists.</p>\n */\nexport interface ResourceAlreadyExistsException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceAlreadyExistsException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceAlreadyExistsException {\n  export const filterSensitiveLog = (obj: ResourceAlreadyExistsException): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectRequest {\n  /**\n   * <p>The name of the project to create.</p>\n   */\n  ProjectName: string | undefined;\n}\n\nexport namespace CreateProjectRequest {\n  export const filterSensitiveLog = (obj: CreateProjectRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectResponse {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the new project. You can use the ARN to\n   *       configure IAM access to the project. </p>\n   */\n  ProjectArn?: string;\n}\n\nexport namespace CreateProjectResponse {\n  export const filterSensitiveLog = (obj: CreateProjectResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An Amazon Rekognition service limit was exceeded. For example, if you start too many Amazon Rekognition Video jobs concurrently, calls to start operations\n *             (<code>StartLabelDetection</code>, for example) will raise a <code>LimitExceededException</code> exception (HTTP status code: 400) until\n *             the number of concurrently running jobs is below the Amazon Rekognition service limit.  </p>\n */\nexport interface LimitExceededException extends __SmithyException, $MetadataBearer {\n  name: \"LimitExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace LimitExceededException {\n  export const filterSensitiveLog = (obj: LimitExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The specified resource is already being used.</p>\n */\nexport interface ResourceInUseException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceInUseException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceInUseException {\n  export const filterSensitiveLog = (obj: ResourceInUseException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket and folder location where training output is placed.</p>\n */\nexport interface OutputConfig {\n  /**\n   * <p>The S3 bucket where training output is placed.</p>\n   */\n  S3Bucket?: string;\n\n  /**\n   * <p>The prefix applied to the training output files. </p>\n   */\n  S3KeyPrefix?: string;\n}\n\nexport namespace OutputConfig {\n  export const filterSensitiveLog = (obj: OutputConfig): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The dataset used for testing. Optionally, if <code>AutoCreate</code> is set,  Amazon Rekognition Custom Labels creates a\n *          testing dataset using an 80/20 split of the training dataset.</p>\n */\nexport interface TestingData {\n  /**\n   * <p>The assets used for testing.</p>\n   */\n  Assets?: Asset[];\n\n  /**\n   * <p>If specified, Amazon Rekognition Custom Labels creates a testing dataset with an 80/20 split of the training dataset.</p>\n   */\n  AutoCreate?: boolean;\n}\n\nexport namespace TestingData {\n  export const filterSensitiveLog = (obj: TestingData): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The dataset used for training.</p>\n */\nexport interface TrainingData {\n  /**\n   * <p>A Sagemaker GroundTruth manifest file that contains the training images (assets).</p>\n   */\n  Assets?: Asset[];\n}\n\nexport namespace TrainingData {\n  export const filterSensitiveLog = (obj: TrainingData): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectVersionRequest {\n  /**\n   * <p>The ARN of the Amazon Rekognition Custom Labels project that\n   *          manages the model that you want to train.</p>\n   */\n  ProjectArn: string | undefined;\n\n  /**\n   * <p>A name for the version of the model. This value must be unique.</p>\n   */\n  VersionName: string | undefined;\n\n  /**\n   * <p>The Amazon S3 location to store the results of training.</p>\n   */\n  OutputConfig: OutputConfig | undefined;\n\n  /**\n   * <p>The dataset to use for training. </p>\n   */\n  TrainingData: TrainingData | undefined;\n\n  /**\n   * <p>The dataset to use for testing.</p>\n   */\n  TestingData: TestingData | undefined;\n}\n\nexport namespace CreateProjectVersionRequest {\n  export const filterSensitiveLog = (obj: CreateProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectVersionResponse {\n  /**\n   * <p>The ARN of the model version that was created. Use <code>DescribeProjectVersion</code>\n   *          to get the current status of the training operation.</p>\n   */\n  ProjectVersionArn?: string;\n}\n\nexport namespace CreateProjectVersionResponse {\n  export const filterSensitiveLog = (obj: CreateProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The collection specified in the request cannot be found.</p>\n */\nexport interface ResourceNotFoundException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceNotFoundException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceNotFoundException {\n  export const filterSensitiveLog = (obj: ResourceNotFoundException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Kinesis video stream stream that provides the source streaming video for a Amazon Rekognition Video stream processor. For more information, see\n *             CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface KinesisVideoStream {\n  /**\n   * <p>ARN of the Kinesis video stream stream that streams the source video.</p>\n   */\n  Arn?: string;\n}\n\nexport namespace KinesisVideoStream {\n  export const filterSensitiveLog = (obj: KinesisVideoStream): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the source streaming video. </p>\n */\nexport interface StreamProcessorInput {\n  /**\n   * <p>The Kinesis video stream input stream for the source streaming video.</p>\n   */\n  KinesisVideoStream?: KinesisVideoStream;\n}\n\nexport namespace StreamProcessorInput {\n  export const filterSensitiveLog = (obj: StreamProcessorInput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The Kinesis data stream Amazon Rekognition to which the analysis results of a Amazon Rekognition stream processor are streamed. For more information, see\n *             CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface KinesisDataStream {\n  /**\n   * <p>ARN of the output Amazon Kinesis Data Streams stream.</p>\n   */\n  Arn?: string;\n}\n\nexport namespace KinesisDataStream {\n  export const filterSensitiveLog = (obj: KinesisDataStream): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the Amazon Kinesis Data Streams stream to which a Amazon Rekognition Video stream processor streams the results of a video analysis. For more\n *            information, see CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface StreamProcessorOutput {\n  /**\n   * <p>The Amazon Kinesis Data Streams stream to which the Amazon Rekognition stream processor streams the analysis results.</p>\n   */\n  KinesisDataStream?: KinesisDataStream;\n}\n\nexport namespace StreamProcessorOutput {\n  export const filterSensitiveLog = (obj: StreamProcessorOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input face recognition parameters for an Amazon Rekognition stream processor. <code>FaceRecognitionSettings</code> is a request\n *         parameter for <a>CreateStreamProcessor</a>.</p>\n */\nexport interface FaceSearchSettings {\n  /**\n   * <p>The ID of a collection that contains faces that you want to search for.</p>\n   */\n  CollectionId?: string;\n\n  /**\n   * <p>Minimum face match confidence score that must be met to return a result for a recognized face. Default is 80.\n   *         0 is the lowest confidence. 100 is the highest confidence.</p>\n   */\n  FaceMatchThreshold?: number;\n}\n\nexport namespace FaceSearchSettings {\n  export const filterSensitiveLog = (obj: FaceSearchSettings): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input parameters used to recognize faces in a streaming video analyzed by a Amazon Rekognition stream processor.</p>\n */\nexport interface StreamProcessorSettings {\n  /**\n   * <p>Face search settings to use on a streaming video. </p>\n   */\n  FaceSearch?: FaceSearchSettings;\n}\n\nexport namespace StreamProcessorSettings {\n  export const filterSensitiveLog = (obj: StreamProcessorSettings): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateStreamProcessorRequest {\n  /**\n   * <p>Kinesis video stream stream that provides the source streaming video. If you are using the AWS CLI, the parameter name is <code>StreamProcessorInput</code>.</p>\n   */\n  Input: StreamProcessorInput | undefined;\n\n  /**\n   * <p>Kinesis data stream stream to which Amazon Rekognition Video puts the analysis results. If you are using the AWS CLI, the parameter name is <code>StreamProcessorOutput</code>.</p>\n   */\n  Output: StreamProcessorOutput | undefined;\n\n  /**\n   * <p>An identifier you assign to the stream processor. You can use <code>Name</code> to\n   *             manage the stream processor. For example, you can get the current status of the stream processor by calling <a>DescribeStreamProcessor</a>.\n   *              <code>Name</code> is idempotent.\n   *        </p>\n   */\n  Name: string | undefined;\n\n  /**\n   * <p>Face recognition input parameters to be used by the stream processor. Includes the collection to use for face recognition and the face\n   *         attributes to detect.</p>\n   */\n  Settings: StreamProcessorSettings | undefined;\n\n  /**\n   * <p>ARN of the IAM role that allows access to the stream processor.</p>\n   */\n  RoleArn: string | undefined;\n}\n\nexport namespace CreateStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: CreateStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateStreamProcessorResponse {\n  /**\n   * <p>ARN for the newly create stream processor.</p>\n   */\n  StreamProcessorArn?: string;\n}\n\nexport namespace CreateStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: CreateStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The X and Y coordinates of a point on an image. The X and Y values returned are ratios\n *       of the overall image size. For example, if the input image is 700x200 and the\n *       operation returns X=0.5 and Y=0.25, then the point is at the (350,50) pixel coordinate on the image.</p>\n *\n *          <p>An array of <code>Point</code> objects,\n *       <code>Polygon</code>, is returned by <a>DetectText</a> and by <a>DetectCustomLabels</a>. <code>Polygon</code>\n *       represents a fine-grained polygon around a detected item. For more information, see Geometry in the\n *       Amazon Rekognition Developer Guide. </p>\n */\nexport interface Point {\n  /**\n   * <p>The value of the X coordinate for a point on a <code>Polygon</code>.</p>\n   */\n  X?: number;\n\n  /**\n   * <p>The value of the Y coordinate for a point on a <code>Polygon</code>.</p>\n   */\n  Y?: number;\n}\n\nexport namespace Point {\n  export const filterSensitiveLog = (obj: Point): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about where an object (<a>DetectCustomLabels</a>) or text (<a>DetectText</a>) is located on\n *       an image.</p>\n */\nexport interface Geometry {\n  /**\n   * <p>An axis-aligned coarse representation of the detected item's location on the\n   *       image.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Within the bounding box, a fine-grained polygon around the detected item.</p>\n   */\n  Polygon?: Point[];\n}\n\nexport namespace Geometry {\n  export const filterSensitiveLog = (obj: Geometry): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A custom label detected in an image by a call to <a>DetectCustomLabels</a>.</p>\n */\nexport interface CustomLabel {\n  /**\n   * <p>The name of the custom label.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The confidence that the model has in the detection of the custom label. The\n   *       range is 0-100. A higher value indicates a higher confidence.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The location of the detected object on the image that corresponds to the custom label.\n   *          Includes an axis aligned coarse bounding box surrounding the object and a finer grain polygon\n   *          for more accurate spatial information.</p>\n   */\n  Geometry?: Geometry;\n}\n\nexport namespace CustomLabel {\n  export const filterSensitiveLog = (obj: CustomLabel): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteCollectionRequest {\n  /**\n   * <p>ID of the collection to delete.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace DeleteCollectionRequest {\n  export const filterSensitiveLog = (obj: DeleteCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteCollectionResponse {\n  /**\n   * <p>HTTP status code that indicates the result of the operation.</p>\n   */\n  StatusCode?: number;\n}\n\nexport namespace DeleteCollectionResponse {\n  export const filterSensitiveLog = (obj: DeleteCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteFacesRequest {\n  /**\n   * <p>Collection from which to remove the specific faces.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>An array of face IDs to delete.</p>\n   */\n  FaceIds: string[] | undefined;\n}\n\nexport namespace DeleteFacesRequest {\n  export const filterSensitiveLog = (obj: DeleteFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteFacesResponse {\n  /**\n   * <p>An array of strings (face IDs) of the faces that were deleted.</p>\n   */\n  DeletedFaces?: string[];\n}\n\nexport namespace DeleteFacesResponse {\n  export const filterSensitiveLog = (obj: DeleteFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteProjectRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project that you want to delete.</p>\n   */\n  ProjectArn: string | undefined;\n}\n\nexport namespace DeleteProjectRequest {\n  export const filterSensitiveLog = (obj: DeleteProjectRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProjectStatus {\n  CREATED = \"CREATED\",\n  CREATING = \"CREATING\",\n  DELETING = \"DELETING\",\n}\n\nexport interface DeleteProjectResponse {\n  /**\n   * <p>The current status of the delete project operation.</p>\n   */\n  Status?: ProjectStatus | string;\n}\n\nexport namespace DeleteProjectResponse {\n  export const filterSensitiveLog = (obj: DeleteProjectResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>\n   */\n  ProjectVersionArn: string | undefined;\n}\n\nexport namespace DeleteProjectVersionRequest {\n  export const filterSensitiveLog = (obj: DeleteProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProjectVersionStatus {\n  DELETING = \"DELETING\",\n  FAILED = \"FAILED\",\n  RUNNING = \"RUNNING\",\n  STARTING = \"STARTING\",\n  STOPPED = \"STOPPED\",\n  STOPPING = \"STOPPING\",\n  TRAINING_COMPLETED = \"TRAINING_COMPLETED\",\n  TRAINING_FAILED = \"TRAINING_FAILED\",\n  TRAINING_IN_PROGRESS = \"TRAINING_IN_PROGRESS\",\n}\n\nexport interface DeleteProjectVersionResponse {\n  /**\n   * <p>The status of the deletion operation.</p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace DeleteProjectVersionResponse {\n  export const filterSensitiveLog = (obj: DeleteProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteStreamProcessorRequest {\n  /**\n   * <p>The name of the stream processor you want to delete.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace DeleteStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: DeleteStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteStreamProcessorResponse {}\n\nexport namespace DeleteStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: DeleteStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeCollectionRequest {\n  /**\n   * <p>The ID of the collection to describe.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace DescribeCollectionRequest {\n  export const filterSensitiveLog = (obj: DescribeCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeCollectionResponse {\n  /**\n   * <p>The number of faces that are indexed into the collection. To index faces into a\n   *          collection, use <a>IndexFaces</a>.</p>\n   */\n  FaceCount?: number;\n\n  /**\n   * <p>The version of the face model that's used by the collection for face detection.</p>\n   *\n   *          <p>For more information, see Model Versioning in the\n   *      Amazon Rekognition Developer Guide.</p>\n   */\n  FaceModelVersion?: string;\n\n  /**\n   * <p>The Amazon Resource Name (ARN) of the collection.</p>\n   */\n  CollectionARN?: string;\n\n  /**\n   * <p>The number of milliseconds since the Unix epoch time until the creation of the collection.\n   *          The Unix epoch time is 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970.</p>\n   */\n  CreationTimestamp?: Date;\n}\n\nexport namespace DescribeCollectionResponse {\n  export const filterSensitiveLog = (obj: DescribeCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectsRequest {\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. You can use this pagination\n   *          token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>The maximum number of results to return per paginated call. The largest value you can specify is 100.\n   *          If you specify a value greater than 100, a ValidationException\n   *          error occurs. The default value is 100. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace DescribeProjectsRequest {\n  export const filterSensitiveLog = (obj: DescribeProjectsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A description of a Amazon Rekognition Custom Labels project.</p>\n */\nexport interface ProjectDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project.</p>\n   */\n  ProjectArn?: string;\n\n  /**\n   * <p>The Unix timestamp for the date and time that the project was created.</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The current status of the project.</p>\n   */\n  Status?: ProjectStatus | string;\n}\n\nexport namespace ProjectDescription {\n  export const filterSensitiveLog = (obj: ProjectDescription): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectsResponse {\n  /**\n   * <p>A list of project descriptions. The list is sorted by the date and time the projects are created.</p>\n   */\n  ProjectDescriptions?: ProjectDescription[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *          You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n}\n\nexport namespace DescribeProjectsResponse {\n  export const filterSensitiveLog = (obj: DescribeProjectsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Pagination token in the request is not valid.</p>\n */\nexport interface InvalidPaginationTokenException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidPaginationTokenException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidPaginationTokenException {\n  export const filterSensitiveLog = (obj: InvalidPaginationTokenException): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectVersionsRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project that contains the models you want to describe.</p>\n   */\n  ProjectArn: string | undefined;\n\n  /**\n   * <p>A list of model version names that you want to describe. You can add up to 10 model version names\n   *          to the list. If you don't specify a value, all model descriptions are returned.  A version name is part of a\n   *          model (ProjectVersion) ARN. For example, <code>my-model.2020-01-21T09.10.15</code> is the version name in the following ARN.\n   *                <code>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/<i>my-model.2020-01-21T09.10.15</i>/1234567890123</code>.</p>\n   */\n  VersionNames?: string[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *            results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *            You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>The maximum number of results to return per paginated call.\n   *           The largest value you can specify is 100. If you specify a value greater than 100, a ValidationException\n   *           error occurs. The default value is 100. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace DescribeProjectVersionsRequest {\n  export const filterSensitiveLog = (obj: DescribeProjectVersionsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket that contains the training summary. The training summary includes\n *          aggregated evaluation metrics for the entire testing dataset and metrics for each\n *          individual label.  </p>\n *          <p>You get the training summary S3 bucket location by calling <a>DescribeProjectVersions</a>.\n *          </p>\n */\nexport interface Summary {\n  /**\n   * <p>Provides the S3 bucket name and object name.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n   *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n   *       Developer Guide. </p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Summary {\n  export const filterSensitiveLog = (obj: Summary): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The evaluation results for the training of a model.</p>\n */\nexport interface EvaluationResult {\n  /**\n   * <p>The F1 score for the evaluation of all labels. The F1 score metric evaluates the overall precision\n   *          and recall performance of the model as a single value. A higher value indicates better precision\n   *          and recall performance. A lower score indicates that precision, recall, or both are performing poorly.\n   *\n   *   </p>\n   */\n  F1Score?: number;\n\n  /**\n   * <p>The S3 bucket that contains the training summary.</p>\n   */\n  Summary?: Summary;\n}\n\nexport namespace EvaluationResult {\n  export const filterSensitiveLog = (obj: EvaluationResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Contains the Amazon S3 bucket location of the validation data for a model training job. </p>\n *\n *          <p>The validation data includes error information for individual\n *          JSON lines in the dataset.\n *             For more information, see Debugging a Failed Model Training in the\n *             Amazon Rekognition Custom Labels Developer Guide. </p>\n *          <p>You get the <code>ValidationData</code> object for the training dataset (<a>TrainingDataResult</a>)\n *          and the test dataset (<a>TestingDataResult</a>) by calling <a>DescribeProjectVersions</a>. </p>\n *          <p>The assets array contains a single <a>Asset</a> object.\n *          The <a>GroundTruthManifest</a> field of the Asset object contains the S3 bucket location of\n *          the validation data.\n * </p>\n */\nexport interface ValidationData {\n  /**\n   * <p>The assets that comprise the validation data. </p>\n   */\n  Assets?: Asset[];\n}\n\nexport namespace ValidationData {\n  export const filterSensitiveLog = (obj: ValidationData): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.</p>\n */\nexport interface TestingDataResult {\n  /**\n   * <p>The testing dataset that was supplied for training.</p>\n   */\n  Input?: TestingData;\n\n  /**\n   * <p>The subset of the dataset that was actually tested. Some images (assets) might not be tested due to\n   *          file formatting and other issues. </p>\n   */\n  Output?: TestingData;\n\n  /**\n   * <p>The location of the data validation manifest. The data validation manifest is created for the test dataset during model training.</p>\n   */\n  Validation?: ValidationData;\n}\n\nexport namespace TestingDataResult {\n  export const filterSensitiveLog = (obj: TestingDataResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.</p>\n */\nexport interface TrainingDataResult {\n  /**\n   * <p>The training assets that you supplied for training.</p>\n   */\n  Input?: TrainingData;\n\n  /**\n   * <p>The images (assets) that were actually trained by Amazon Rekognition Custom Labels. </p>\n   */\n  Output?: TrainingData;\n\n  /**\n   * <p>The location of the data validation manifest. The data validation manifest is created for the training dataset during model training.</p>\n   */\n  Validation?: ValidationData;\n}\n\nexport namespace TrainingDataResult {\n  export const filterSensitiveLog = (obj: TrainingDataResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The description of a version of a model.</p>\n */\nexport interface ProjectVersionDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version. </p>\n   */\n  ProjectVersionArn?: string;\n\n  /**\n   * <p>The Unix datetime for the date and time that training started.</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The minimum number of inference units used by the model. For more information,\n   *       see <a>StartProjectVersion</a>.</p>\n   */\n  MinInferenceUnits?: number;\n\n  /**\n   * <p>The current status of the model version.</p>\n   */\n  Status?: ProjectVersionStatus | string;\n\n  /**\n   * <p>A descriptive message for an error or warning that occurred.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>The duration, in seconds, that the model version has been billed for training.\n   *       This value is only returned if the model version has been successfully trained.</p>\n   */\n  BillableTrainingTimeInSeconds?: number;\n\n  /**\n   * <p>The Unix date and time that training of the model ended.</p>\n   */\n  TrainingEndTimestamp?: Date;\n\n  /**\n   * <p>The location where training results are saved.</p>\n   */\n  OutputConfig?: OutputConfig;\n\n  /**\n   * <p>Contains information about the training results.</p>\n   */\n  TrainingDataResult?: TrainingDataResult;\n\n  /**\n   * <p>Contains information about the testing results.</p>\n   */\n  TestingDataResult?: TestingDataResult;\n\n  /**\n   * <p>The training results. <code>EvaluationResult</code> is only returned if training is successful.</p>\n   */\n  EvaluationResult?: EvaluationResult;\n\n  /**\n   * <p>The location of the summary manifest. The summary manifest provides aggregate data validation results for the training\n   *          and test datasets.</p>\n   */\n  ManifestSummary?: GroundTruthManifest;\n}\n\nexport namespace ProjectVersionDescription {\n  export const filterSensitiveLog = (obj: ProjectVersionDescription): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectVersionsResponse {\n  /**\n   * <p>A list of model descriptions. The list is sorted by the creation date and time of\n   *          the model versions, latest to earliest.</p>\n   */\n  ProjectVersionDescriptions?: ProjectVersionDescription[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *          You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n}\n\nexport namespace DescribeProjectVersionsResponse {\n  export const filterSensitiveLog = (obj: DescribeProjectVersionsResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeStreamProcessorRequest {\n  /**\n   * <p>Name of the stream processor for which you want information.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace DescribeStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: DescribeStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum StreamProcessorStatus {\n  FAILED = \"FAILED\",\n  RUNNING = \"RUNNING\",\n  STARTING = \"STARTING\",\n  STOPPED = \"STOPPED\",\n  STOPPING = \"STOPPING\",\n}\n\nexport interface DescribeStreamProcessorResponse {\n  /**\n   * <p>Name of the stream processor. </p>\n   */\n  Name?: string;\n\n  /**\n   * <p>ARN of the stream processor.</p>\n   */\n  StreamProcessorArn?: string;\n\n  /**\n   * <p>Current status of the stream processor.</p>\n   */\n  Status?: StreamProcessorStatus | string;\n\n  /**\n   * <p>Detailed status message about the stream processor.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Date and time the stream processor was created</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The time, in Unix format, the stream processor was last updated. For example, when the stream\n   *         processor moves from a running state to a failed state, or when the user starts or stops the stream processor.</p>\n   */\n  LastUpdateTimestamp?: Date;\n\n  /**\n   * <p>Kinesis video stream that provides the source streaming video.</p>\n   */\n  Input?: StreamProcessorInput;\n\n  /**\n   * <p>Kinesis data stream to which Amazon Rekognition Video puts the analysis results.</p>\n   */\n  Output?: StreamProcessorOutput;\n\n  /**\n   * <p>ARN of the IAM role that allows access to the stream processor.</p>\n   */\n  RoleArn?: string;\n\n  /**\n   * <p>Face recognition input parameters that are being used by the stream processor.\n   *             Includes the collection to use for face recognition and the face\n   *             attributes to detect.</p>\n   */\n  Settings?: StreamProcessorSettings;\n}\n\nexport namespace DescribeStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: DescribeStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectCustomLabelsRequest {\n  /**\n   * <p>The ARN of the model version that you want to use.</p>\n   */\n  ProjectVersionArn: string | undefined;\n\n  /**\n   * <p>Provides the input image either as bytes or an S3 object.</p>\n   *          <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code>\n   *       property. For example, you would use the <code>Bytes</code> property to pass an image loaded\n   *       from a local file system. Image bytes passed by using the <code>Bytes</code> property must be\n   *       base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to\n   *       call Amazon Rekognition API operations. </p>\n   *\n   *          <p>For more information, see Analyzing an Image Loaded from a Local File System\n   *       in the Amazon Rekognition Developer Guide.</p>\n   *          <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the\n   *         <code>S3Object</code> property. Images stored in an S3 bucket do not need to be\n   *       base64-encoded.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *          <p>If you use the\n   *       AWS\n   *       CLI to call Amazon Rekognition operations, passing image bytes using the Bytes\n   *       property is not supported. You must first upload the image to an Amazon S3 bucket and then\n   *       call the operation using the S3Object property.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3\n   *       object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide.\n   *     </p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of results you want the service to return in the response.\n   *          The service returns the specified number of highest confidence labels ranked from highest confidence\n   *       to lowest.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return.\n   *          Amazon Rekognition doesn't return any labels with a confidence lower than this specified value. If you specify a\n   *       value of 0, all labels are return, regardless of the default thresholds that the model version applies.</p>\n   */\n  MinConfidence?: number;\n}\n\nexport namespace DetectCustomLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectCustomLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectCustomLabelsResponse {\n  /**\n   * <p>An array of custom labels detected in the input image.</p>\n   */\n  CustomLabels?: CustomLabel[];\n}\n\nexport namespace DetectCustomLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectCustomLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The requested resource isn't ready. For example,\n *          this exception occurs when you call <code>DetectCustomLabels</code> with a\n *          model version that isn't deployed. </p>\n */\nexport interface ResourceNotReadyException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceNotReadyException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceNotReadyException {\n  export const filterSensitiveLog = (obj: ResourceNotReadyException): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectFacesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>An array of facial attributes you want to be returned. This can be the default list of\n   *       attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if\n   *       you specify <code>[\"DEFAULT\"]</code>, the API returns the following subset of facial\n   *       attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,\n   *         <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>[\"ALL\"]</code>, all\n   *       facial attributes are returned, but the operation takes longer to complete.</p>\n   *          <p>If you provide both, <code>[\"ALL\", \"DEFAULT\"]</code>, the service uses a logical AND\n   *       operator to determine which attributes to return (in this case, all attributes). </p>\n   */\n  Attributes?: (Attribute | string)[];\n}\n\nexport namespace DetectFacesRequest {\n  export const filterSensitiveLog = (obj: DetectFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectFacesResponse {\n  /**\n   * <p>Details of each face found in the image. </p>\n   */\n  FaceDetails?: FaceDetail[];\n\n  /**\n   * <p>The value of <code>OrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesnt perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace DetectFacesResponse {\n  export const filterSensitiveLog = (obj: DetectFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A set of parameters that allow you to filter out certain results from your returned results.</p>\n */\nexport interface DetectionFilter {\n  /**\n   * <p>Sets confidence of word detection. Words with detection confidence below this will be excluded\n   *       from the result. Values should be between 0.5 and 1 as Text in Video will not return any result below\n   *       0.5.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Sets the minimum height of the word bounding box. Words with bounding box heights lesser than\n   *       this value will be excluded from the result. Value is relative to the video frame height.</p>\n   */\n  MinBoundingBoxHeight?: number;\n\n  /**\n   * <p>Sets the minimum width of the word bounding box. Words with bounding boxes widths lesser than\n   *       this value will be excluded from the result. Value is relative to the video frame width.</p>\n   */\n  MinBoundingBoxWidth?: number;\n}\n\nexport namespace DetectionFilter {\n  export const filterSensitiveLog = (obj: DetectionFilter): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectLabelsRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing image bytes is not supported. Images stored in an S3 Bucket do\n   *     not need to be base64-encoded.</p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of labels you want the service to return in the response. The service\n   *       returns the specified number of highest confidence labels. </p>\n   */\n  MaxLabels?: number;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't\n   *       return any labels with confidence lower than this specified value.</p>\n   *          <p>If <code>MinConfidence</code> is not specified, the operation returns labels with a\n   *       confidence values greater than or equal to 55 percent.</p>\n   */\n  MinConfidence?: number;\n}\n\nexport namespace DetectLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An instance of a label returned by Amazon Rekognition Image (<a>DetectLabels</a>)\n *       or by Amazon Rekognition Video (<a>GetLabelDetection</a>).</p>\n */\nexport interface Instance {\n  /**\n   * <p>The position of the label instance on the image.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the accuracy of the bounding box.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Instance {\n  export const filterSensitiveLog = (obj: Instance): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A parent label for a label. A label can have 0, 1, or more parents. </p>\n */\nexport interface Parent {\n  /**\n   * <p>The name of the parent label.</p>\n   */\n  Name?: string;\n}\n\nexport namespace Parent {\n  export const filterSensitiveLog = (obj: Parent): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing details about the detected label, including the name, detected instances, parent labels, and level of\n *       confidence.</p>\n *          <p>\n *     </p>\n */\nexport interface Label {\n  /**\n   * <p>The name (label) of the object or scene.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>Level of confidence.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>If <code>Label</code> represents an object, <code>Instances</code> contains the bounding boxes for each instance of the detected object.\n   *       Bounding boxes are returned for common object labels such as people, cars, furniture, apparel or pets.</p>\n   */\n  Instances?: Instance[];\n\n  /**\n   * <p>The parent labels for a label. The response includes all ancestor labels.</p>\n   */\n  Parents?: Parent[];\n}\n\nexport namespace Label {\n  export const filterSensitiveLog = (obj: Label): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectLabelsResponse {\n  /**\n   * <p>An array of labels for the real-world objects detected. </p>\n   */\n  Labels?: Label[];\n\n  /**\n   * <p>The value of <code>OrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesnt perform image correction for images in .png format and\n   *          .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *          coordinates aren't translated and represent the object locations before the image is rotated.\n   *       </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>Version number of the label detection model that was used to detect labels.</p>\n   */\n  LabelModelVersion?: string;\n}\n\nexport namespace DetectLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Allows you to set attributes of the image. Currently, you can declare an image as free of\n *       personally identifiable information.</p>\n */\nexport interface HumanLoopDataAttributes {\n  /**\n   * <p>Sets whether the input image is free of personally identifiable information.</p>\n   */\n  ContentClassifiers?: (ContentClassifier | string)[];\n}\n\nexport namespace HumanLoopDataAttributes {\n  export const filterSensitiveLog = (obj: HumanLoopDataAttributes): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sets up the flow definition the image will be sent to if one of the conditions is met.\n *       You can also set certain attributes of the image before review.</p>\n */\nexport interface HumanLoopConfig {\n  /**\n   * <p>The name of the human review used for this image. This should be kept unique within a region.</p>\n   */\n  HumanLoopName: string | undefined;\n\n  /**\n   * <p>The Amazon Resource Name (ARN) of the flow definition. You can create a flow definition by using the Amazon Sagemaker\n   *       <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateFlowDefinition.html\">CreateFlowDefinition</a>\n   *      Operation. </p>\n   */\n  FlowDefinitionArn: string | undefined;\n\n  /**\n   * <p>Sets attributes of the input data.</p>\n   */\n  DataAttributes?: HumanLoopDataAttributes;\n}\n\nexport namespace HumanLoopConfig {\n  export const filterSensitiveLog = (obj: HumanLoopConfig): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectModerationLabelsRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't\n   *       return any labels with a confidence level lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with\n   *       confidence values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Sets up the configuration for human evaluation, including the FlowDefinition\n   *       the image will be sent to.</p>\n   */\n  HumanLoopConfig?: HumanLoopConfig;\n}\n\nexport namespace DetectModerationLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectModerationLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Shows the results of the human in the loop evaluation. If there is no HumanLoopArn, the input did\n *        not trigger human review.</p>\n */\nexport interface HumanLoopActivationOutput {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the HumanLoop created.</p>\n   */\n  HumanLoopArn?: string;\n\n  /**\n   * <p>Shows if and why human review was needed.</p>\n   */\n  HumanLoopActivationReasons?: string[];\n\n  /**\n   * <p>Shows the result of condition evaluations, including those conditions which activated a\n   *       human review.</p>\n   */\n  HumanLoopActivationConditionsEvaluationResults?: __LazyJsonString | string;\n}\n\nexport namespace HumanLoopActivationOutput {\n  export const filterSensitiveLog = (obj: HumanLoopActivationOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectModerationLabelsResponse {\n  /**\n   * <p>Array of detected Moderation labels and the time, in milliseconds from the\n   *       start of the video, they were detected.</p>\n   */\n  ModerationLabels?: ModerationLabel[];\n\n  /**\n   * <p>Version number of the moderation detection model that was used to detect unsafe content.</p>\n   */\n  ModerationModelVersion?: string;\n\n  /**\n   * <p>Shows the results of the human in the loop evaluation.</p>\n   */\n  HumanLoopActivationOutput?: HumanLoopActivationOutput;\n}\n\nexport namespace DetectModerationLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectModerationLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The number of in-progress human reviews you have has exceeded the number allowed.</p>\n */\nexport interface HumanLoopQuotaExceededException extends __SmithyException, $MetadataBearer {\n  name: \"HumanLoopQuotaExceededException\";\n  $fault: \"client\";\n  /**\n   * <p>The resource type.</p>\n   */\n  ResourceType?: string;\n\n  /**\n   * <p>The quota code.</p>\n   */\n  QuotaCode?: string;\n\n  /**\n   * <p>The service code.</p>\n   */\n  ServiceCode?: string;\n\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace HumanLoopQuotaExceededException {\n  export const filterSensitiveLog = (obj: HumanLoopQuotaExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies summary attributes to return from a call to <a>DetectProtectiveEquipment</a>.\n *          You can specify which types of PPE to summarize. You can also specify a minimum confidence value for detections.\n *          Summary information is returned in the <code>Summary</code> (<a>ProtectiveEquipmentSummary</a>) field of the response from\n *          <code>DetectProtectiveEquipment</code>.\n *          The summary includes which persons in an image were detected wearing the requested types of person protective equipment (PPE), which persons\n *          were detected as not wearing PPE, and the persons in which a determination could not be made. For more information,\n *          see <a>ProtectiveEquipmentSummary</a>.</p>\n */\nexport interface ProtectiveEquipmentSummarizationAttributes {\n  /**\n   * <p>The minimum confidence level for which you want summary information.\n   *          The confidence level applies to person detection, body part detection, equipment detection, and body part coverage.\n   *          Amazon Rekognition doesn't return summary information with a confidence than this specified value. There isn't a\n   *          default value.</p>\n   *          <p>Specify a <code>MinConfidence</code> value that is between 50-100% as <code>DetectProtectiveEquipment</code>\n   *          returns predictions only where the detection confidence is between 50% - 100%.\n   *          If you specify a value that is less than 50%, the results are the same specifying a value of 50%.</p>\n   *          <p>\n   *       </p>\n   */\n  MinConfidence: number | undefined;\n\n  /**\n   * <p>An array of personal protective equipment types for which you want summary information.\n   *          If a person is detected wearing a required requipment type, the person's ID is added to the\n   *          <code>PersonsWithRequiredEquipment</code> array field returned in <a>ProtectiveEquipmentSummary</a>\n   *          by <code>DetectProtectiveEquipment</code>.  </p>\n   */\n  RequiredEquipmentTypes: (ProtectiveEquipmentType | string)[] | undefined;\n}\n\nexport namespace ProtectiveEquipmentSummarizationAttributes {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentSummarizationAttributes): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectProtectiveEquipmentRequest {\n  /**\n   * <p>The image in which you want to detect PPE on detected persons. The image can be passed as image bytes or you can\n   *          reference an image stored in an Amazon S3 bucket. </p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>An array of PPE types that you want to summarize.</p>\n   */\n  SummarizationAttributes?: ProtectiveEquipmentSummarizationAttributes;\n}\n\nexport namespace DetectProtectiveEquipmentRequest {\n  export const filterSensitiveLog = (obj: DetectProtectiveEquipmentRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A person detected by a call to <a>DetectProtectiveEquipment</a>. The API returns\n *          all persons detected in the input\n *          image in an array of <code>ProtectiveEquipmentPerson</code> objects.</p>\n */\nexport interface ProtectiveEquipmentPerson {\n  /**\n   * <p>An array of body parts detected on a person's body (including body parts without PPE). </p>\n   */\n  BodyParts?: ProtectiveEquipmentBodyPart[];\n\n  /**\n   * <p>A bounding box around the detected person.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has that the bounding box contains a person.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The identifier for the detected person. The identifier is only unique for a single call to\n   *          <code>DetectProtectiveEquipment</code>.</p>\n   */\n  Id?: number;\n}\n\nexport namespace ProtectiveEquipmentPerson {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentPerson): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Summary information for required items of personal protective equipment (PPE) detected on persons by a call\n *          to <a>DetectProtectiveEquipment</a>. You specify the required type of PPE in\n *          the <code>SummarizationAttributes</code>\n *          (<a>ProtectiveEquipmentSummarizationAttributes</a>) input parameter.\n *          The summary includes which persons were detected wearing the required personal protective equipment\n *          (<code>PersonsWithRequiredEquipment</code>),\n *          which persons were detected as not wearing the required PPE (<code>PersonsWithoutRequiredEquipment</code>),\n *          and the persons in which a determination\n *          could not be made (<code>PersonsIndeterminate</code>).</p>\n *          <p>To get a total for each category, use the size of the field array. For example,\n *          to find out how many people were detected as wearing the specified PPE,\n *          use the size of the <code>PersonsWithRequiredEquipment</code> array.\n *          If you want to find out more about a person, such as the\n *          location (<a>BoundingBox</a>) of the person on the image, use the person ID in each array element.\n *          Each person ID matches the ID field of a <a>ProtectiveEquipmentPerson</a> object returned\n *          in the <code>Persons</code> array by <code>DetectProtectiveEquipment</code>.</p>\n */\nexport interface ProtectiveEquipmentSummary {\n  /**\n   * <p>An array of IDs for persons who are wearing detected personal protective equipment.\n   *       </p>\n   */\n  PersonsWithRequiredEquipment?: number[];\n\n  /**\n   * <p>An array of IDs for persons who are not wearing all of the types of PPE specified in the RequiredEquipmentTypes field of\n   *          the detected personal protective equipment.\n   *       </p>\n   */\n  PersonsWithoutRequiredEquipment?: number[];\n\n  /**\n   * <p>An array of IDs for persons where it was not possible to determine if they are wearing personal protective equipment.\n   *       </p>\n   */\n  PersonsIndeterminate?: number[];\n}\n\nexport namespace ProtectiveEquipmentSummary {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentSummary): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectProtectiveEquipmentResponse {\n  /**\n   * <p>The version number of the PPE detection model used to detect PPE in the image.</p>\n   */\n  ProtectiveEquipmentModelVersion?: string;\n\n  /**\n   * <p>An array of persons detected in the image (including persons not wearing PPE).</p>\n   */\n  Persons?: ProtectiveEquipmentPerson[];\n\n  /**\n   * <p>Summary information for the types of PPE specified in the <code>SummarizationAttributes</code> input\n   *       parameter.</p>\n   */\n  Summary?: ProtectiveEquipmentSummary;\n}\n\nexport namespace DetectProtectiveEquipmentResponse {\n  export const filterSensitiveLog = (obj: DetectProtectiveEquipmentResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies a location within the frame that Rekognition checks for text. Uses a <code>BoundingBox</code>\n *       object to set a region of the screen.</p>\n *          <p>A word is included in the region if the word is more than half in that region. If there is more than\n *       one region, the word will be compared with all regions of the screen. Any word more than half in a region\n *       is kept in the results.</p>\n */\nexport interface RegionOfInterest {\n  /**\n   * <p>The box representing a region of interest on screen.</p>\n   */\n  BoundingBox?: BoundingBox;\n}\n\nexport namespace RegionOfInterest {\n  export const filterSensitiveLog = (obj: RegionOfInterest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A set of optional parameters that you can use to set the criteria that the text must meet to be included in your response.\n *       <code>WordFilter</code> looks at a words height, width, and minimum confidence. <code>RegionOfInterest</code>\n *       lets you set a specific region of the image to look for text in.\n *       </p>\n */\nexport interface DetectTextFilters {\n  /**\n   * <p>A set of parameters that allow you to filter out certain results from your returned results.</p>\n   */\n  WordFilter?: DetectionFilter;\n\n  /**\n   * <p> A Filter focusing on a certain area of the image. Uses a <code>BoundingBox</code> object to set the region\n   *       of the image.</p>\n   */\n  RegionsOfInterest?: RegionOfInterest[];\n}\n\nexport namespace DetectTextFilters {\n  export const filterSensitiveLog = (obj: DetectTextFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectTextRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an Amazon S3 object. If you use the AWS CLI\n   *       to call Amazon Rekognition operations, you can't pass image bytes. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Optional parameters that let you set the criteria that the text must meet to be included in your response.</p>\n   */\n  Filters?: DetectTextFilters;\n}\n\nexport namespace DetectTextRequest {\n  export const filterSensitiveLog = (obj: DetectTextRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum TextTypes {\n  LINE = \"LINE\",\n  WORD = \"WORD\",\n}\n\n/**\n * <p>Information about a word or line of text detected by <a>DetectText</a>.</p>\n *          <p>The <code>DetectedText</code> field contains the text that Amazon Rekognition detected in the\n *       image. </p>\n *          <p>Every word and line has an identifier (<code>Id</code>). Each word belongs to a line\n *       and has a parent identifier (<code>ParentId</code>) that identifies the line of text in which\n *       the word appears. The word <code>Id</code> is also an index for the word within a line of\n *       words. </p>\n *\n *          <p>For more information, see Detecting Text in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface TextDetection {\n  /**\n   * <p>The word or line of text recognized by Amazon Rekognition. </p>\n   */\n  DetectedText?: string;\n\n  /**\n   * <p>The type of text that was detected.</p>\n   */\n  Type?: TextTypes | string;\n\n  /**\n   * <p>The identifier for the detected text. The identifier is only unique for a single call\n   *       to <code>DetectText</code>. </p>\n   */\n  Id?: number;\n\n  /**\n   * <p>The Parent identifier for the detected text identified by the value of <code>ID</code>.\n   *       If the type of detected text is <code>LINE</code>, the value of <code>ParentId</code> is\n   *         <code>Null</code>. </p>\n   */\n  ParentId?: number;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the accuracy of the detected text and the accuracy\n   *       of the geometry points around the detected text.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The location of the detected text on the image. Includes an axis aligned coarse\n   *       bounding box surrounding the text and a finer grain polygon for more accurate spatial\n   *       information.</p>\n   */\n  Geometry?: Geometry;\n}\n\nexport namespace TextDetection {\n  export const filterSensitiveLog = (obj: TextDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectTextResponse {\n  /**\n   * <p>An array of text that was detected in the input image.</p>\n   */\n  TextDetections?: TextDetection[];\n\n  /**\n   * <p>The model version used to detect text.</p>\n   */\n  TextModelVersion?: string;\n}\n\nexport namespace DetectTextResponse {\n  export const filterSensitiveLog = (obj: DetectTextResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the face properties such as the bounding box, face ID, image ID of the input\n *       image, and external image ID that you assigned. </p>\n */\nexport interface Face {\n  /**\n   * <p>Unique identifier that Amazon Rekognition assigns to the face.</p>\n   */\n  FaceId?: string;\n\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Unique identifier that Amazon Rekognition assigns to the input image.</p>\n   */\n  ImageId?: string;\n\n  /**\n   * <p>Identifier that you assign to all the faces in the input image.</p>\n   */\n  ExternalImageId?: string;\n\n  /**\n   * <p>Confidence level that the bounding box contains a face (and not a different object such\n   *       as a tree).</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Face {\n  export const filterSensitiveLog = (obj: Face): any => ({\n    ...obj,\n  });\n}\n\nexport enum FaceAttributes {\n  ALL = \"ALL\",\n  DEFAULT = \"DEFAULT\",\n}\n\n/**\n * <p>Information about a face detected in a video analysis request and the time the face was detected in the video. </p>\n */\nexport interface FaceDetection {\n  /**\n   * <p>Time, in milliseconds from the start of the video, that the face was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>The face properties for the detected face.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace FaceDetection {\n  export const filterSensitiveLog = (obj: FaceDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides face metadata. In addition, it also provides the confidence in the match of\n *       this face with the input face.</p>\n */\nexport interface FaceMatch {\n  /**\n   * <p>Confidence in the match of this face with the input face.</p>\n   */\n  Similarity?: number;\n\n  /**\n   * <p>Describes the face properties such as the bounding box, face ID, image ID of the source\n   *       image, and external image ID that you assigned.</p>\n   */\n  Face?: Face;\n}\n\nexport namespace FaceMatch {\n  export const filterSensitiveLog = (obj: FaceMatch): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Object containing both the face metadata (stored in the backend database), and facial\n *       attributes that are detected but aren't stored in the database.</p>\n */\nexport interface FaceRecord {\n  /**\n   * <p>Describes the face properties such as the bounding box, face ID, image ID of the input\n   *       image, and external image ID that you assigned. </p>\n   */\n  Face?: Face;\n\n  /**\n   * <p>Structure containing attributes of the face that the algorithm detected.</p>\n   */\n  FaceDetail?: FaceDetail;\n}\n\nexport namespace FaceRecord {\n  export const filterSensitiveLog = (obj: FaceRecord): any => ({\n    ...obj,\n  });\n}\n\nexport enum FaceSearchSortBy {\n  INDEX = \"INDEX\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetCelebrityInfoRequest {\n  /**\n   * <p>The ID for the celebrity. You get the celebrity ID from a call to the <a>RecognizeCelebrities</a> operation,\n   *    which recognizes celebrities in an image. </p>\n   */\n  Id: string | undefined;\n}\n\nexport namespace GetCelebrityInfoRequest {\n  export const filterSensitiveLog = (obj: GetCelebrityInfoRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityInfoResponse {\n  /**\n   * <p>An array of URLs pointing to additional celebrity information. </p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n}\n\nexport namespace GetCelebrityInfoResponse {\n  export const filterSensitiveLog = (obj: GetCelebrityInfoResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityRecognitionRequest {\n  /**\n   * <p>Job identifier for the required celebrity recognition analysis. You can get the job identifer from\n   *       a call to <code>StartCelebrityRecognition</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more recognized celebrities to retrieve), Amazon Rekognition Video returns a pagination\n   *       token in the response. You can use this pagination token to retrieve the next set of celebrities. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for celebrities returned in <code>Celebrities</code> field. Specify <code>ID</code> to sort by the celebrity identifier,\n   *         specify <code>TIMESTAMP</code> to sort by the time the celebrity was recognized.</p>\n   */\n  SortBy?: CelebrityRecognitionSortBy | string;\n}\n\nexport namespace GetCelebrityRecognitionRequest {\n  export const filterSensitiveLog = (obj: GetCelebrityRecognitionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum VideoJobStatus {\n  FAILED = \"FAILED\",\n  IN_PROGRESS = \"IN_PROGRESS\",\n  SUCCEEDED = \"SUCCEEDED\",\n}\n\n/**\n * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in\n *             every page of paginated responses from a Amazon Rekognition video operation.</p>\n */\nexport interface VideoMetadata {\n  /**\n   * <p>Type of compression used in the analyzed video. </p>\n   */\n  Codec?: string;\n\n  /**\n   * <p>Length of the video in milliseconds.</p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>Format of the analyzed video. Possible values are MP4, MOV and AVI. </p>\n   */\n  Format?: string;\n\n  /**\n   * <p>Number of frames per second in the video.</p>\n   */\n  FrameRate?: number;\n\n  /**\n   * <p>Vertical pixel dimension of the video.</p>\n   */\n  FrameHeight?: number;\n\n  /**\n   * <p>Horizontal pixel dimension of the video.</p>\n   */\n  FrameWidth?: number;\n}\n\nexport namespace VideoMetadata {\n  export const filterSensitiveLog = (obj: VideoMetadata): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityRecognitionResponse {\n  /**\n   * <p>The current status of the celebrity recognition job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *       every page of paginated responses from a Amazon Rekognition Video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request\n   *       to retrieve the next set of celebrities.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Array of celebrities recognized in the video.</p>\n   */\n  Celebrities?: CelebrityRecognition[];\n}\n\nexport namespace GetCelebrityRecognitionResponse {\n  export const filterSensitiveLog = (obj: GetCelebrityRecognitionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetContentModerationRequest {\n  /**\n   * <p>The identifier for the unsafe content job. Use <code>JobId</code> to identify the job in\n   *        a subsequent call to <code>GetContentModeration</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *     If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *     The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more data to retrieve), Amazon Rekognition\n   *         returns a pagination token in the response. You can use this pagination token\n   *         to retrieve the next set of unsafe content labels.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>ModerationLabelDetections</code> array.\n   *        Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.\n   *        Use <code>NAME</code> to alphabetically group elements for a label together.\n   *        Within each label group, the array element are sorted by detection confidence.\n   *        The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: ContentModerationSortBy | string;\n}\n\nexport namespace GetContentModerationRequest {\n  export const filterSensitiveLog = (obj: GetContentModerationRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetContentModerationResponse {\n  /**\n   * <p>The current status of the unsafe content analysis job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code>\n   *      is returned in every page of paginated responses from <code>GetContentModeration</code>. </p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>The detected unsafe content labels and the time(s) they were detected.</p>\n   */\n  ModerationLabels?: ContentModerationDetection[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *      request to retrieve the next set of unsafe content labels. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the moderation detection model that was used to detect unsafe content.</p>\n   */\n  ModerationModelVersion?: string;\n}\n\nexport namespace GetContentModerationResponse {\n  export const filterSensitiveLog = (obj: GetContentModerationResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceDetectionRequest {\n  /**\n   * <p>Unique identifier for the face detection job. The <code>JobId</code> is returned from <code>StartFaceDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *        If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *        The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more faces to retrieve), Amazon Rekognition Video returns a pagination\n   *        token in the response. You can use this pagination token to retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetFaceDetectionRequest {\n  export const filterSensitiveLog = (obj: GetFaceDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceDetectionResponse {\n  /**\n   * <p>The current status of the face detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of faces detected in the video. Each element contains a detected face's details and the time,\n   *        in milliseconds from the start of the video, the face was detected. </p>\n   */\n  Faces?: FaceDetection[];\n}\n\nexport namespace GetFaceDetectionResponse {\n  export const filterSensitiveLog = (obj: GetFaceDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceSearchRequest {\n  /**\n   * <p>The job identifer for the search request. You get the job identifier from an initial call to <code>StartFaceSearch</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more search results to retrieve), Amazon Rekognition Video returns a pagination\n   *       token in the response. You can use this pagination token to retrieve the next set of search results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for grouping faces in the response. Use <code>TIMESTAMP</code> to group faces by the time\n   *       that they are recognized. Use <code>INDEX</code> to sort by recognized faces. </p>\n   */\n  SortBy?: FaceSearchSortBy | string;\n}\n\nexport namespace GetFaceSearchRequest {\n  export const filterSensitiveLog = (obj: GetFaceSearchRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Details about a person detected in a video analysis request.</p>\n */\nexport interface PersonDetail {\n  /**\n   * <p>Identifier for the person detected person within a video. Use to keep track of the person throughout the video. The identifier is not stored by Amazon Rekognition.</p>\n   */\n  Index?: number;\n\n  /**\n   * <p>Bounding box around the detected person.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Face details for the detected person.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace PersonDetail {\n  export const filterSensitiveLog = (obj: PersonDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a person whose face matches a face(s) in an Amazon Rekognition collection.\n *       Includes information about the faces in the Amazon Rekognition collection (<a>FaceMatch</a>), information about the person (<a>PersonDetail</a>),\n *       and the time stamp for when the person was detected in a video. An array of\n *         <code>PersonMatch</code> objects is returned by <a>GetFaceSearch</a>. </p>\n */\nexport interface PersonMatch {\n  /**\n   * <p>The time, in milliseconds from the beginning of the video, that the person was matched in the video.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Information about the matched person.</p>\n   */\n  Person?: PersonDetail;\n\n  /**\n   * <p>Information about the faces in the input collection that match the face of a person in the video.</p>\n   */\n  FaceMatches?: FaceMatch[];\n}\n\nexport namespace PersonMatch {\n  export const filterSensitiveLog = (obj: PersonMatch): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceSearchResponse {\n  /**\n   * <p>The current status of the face search job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses\n   *       from a Amazon Rekognition Video operation. </p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>An array of persons,  <a>PersonMatch</a>,\n   *       in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information\n   *        for when persons are matched in the video.\n   *       You specify the input collection in an initial call to <code>StartFaceSearch</code>.\n   *       Each  <code>Persons</code> element includes a time the person was matched,\n   *       face match details (<code>FaceMatches</code>) for matching faces in the collection,\n   *        and person information (<code>Person</code>) for the matched person. </p>\n   */\n  Persons?: PersonMatch[];\n}\n\nexport namespace GetFaceSearchResponse {\n  export const filterSensitiveLog = (obj: GetFaceSearchResponse): any => ({\n    ...obj,\n  });\n}\n\nexport enum LabelDetectionSortBy {\n  NAME = \"NAME\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetLabelDetectionRequest {\n  /**\n   * <p>Job identifier for the label detection operation for which you want results returned. You get the job identifer from\n   *       an initial call to <code>StartlabelDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *        If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *        The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination\n   *          token in the response. You can use this pagination token to retrieve the next set of labels. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>Labels</code> array.\n   *       Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.\n   *       Use <code>NAME</code> to alphabetically group elements for a label together.\n   *       Within each label group, the array element are sorted by detection confidence.\n   *       The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: LabelDetectionSortBy | string;\n}\n\nexport namespace GetLabelDetectionRequest {\n  export const filterSensitiveLog = (obj: GetLabelDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a label detected in a video analysis request and the time the label was detected in the video. </p>\n */\nexport interface LabelDetection {\n  /**\n   * <p>Time, in milliseconds from the start of the video, that the label was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about the detected label.</p>\n   */\n  Label?: Label;\n}\n\nexport namespace LabelDetection {\n  export const filterSensitiveLog = (obj: LabelDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetLabelDetectionResponse {\n  /**\n   * <p>The current status of the label detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request\n   *         to retrieve the next set of labels.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of labels detected in the video. Each element contains the detected label and the time,\n   *         in milliseconds from the start of the video, that the label was detected. </p>\n   */\n  Labels?: LabelDetection[];\n\n  /**\n   * <p>Version number of the label detection model that was used to detect labels.</p>\n   */\n  LabelModelVersion?: string;\n}\n\nexport namespace GetLabelDetectionResponse {\n  export const filterSensitiveLog = (obj: GetLabelDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport enum PersonTrackingSortBy {\n  INDEX = \"INDEX\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetPersonTrackingRequest {\n  /**\n   * <p>The identifier for a job that tracks persons in a video. You get the <code>JobId</code> from a call to <code>StartPersonTracking</code>.\n   *         </p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more persons to retrieve), Amazon Rekognition Video returns a pagination\n   *        token in the response. You can use this pagination token to retrieve the next set of persons. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>Persons</code> array. Use <code>TIMESTAMP</code> to sort array elements\n   *        by the time persons are detected. Use <code>INDEX</code> to sort by the tracked persons.\n   *        If you sort by <code>INDEX</code>, the array elements for each person are sorted by detection confidence.\n   *        The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: PersonTrackingSortBy | string;\n}\n\nexport namespace GetPersonTrackingRequest {\n  export const filterSensitiveLog = (obj: GetPersonTrackingRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Details and path tracking information for a single time a person's path is tracked in a video.\n *             Amazon Rekognition operations that track people's paths return an array of <code>PersonDetection</code> objects\n *             with elements for each time a person's path is tracked in a video. </p>\n *\n *          <p>For more information, see GetPersonTracking in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface PersonDetection {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the person's path was tracked.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about a person whose path was tracked in a video.</p>\n   */\n  Person?: PersonDetail;\n}\n\nexport namespace PersonDetection {\n  export const filterSensitiveLog = (obj: PersonDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetPersonTrackingResponse {\n  /**\n   * <p>The current status of the person tracking job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition Video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of persons. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of the persons detected in the video and the time(s) their path was tracked throughout the video.\n   *         An array element will exist for each time a person's path is tracked. </p>\n   */\n  Persons?: PersonDetection[];\n}\n\nexport namespace GetPersonTrackingResponse {\n  export const filterSensitiveLog = (obj: GetPersonTrackingResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetSegmentDetectionRequest {\n  /**\n   * <p>Job identifier for the text detection operation for which you want results returned.\n   *       You get the job identifer from an initial call to <code>StartSegmentDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *       request to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetSegmentDetectionRequest {\n  export const filterSensitiveLog = (obj: GetSegmentDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a shot detection segment detected in a video. For more information,\n *       see <a>SegmentDetection</a>.</p>\n */\nexport interface ShotSegment {\n  /**\n   * <p>An Identifier for a shot detection segment detected in a video. </p>\n   */\n  Index?: number;\n\n  /**\n   * <p>The confidence that Amazon Rekognition Video has in the accuracy of the detected segment.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace ShotSegment {\n  export const filterSensitiveLog = (obj: ShotSegment): any => ({\n    ...obj,\n  });\n}\n\nexport enum TechnicalCueType {\n  BLACK_FRAMES = \"BlackFrames\",\n  COLOR_BARS = \"ColorBars\",\n  END_CREDITS = \"EndCredits\",\n}\n\n/**\n * <p>Information about a technical cue segment. For more information, see <a>SegmentDetection</a>.</p>\n */\nexport interface TechnicalCueSegment {\n  /**\n   * <p>The type of the technical cue.</p>\n   */\n  Type?: TechnicalCueType | string;\n\n  /**\n   * <p>The confidence that Amazon Rekognition Video has in the accuracy of the detected segment.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace TechnicalCueSegment {\n  export const filterSensitiveLog = (obj: TechnicalCueSegment): any => ({\n    ...obj,\n  });\n}\n\nexport enum SegmentType {\n  SHOT = \"SHOT\",\n  TECHNICAL_CUE = \"TECHNICAL_CUE\",\n}\n\n/**\n * <p>A technical cue or shot detection segment detected in a video. An array\n *     of <code>SegmentDetection</code> objects containing all segments detected in a stored video\n *       is returned by <a>GetSegmentDetection</a>.\n *     </p>\n */\nexport interface SegmentDetection {\n  /**\n   * <p>The type of the  segment. Valid values are <code>TECHNICAL_CUE</code> and <code>SHOT</code>.</p>\n   */\n  Type?: SegmentType | string;\n\n  /**\n   * <p>The start time of the detected segment in milliseconds from the start of the video. This value\n   *       is rounded down. For example, if the actual timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a value of\n   *       100 millis.</p>\n   */\n  StartTimestampMillis?: number;\n\n  /**\n   * <p>The end time of the detected segment, in milliseconds, from the start of the video.\n   *     This value is rounded down.</p>\n   */\n  EndTimestampMillis?: number;\n\n  /**\n   * <p>The duration of the detected segment in milliseconds. </p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>The frame-accurate SMPTE timecode, from the start of a video, for the start of a detected segment.\n   *       <code>StartTimecode</code> is in <i>HH:MM:SS:fr</i> format\n   *       (and <i>;fr</i> for drop frame-rates). </p>\n   */\n  StartTimecodeSMPTE?: string;\n\n  /**\n   * <p>The frame-accurate SMPTE timecode, from the start of a video, for the end of a detected segment.\n   *       <code>EndTimecode</code> is in <i>HH:MM:SS:fr</i> format\n   *       (and <i>;fr</i> for drop frame-rates).</p>\n   */\n  EndTimecodeSMPTE?: string;\n\n  /**\n   * <p>The duration of the timecode for the detected segment in SMPTE format.</p>\n   */\n  DurationSMPTE?: string;\n\n  /**\n   * <p>If the segment is a technical cue, contains information about the technical cue.</p>\n   */\n  TechnicalCueSegment?: TechnicalCueSegment;\n\n  /**\n   * <p>If the segment is a shot detection, contains information about the shot detection.</p>\n   */\n  ShotSegment?: ShotSegment;\n}\n\nexport namespace SegmentDetection {\n  export const filterSensitiveLog = (obj: SegmentDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the type of a segment requested in a call to <a>StartSegmentDetection</a>.\n *       An array of <code>SegmentTypeInfo</code> objects is returned  by the response from <a>GetSegmentDetection</a>.</p>\n */\nexport interface SegmentTypeInfo {\n  /**\n   * <p>The type of a segment (technical cue or shot detection).</p>\n   */\n  Type?: SegmentType | string;\n\n  /**\n   * <p>The version of the model used to detect segments.</p>\n   */\n  ModelVersion?: string;\n}\n\nexport namespace SegmentTypeInfo {\n  export const filterSensitiveLog = (obj: SegmentTypeInfo): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetSegmentDetectionResponse {\n  /**\n   * <p>Current status of the segment detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Currently, Amazon Rekognition Video returns a single   object in the\n   *       <code>VideoMetadata</code> array. The object\n   *       contains information about the video stream in the input file that Amazon Rekognition Video chose to analyze.\n   *       The <code>VideoMetadata</code> object includes the video codec, video format and other information.\n   *       Video metadata is returned in each page of information returned by <code>GetSegmentDetection</code>.</p>\n   */\n  VideoMetadata?: VideoMetadata[];\n\n  /**\n   * <p>An array of\n   *        objects. There can be multiple audio streams.\n   *       Each <code>AudioMetadata</code> object contains metadata for a single audio stream.\n   *       Audio information in an <code>AudioMetadata</code> objects includes\n   *       the audio codec, the number of audio channels, the duration of the audio stream,\n   *       and the sample rate. Audio metadata is returned in each page of information returned\n   *       by <code>GetSegmentDetection</code>.</p>\n   */\n  AudioMetadata?: AudioMetadata[];\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns\n   *       a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of segments detected in a video.  The array is sorted by the segment types (TECHNICAL_CUE or SHOT)\n   *       specified in the <code>SegmentTypes</code> input parameter of <code>StartSegmentDetection</code>. Within\n   *       each segment type the array is sorted by timestamp values.</p>\n   */\n  Segments?: SegmentDetection[];\n\n  /**\n   * <p>An array containing the segment types requested in the call to <code>StartSegmentDetection</code>.\n   *     </p>\n   */\n  SelectedSegmentTypes?: SegmentTypeInfo[];\n}\n\nexport namespace GetSegmentDetectionResponse {\n  export const filterSensitiveLog = (obj: GetSegmentDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetTextDetectionRequest {\n  /**\n   * <p>Job identifier for the text detection operation for which you want results returned.\n   *         You get the job identifer from an initial call to <code>StartTextDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns\n   *       a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetTextDetectionRequest {\n  export const filterSensitiveLog = (obj: GetTextDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about text detected in a video. Incudes the detected text,\n *         the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>\n */\nexport interface TextDetectionResult {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the text was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about text detected in a video.</p>\n   */\n  TextDetection?: TextDetection;\n}\n\nexport namespace TextDetectionResult {\n  export const filterSensitiveLog = (obj: TextDetectionResult): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetTextDetectionResponse {\n  /**\n   * <p>Current status of the text detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in\n   *             every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds\n   *       from the start of the video that the text was detected, and where it was detected on the screen.</p>\n   */\n  TextDetections?: TextDetectionResult[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *         request to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the text detection model that was used to detect text.</p>\n   */\n  TextModelVersion?: string;\n}\n\nexport namespace GetTextDetectionResponse {\n  export const filterSensitiveLog = (obj: GetTextDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A <code>ClientRequestToken</code> input parameter was reused with an operation, but at least one of the other input\n *         parameters is different from the previous call to the operation.</p>\n */\nexport interface IdempotentParameterMismatchException extends __SmithyException, $MetadataBearer {\n  name: \"IdempotentParameterMismatchException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace IdempotentParameterMismatchException {\n  export const filterSensitiveLog = (obj: IdempotentParameterMismatchException): any => ({\n    ...obj,\n  });\n}\n\nexport interface IndexFacesRequest {\n  /**\n   * <p>The ID of an existing collection to which you want to add the faces that are detected\n   *       in the input images.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>The ID you want to assign to all the faces detected in the image.</p>\n   */\n  ExternalImageId?: string;\n\n  /**\n   * <p>An array of facial attributes that you want to be returned. This can be the default\n   *       list of attributes or all attributes. If you don't specify a value for <code>Attributes</code>\n   *       or if you specify <code>[\"DEFAULT\"]</code>, the API returns the following subset of facial\n   *       attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,\n   *         <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>[\"ALL\"]</code>, all\n   *       facial attributes are returned, but the operation takes longer to complete.</p>\n   *          <p>If you provide both, <code>[\"ALL\", \"DEFAULT\"]</code>, the service uses a logical AND\n   *       operator to determine which attributes to return (in this case, all attributes). </p>\n   */\n  DetectionAttributes?: (Attribute | string)[];\n\n  /**\n   * <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater\n   *       than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an\n   *       image, even if you specify a larger value for <code>MaxFaces</code>.</p>\n   *          <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the\n   *       faces with the lowest quality are filtered out first. If there are still more faces than the\n   *       value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out\n   *       (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information\n   *       about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>\n   *          <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face\n   *       bounding box size to the smallest size, in descending order.</p>\n   *          <p>\n   *             <code>MaxFaces</code> can be used with a collection associated with any version of\n   *       the face model.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *     Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.\n   *       If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       dont meet the chosen quality bar.  The default value is <code>AUTO</code>.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace IndexFacesRequest {\n  export const filterSensitiveLog = (obj: IndexFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum Reason {\n  EXCEEDS_MAX_FACES = \"EXCEEDS_MAX_FACES\",\n  EXTREME_POSE = \"EXTREME_POSE\",\n  LOW_BRIGHTNESS = \"LOW_BRIGHTNESS\",\n  LOW_CONFIDENCE = \"LOW_CONFIDENCE\",\n  LOW_FACE_QUALITY = \"LOW_FACE_QUALITY\",\n  LOW_SHARPNESS = \"LOW_SHARPNESS\",\n  SMALL_BOUNDING_BOX = \"SMALL_BOUNDING_BOX\",\n}\n\n/**\n * <p>A face that <a>IndexFaces</a> detected, but didn't index. Use the\n *         <code>Reasons</code> response attribute to determine why a face wasn't indexed.</p>\n */\nexport interface UnindexedFace {\n  /**\n   * <p>An array of reasons that specify why a face wasn't indexed. </p>\n   *          <ul>\n   *             <li>\n   *                <p>EXTREME_POSE - The face is at a pose that can't be detected. For example, the head is turned\n   *           too far away from the camera.</p>\n   *             </li>\n   *             <li>\n   *                <p>EXCEEDS_MAX_FACES - The number of faces detected is already higher than that specified by the\n   *       <code>MaxFaces</code> input parameter for <code>IndexFaces</code>.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_BRIGHTNESS - The image is too dark.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_SHARPNESS - The image is too blurry.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_CONFIDENCE - The face was detected with a low confidence.</p>\n   *             </li>\n   *             <li>\n   *                <p>SMALL_BOUNDING_BOX - The bounding box around the face is too small.</p>\n   *             </li>\n   *          </ul>\n   */\n  Reasons?: (Reason | string)[];\n\n  /**\n   * <p>The\n   *       structure that contains attributes of a face that\n   *       <code>IndexFaces</code>detected, but didn't index. </p>\n   */\n  FaceDetail?: FaceDetail;\n}\n\nexport namespace UnindexedFace {\n  export const filterSensitiveLog = (obj: UnindexedFace): any => ({\n    ...obj,\n  });\n}\n\nexport interface IndexFacesResponse {\n  /**\n   * <p>An array of faces detected and added to the collection.\n   *       For more information, see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.\n   *     </p>\n   */\n  FaceRecords?: FaceRecord[];\n\n  /**\n   * <p>If your collection is associated with a face detection model that's later\n   *       than version 3.0, the value of <code>OrientationCorrection</code>\n   *       is always null and no orientation information is returned.</p>\n   *\n   *          <p>If your collection is associated with a face detection model that's\n   *       version 3.0 or earlier, the following applies:</p>\n   *          <ul>\n   *             <li>\n   *                <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *         that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *         image correction - the bounding box coordinates are translated to represent object locations\n   *         after the orientation information in the Exif metadata is used to correct the image orientation.\n   *         Images in .png format don't contain Exif metadata. The value of <code>OrientationCorrection</code>\n   *         is null.</p>\n   *             </li>\n   *             <li>\n   *                <p>If the image doesn't contain orientation information in its Exif metadata, Amazon Rekognition returns\n   *       an estimated orientation (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition doesnt perform\n   *       image correction for images. The bounding box coordinates aren't translated and represent the\n   *       object locations before the image is rotated.</p>\n   *             </li>\n   *          </ul>\n   *\n   *\n   *\n   *          <p>Bounding box information is returned in the <code>FaceRecords</code> array. You can get the\n   *     version of the face detection model by calling <a>DescribeCollection</a>. </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>The version number of the face detection model that's associated with the input\n   *       collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n\n  /**\n   * <p>An array of faces that were detected in the image but weren't indexed. They weren't\n   *       indexed because the quality filter identified them as low quality, or the\n   *         <code>MaxFaces</code> request parameter filtered them out. To use the quality filter, you\n   *       specify the <code>QualityFilter</code> request parameter.</p>\n   */\n  UnindexedFaces?: UnindexedFace[];\n}\n\nexport namespace IndexFacesResponse {\n  export const filterSensitiveLog = (obj: IndexFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p></p>\n *\n *\n *          <p>The size of the collection exceeds the allowed limit. For more information, see\n *       Limits in Amazon Rekognition in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface ServiceQuotaExceededException extends __SmithyException, $MetadataBearer {\n  name: \"ServiceQuotaExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ServiceQuotaExceededException {\n  export const filterSensitiveLog = (obj: ServiceQuotaExceededException): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListCollectionsRequest {\n  /**\n   * <p>Pagination token from the previous response.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of collection IDs to return. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListCollectionsRequest {\n  export const filterSensitiveLog = (obj: ListCollectionsRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListCollectionsResponse {\n  /**\n   * <p>An array of collection IDs.</p>\n   */\n  CollectionIds?: string[];\n\n  /**\n   * <p>If the result is truncated, the response provides a <code>NextToken</code> that you can\n   *       use in the subsequent request to fetch the next set of collection IDs.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version numbers of the face detection models associated with the collections in the array <code>CollectionIds</code>.\n   *     For example, the value of <code>FaceModelVersions[2]</code> is the version number for the face detection model used\n   *       by the collection in <code>CollectionId[2]</code>.</p>\n   */\n  FaceModelVersions?: string[];\n}\n\nexport namespace ListCollectionsResponse {\n  export const filterSensitiveLog = (obj: ListCollectionsResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListFacesRequest {\n  /**\n   * <p>ID of the collection from which to list the faces.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more data to retrieve),\n   *       Amazon Rekognition returns a pagination token in the response. You can use this pagination token to\n   *       retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of faces to return.</p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListFacesRequest {\n  export const filterSensitiveLog = (obj: ListFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListFacesResponse {\n  /**\n   * <p>An array of <code>Face</code> objects. </p>\n   */\n  Faces?: Face[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition returns this token that you can use in the\n   *       subsequent request to retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace ListFacesResponse {\n  export const filterSensitiveLog = (obj: ListFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListStreamProcessorsRequest {\n  /**\n   * <p>If the previous response was incomplete (because there are more stream processors to retrieve), Amazon Rekognition Video\n   *             returns a pagination token in the response. You can use this pagination token to retrieve the next set of stream processors. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of stream processors you want Amazon Rekognition Video to return in the response. The default is 1000. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListStreamProcessorsRequest {\n  export const filterSensitiveLog = (obj: ListStreamProcessorsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An object that recognizes faces in a streaming video. An Amazon Rekognition stream processor is created by a call to <a>CreateStreamProcessor</a>.  The request\n *         parameters for <code>CreateStreamProcessor</code> describe the Kinesis video stream source for the streaming video, face recognition parameters, and where to stream the analysis resullts.\n *\n *         </p>\n */\nexport interface StreamProcessor {\n  /**\n   * <p>Name of the Amazon Rekognition stream processor. </p>\n   */\n  Name?: string;\n\n  /**\n   * <p>Current status of the Amazon Rekognition stream processor.</p>\n   */\n  Status?: StreamProcessorStatus | string;\n}\n\nexport namespace StreamProcessor {\n  export const filterSensitiveLog = (obj: StreamProcessor): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListStreamProcessorsResponse {\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *             request to retrieve the next set of stream processors. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>List of stream processors that you have created.</p>\n   */\n  StreamProcessors?: StreamProcessor[];\n}\n\nexport namespace ListStreamProcessorsResponse {\n  export const filterSensitiveLog = (obj: ListStreamProcessorsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see\n *             <a>api-video</a>.</p>\n */\nexport interface NotificationChannel {\n  /**\n   * <p>The Amazon SNS topic to which Amazon Rekognition to posts the completion status.</p>\n   */\n  SNSTopicArn: string | undefined;\n\n  /**\n   * <p>The ARN of an IAM role that gives Amazon Rekognition publishing permissions to the Amazon SNS topic. </p>\n   */\n  RoleArn: string | undefined;\n}\n\nexport namespace NotificationChannel {\n  export const filterSensitiveLog = (obj: NotificationChannel): any => ({\n    ...obj,\n  });\n}\n\nexport interface RecognizeCelebritiesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n}\n\nexport namespace RecognizeCelebritiesRequest {\n  export const filterSensitiveLog = (obj: RecognizeCelebritiesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface RecognizeCelebritiesResponse {\n  /**\n   * <p>Details about each celebrity found in the image. Amazon Rekognition can detect a maximum of 64\n   *       celebrities in an image.</p>\n   */\n  CelebrityFaces?: Celebrity[];\n\n  /**\n   * <p>Details about each unrecognized face in the image.</p>\n   */\n  UnrecognizedFaces?: ComparedFace[];\n\n  /**\n   * <p>The orientation of the input image (counterclockwise direction). If your application\n   *       displays the image, you can use this value to correct the orientation. The bounding box\n   *       coordinates returned in <code>CelebrityFaces</code> and <code>UnrecognizedFaces</code>\n   *       represent face locations before the image orientation is corrected. </p>\n   *          <note>\n   *             <p>If the input image is in .jpeg format, it might contain exchangeable image (Exif)\n   *         metadata that includes the image's orientation. If so, and the Exif metadata for the input\n   *         image populates the orientation field, the value of <code>OrientationCorrection</code> is\n   *         null. The <code>CelebrityFaces</code> and <code>UnrecognizedFaces</code> bounding box\n   *         coordinates represent face locations after Exif metadata is used to correct the image\n   *         orientation. Images in .png format don't contain Exif metadata. </p>\n   *          </note>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace RecognizeCelebritiesResponse {\n  export const filterSensitiveLog = (obj: RecognizeCelebritiesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesRequest {\n  /**\n   * <p>ID of the collection the face belongs to.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>ID of a face to find matches for in the collection.</p>\n   */\n  FaceId: string | undefined;\n\n  /**\n   * <p>Maximum number of faces to return. The operation returns the maximum number of faces\n   *       with the highest confidence in the match.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>Optional value specifying the minimum confidence in the face match to return. For\n   *       example, don't return any matches where confidence in matches is less than 70%.\n   *       The default value is 80%.\n   *     </p>\n   */\n  FaceMatchThreshold?: number;\n}\n\nexport namespace SearchFacesRequest {\n  export const filterSensitiveLog = (obj: SearchFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesResponse {\n  /**\n   * <p>ID of the face that was searched for matches in a collection.</p>\n   */\n  SearchedFaceId?: string;\n\n  /**\n   * <p>An array of faces that matched the input face, along with the confidence in the\n   *       match.</p>\n   */\n  FaceMatches?: FaceMatch[];\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace SearchFacesResponse {\n  export const filterSensitiveLog = (obj: SearchFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesByImageRequest {\n  /**\n   * <p>ID of the collection to search.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of faces to return. The operation returns the maximum number of faces\n   *       with the highest confidence in the match.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>(Optional) Specifies the minimum confidence in the face match to return. For example,\n   *       don't return any matches where confidence in matches is less than 70%.\n   *     The default value is 80%.</p>\n   */\n  FaceMatchThreshold?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *       Filtered faces aren't searched for in the collection. If you specify <code>AUTO</code>, Amazon Rekognition\n   *       chooses the quality bar.  If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       dont meet the chosen quality bar.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed.  The default value is <code>NONE</code>.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace SearchFacesByImageRequest {\n  export const filterSensitiveLog = (obj: SearchFacesByImageRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesByImageResponse {\n  /**\n   * <p>The bounding box around the face in the input image that Amazon Rekognition used for the\n   *       search.</p>\n   */\n  SearchedFaceBoundingBox?: BoundingBox;\n\n  /**\n   * <p>The level of confidence that the <code>searchedFaceBoundingBox</code>, contains a\n   *       face.</p>\n   */\n  SearchedFaceConfidence?: number;\n\n  /**\n   * <p>An array of faces that match the input face, along with the confidence in the\n   *       match.</p>\n   */\n  FaceMatches?: FaceMatch[];\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace SearchFacesByImageResponse {\n  export const filterSensitiveLog = (obj: SearchFacesByImageResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n */\nexport interface Video {\n  /**\n   * <p>The Amazon S3 bucket name and file name for the video.</p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Video {\n  export const filterSensitiveLog = (obj: Video): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartCelebrityRecognitionRequest {\n  /**\n   * <p>The video in which you want to recognize celebrities. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *     <code>StartCelebrityRecognition</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the\n   *       celebrity recognition analysis to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartCelebrityRecognitionRequest {\n  export const filterSensitiveLog = (obj: StartCelebrityRecognitionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartCelebrityRecognitionResponse {\n  /**\n   * <p>The identifier for the celebrity recognition analysis job. Use <code>JobId</code> to identify the job in\n   *       a subsequent call to <code>GetCelebrityRecognition</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartCelebrityRecognitionResponse {\n  export const filterSensitiveLog = (obj: StartCelebrityRecognitionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The file size or duration of the supplied media is too large. The maximum file size is 10GB.\n *         The maximum duration is 6 hours. </p>\n */\nexport interface VideoTooLargeException extends __SmithyException, $MetadataBearer {\n  name: \"VideoTooLargeException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace VideoTooLargeException {\n  export const filterSensitiveLog = (obj: VideoTooLargeException): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartContentModerationRequest {\n  /**\n   * <p>The video in which you want to detect unsafe content. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition must have in order to return a moderated content label. Confidence\n   *       represents how certain Amazon Rekognition is that the moderated content is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition doesn't return any moderated content labels with a confidence level\n   *       lower than this specified value. If you don't specify <code>MinConfidence</code>, <code>GetContentModeration</code>\n   *        returns labels with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartContentModeration</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the\n   *       unsafe content analysis to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartContentModerationRequest {\n  export const filterSensitiveLog = (obj: StartContentModerationRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartContentModerationResponse {\n  /**\n   * <p>The identifier for the unsafe content analysis job. Use <code>JobId</code> to identify the job in\n   *       a subsequent call to <code>GetContentModeration</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartContentModerationResponse {\n  export const filterSensitiveLog = (obj: StartContentModerationResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceDetectionRequest {\n  /**\n   * <p>The video in which you want to detect faces. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartFaceDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the\n   *          face detection operation.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>The face attributes you want returned.</p>\n   *          <p>\n   *             <code>DEFAULT</code> - The following subset of facial attributes are returned: BoundingBox, Confidence, Pose, Quality and Landmarks. </p>\n   *          <p>\n   *             <code>ALL</code> - All facial attributes are returned.</p>\n   */\n  FaceAttributes?: FaceAttributes | string;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartFaceDetectionRequest {\n  export const filterSensitiveLog = (obj: StartFaceDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceDetectionResponse {\n  /**\n   * <p>The identifier for the face detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetFaceDetection</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartFaceDetectionResponse {\n  export const filterSensitiveLog = (obj: StartFaceDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceSearchRequest {\n  /**\n   * <p>The video you want to search. The video must be stored in an Amazon S3 bucket. </p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartFaceSearch</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The minimum confidence in the person match to return. For example, don't return any matches where confidence in matches is less than 70%.\n   *       The default value is 80%.</p>\n   */\n  FaceMatchThreshold?: number;\n\n  /**\n   * <p>ID of the collection that contains the faces you want to search for.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the search. </p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartFaceSearchRequest {\n  export const filterSensitiveLog = (obj: StartFaceSearchRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceSearchResponse {\n  /**\n   * <p>The identifier for the search job. Use <code>JobId</code> to identify the job in a subsequent call to <code>GetFaceSearch</code>. </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartFaceSearchResponse {\n  export const filterSensitiveLog = (obj: StartFaceSearchResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartLabelDetectionRequest {\n  /**\n   * <p>The video in which you want to detect labels. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartLabelDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected label. Confidence\n   *        represents how certain Amazon Rekognition is that a label is correctly identified.0 is the lowest confidence.\n   *        100 is the highest confidence.  Amazon Rekognition Video doesn't return any labels with a confidence level\n   *        lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with confidence\n   *      values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the label detection\n   *         operation to. </p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartLabelDetectionRequest {\n  export const filterSensitiveLog = (obj: StartLabelDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartLabelDetectionResponse {\n  /**\n   * <p>The identifier for the label detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetLabelDetection</code>. </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartLabelDetectionResponse {\n  export const filterSensitiveLog = (obj: StartLabelDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartPersonTrackingRequest {\n  /**\n   * <p>The video in which you want to detect people. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartPersonTracking</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the people detection\n   *         operation to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartPersonTrackingRequest {\n  export const filterSensitiveLog = (obj: StartPersonTrackingRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartPersonTrackingResponse {\n  /**\n   * <p>The identifier for the person detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetPersonTracking</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartPersonTrackingResponse {\n  export const filterSensitiveLog = (obj: StartPersonTrackingResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name(ARN) of the model version that you want to start.</p>\n   */\n  ProjectVersionArn: string | undefined;\n\n  /**\n   * <p>The minimum number of inference units to use. A single\n   *       inference unit represents 1 hour of processing and can support up to 5 Transaction Pers Second (TPS).\n   *       Use a higher number to increase the TPS throughput of your model. You are charged for the number\n   *       of inference units that you use.\n   *     </p>\n   */\n  MinInferenceUnits: number | undefined;\n}\n\nexport namespace StartProjectVersionRequest {\n  export const filterSensitiveLog = (obj: StartProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartProjectVersionResponse {\n  /**\n   * <p>The current running status of the model. </p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace StartProjectVersionResponse {\n  export const filterSensitiveLog = (obj: StartProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters for the shot detection segments returned by <code>GetSegmentDetection</code>.\n *       For more information, see <a>StartSegmentDetectionFilters</a>.</p>\n */\nexport interface StartShotDetectionFilter {\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. Confidence\n   *       represents how certain Amazon Rekognition is that a segment is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition Video doesn't return any segments with a confidence level\n   *       lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinSegmentConfidence</code>, the <code>GetSegmentDetection</code> returns\n   *         segments with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinSegmentConfidence?: number;\n}\n\nexport namespace StartShotDetectionFilter {\n  export const filterSensitiveLog = (obj: StartShotDetectionFilter): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters for the technical segments returned by <a>GetSegmentDetection</a>. For more information,\n *       see <a>StartSegmentDetectionFilters</a>.</p>\n */\nexport interface StartTechnicalCueDetectionFilter {\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. Confidence\n   *       represents how certain Amazon Rekognition is that a segment is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition Video doesn't return any segments with a confidence level\n   *       lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinSegmentConfidence</code>, <code>GetSegmentDetection</code> returns\n   *       segments with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinSegmentConfidence?: number;\n}\n\nexport namespace StartTechnicalCueDetectionFilter {\n  export const filterSensitiveLog = (obj: StartTechnicalCueDetectionFilter): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters applied to the technical cue or shot detection segments.\n *       For more information, see <a>StartSegmentDetection</a>.\n *     </p>\n */\nexport interface StartSegmentDetectionFilters {\n  /**\n   * <p>Filters that are specific to technical cues.</p>\n   */\n  TechnicalCueFilter?: StartTechnicalCueDetectionFilter;\n\n  /**\n   * <p>Filters that are specific to shot detections.</p>\n   */\n  ShotFilter?: StartShotDetectionFilter;\n}\n\nexport namespace StartSegmentDetectionFilters {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartSegmentDetectionRequest {\n  /**\n   * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n   *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartSegmentDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the\n   *       segment detection operation.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n\n  /**\n   * <p>Filters for technical cue or shot detection.</p>\n   */\n  Filters?: StartSegmentDetectionFilters;\n\n  /**\n   * <p>An array of segment types to detect in the video. Valid values are TECHNICAL_CUE and SHOT.</p>\n   */\n  SegmentTypes: (SegmentType | string)[] | undefined;\n}\n\nexport namespace StartSegmentDetectionRequest {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartSegmentDetectionResponse {\n  /**\n   * <p>Unique identifier for the segment detection job. The <code>JobId</code> is returned from <code>StartSegmentDetection</code>.\n   *     </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartSegmentDetectionResponse {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartStreamProcessorRequest {\n  /**\n   * <p>The name of the stream processor to start processing.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace StartStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: StartStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartStreamProcessorResponse {}\n\nexport namespace StartStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: StartStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Set of optional parameters that let you set the criteria text must meet to be included in your response.\n *       <code>WordFilter</code> looks at a word's height, width and minimum confidence. <code>RegionOfInterest</code>\n *       lets you set a specific region of the screen to look for text in.</p>\n */\nexport interface StartTextDetectionFilters {\n  /**\n   * <p>Filters focusing on qualities of the text, such as confidence or size.</p>\n   */\n  WordFilter?: DetectionFilter;\n\n  /**\n   * <p>Filter focusing on a certain area of the frame. Uses a <code>BoundingBox</code> object to set the region\n   *       of the screen.</p>\n   */\n  RegionsOfInterest?: RegionOfInterest[];\n}\n\nexport namespace StartTextDetectionFilters {\n  export const filterSensitiveLog = (obj: StartTextDetectionFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartTextDetectionRequest {\n  /**\n   * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n   *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple <code>StartTextDetection</code>\n   *       requests, the same <code>JobId</code> is returned. Use <code>ClientRequestToken</code> to prevent the same job\n   *         from being accidentaly started more than once.</p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see\n   *             <a>api-video</a>.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier returned in the completion status published by your Amazon Simple Notification Service topic.  For example, you can use <code>JobTag</code> to group related jobs\n   *       and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n\n  /**\n   * <p>Optional parameters that let you set criteria the text must meet to be included in your response.</p>\n   */\n  Filters?: StartTextDetectionFilters;\n}\n\nexport namespace StartTextDetectionRequest {\n  export const filterSensitiveLog = (obj: StartTextDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartTextDetectionResponse {\n  /**\n   * <p>Identifier for the text detection job.  Use <code>JobId</code> to identify the job in a subsequent call to <code>GetTextDetection</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartTextDetectionResponse {\n  export const filterSensitiveLog = (obj: StartTextDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>\n   *          <p>This operation requires permissions to perform the <code>rekognition:StopProjectVersion</code> action.</p>\n   */\n  ProjectVersionArn: string | undefined;\n}\n\nexport namespace StopProjectVersionRequest {\n  export const filterSensitiveLog = (obj: StopProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopProjectVersionResponse {\n  /**\n   * <p>The current status of the stop operation. </p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace StopProjectVersionResponse {\n  export const filterSensitiveLog = (obj: StopProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopStreamProcessorRequest {\n  /**\n   * <p>The name of a stream processor created by <a>CreateStreamProcessor</a>.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace StopStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: StopStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopStreamProcessorResponse {}\n\nexport namespace StopStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: StopStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n"]},"metadata":{},"sourceType":"module"}